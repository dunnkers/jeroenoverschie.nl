{"meta":{"exported_on":1744206721910,"version":"5.116.1"},"data":{"newsletters":[{"id":"62a48eba9cd77522dc21c094","uuid":"5a037ed9-61d7-4dfe-ba69-a37c1aaf5d6d","name":"Jeroen Overschie","description":null,"slug":"default-newsletter","sender_name":null,"sender_email":null,"sender_reply_to":"newsletter","status":"active","visibility":"members","subscribe_on_signup":1,"sort_order":0,"header_image":null,"show_header_icon":1,"show_header_title":1,"title_font_category":"sans_serif","title_alignment":"center","show_feature_image":1,"body_font_category":"sans_serif","footer_content":null,"show_badge":1,"show_header_name":0,"created_at":"2022-06-11 12:46:50","updated_at":"2022-06-11 12:47:41","feedback_enabled":0,"show_post_title_section":1,"show_comment_cta":1,"show_subscription_details":0,"show_latest_posts":0,"background_color":"light","border_color":null,"title_color":null,"show_excerpt":0}],"posts":[{"id":"62a48f659cd77522dc21c275","uuid":"50ddcc96-fd94-4a6e-9823-66a18b626b81","title":"Automated curtains project","slug":"automated-curtains","mobiledoc":null,"html":"<p>An idea sprung up in my mind some while ago. In my student dorm, I have electric curtains. They can be operated using a little remote, allowing one to open or close the curtains. This is pretty useful, because I don't even have to get out of bed to open my curtains ‚Äì I can just use the remote. But the remote uses radio waves to operate the curtains - and I have a Raspberry Pi laying around, doing nothing. What if I could operate the curtains using my Raspberry Pi? Such, that the curtains open at a certain time in the morning. In this way, my curtains would function as an alarm clock! In this project, I did exactly that üòâ.</p><h3 id=\"how\">How</h3><p>First, I have to figure out at all how to do this. Taking a look at the curtain remote, I found the brand to be '<em>Somfy'</em>. After some Google image searches I found the name of my remote model, the Somfy Telis 1-RTS:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/SOMFY-TELIS-1-RTS-old.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"300\" height=\"300\"><figcaption>My curtain remote. The goal is to emulate whatever signal it is sending to the curtains using a Raspberry Pi.</figcaption></figure><p>I want to emulate the RF (Radio Frequency) signal the remote is emitting. Such that, instead of pressing a button on the remote, I can control the curtains programmatically using code. Then, because the Raspberry Pi will always be on, I can configure certain times to open/close the curtains.</p><p>But surely, other people have wanted to do this too. Somfy is a popular brand for electric curtains after all. So, I searched, and found <a href=\"https://github.com/Nickduino/Pi-Somfy\">Github project</a> containing code to control the curtains using a Raspberry Pi, if correctly assembled. Let's start!</p><h3 id=\"preparation\">Preparation</h3><p>I need a couple things to make this work.</p><ol><li>Raspberry Pi (I am using a Raspberry Pi 2011 edition - Model B)</li><li>RF (Radio Frequency) transmitter (with an oscillator at 433.42 Mhz)</li><li>Cables to connect the RF emitter to the Raspberry Pi</li></ol><p>Most parts could easily be ordered through Ebay. However, Somfy did something smart in their product. They intentionally set their oscillator frequency to an odd number, 433.42 Mhz. Most other RF emitters run at 433.93 Mhz. There are, luckily, some places you can order a 433.42 oscillator. But only the oscillator. This means we are going to have to do some soldering to replace the oscillator. After a couple weeks, my parts arrived.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/Photo-from-Jeroen-Overschie-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/11/Photo-from-Jeroen-Overschie-3.png 600w, __GHOST_URL__/content/images/2021/11/Photo-from-Jeroen-Overschie-3.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The RF emitter on the left and a replacement part for changing its oscillator frequency on the right.</figcaption></figure><h3 id=\"building-the-pi-emitter\">Building the Pi emitter</h3><p>My friend happened to possess a soldering set, so after a quick visit I managed to solder the correct oscillator onto the RF emitter board. ¬†Using a set of cables, I could attach the RF transmitter to the Raspberry Pi üôåüèª.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/Screen-Shot-2021-11-26-at-14.49.31.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1008\" height=\"942\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/11/Screen-Shot-2021-11-26-at-14.49.31.png 600w, __GHOST_URL__/content/images/size/w1000/2021/11/Screen-Shot-2021-11-26-at-14.49.31.png 1000w, __GHOST_URL__/content/images/2021/11/Screen-Shot-2021-11-26-at-14.49.31.png 1008w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The fully assembled Raspberry Pi, with its RF emitter attached via a cable.</figcaption></figure><p>I also bought an extra enclosure to keep the thing a bit more safe:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/image00003.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"540\" height=\"720\"><figcaption>Assembled Raspberry Pi RF transmitter, with enclosure.</figcaption></figure><p>Now all there's left is configure the correct software on the Raspberry Pi. Using the Github project I found, I was able to install the software and make the software automatically start on a reboot. It has a pretty neat interface, allowing one to set CRON jobs to open/close the curtains. In non-nerd speech we would just call this 'an alarm' üòÖ.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/p3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"982\" height=\"545\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/11/p3.png 600w, __GHOST_URL__/content/images/2021/11/p3.png 982w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The Pi-Somfy interface. One can easily connect to a web server running on the Pi if on the same network, allowing me to configure the alarms even on my phone.</figcaption></figure><p>I now just had to execute a certain pattern of button presses to emulate pairing a new remote. And then ... it worked! üéâ</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/ezgif-2-a5aab328d979.gif\" class=\"kg-image\" alt loading=\"lazy\" width=\"320\" height=\"569\"><figcaption>Opening my curtains using the Raspberry Pi and its RF sensor.</figcaption></figure><p>Now, I can go to sleep in darkness, and wake up with sunlight hitting my face üåû. Awesome! The project has succeeded and the cool thing is I have been using this every day ever since. The cool thing about doing a Computer Science degree is when you can apply your knowledge to solve real-world problems. When I leave this student dormitory, I will leave the Raspberry Pi right where it is, so others can also benefit from automated curtains üòä. Cheers!</p>","comment_id":"61a0e5ee72c7ac1e20461f81","plaintext":"An idea sprung up in my mind some while ago. In my student dorm, I have electric curtains. They can be operated using a little remote, allowing one to open or close the curtains. This is pretty useful, because I don't even have to get out of bed to open my curtains ‚Äì I can just use the remote. But the remote uses radio waves to operate the curtains - and I have a Raspberry Pi laying around, doing nothing. What if I could operate the curtains using my Raspberry Pi? Such, that the curtains open at a certain time in the morning. In this way, my curtains would function as an alarm clock! In this project, I did exactly that üòâ.\n\n\nHow\n\nFirst, I have to figure out at all how to do this. Taking a look at the curtain remote, I found the brand to be 'Somfy'. After some Google image searches I found the name of my remote model, the Somfy Telis 1-RTS:\n\nI want to emulate the RF (Radio Frequency) signal the remote is emitting. Such that, instead of pressing a button on the remote, I can control the curtains programmatically using code. Then, because the Raspberry Pi will always be on, I can configure certain times to open/close the curtains.\n\nBut surely, other people have wanted to do this too. Somfy is a popular brand for electric curtains after all. So, I searched, and found Github project containing code to control the curtains using a Raspberry Pi, if correctly assembled. Let's start!\n\n\nPreparation\n\nI need a couple things to make this work.\n\n 1. Raspberry Pi (I am using a Raspberry Pi 2011 edition - Model B)\n 2. RF (Radio Frequency) transmitter (with an oscillator at 433.42 Mhz)\n 3. Cables to connect the RF emitter to the Raspberry Pi\n\nMost parts could easily be ordered through Ebay. However, Somfy did something smart in their product. They intentionally set their oscillator frequency to an odd number, 433.42 Mhz. Most other RF emitters run at 433.93 Mhz. There are, luckily, some places you can order a 433.42 oscillator. But only the oscillator. This means we are going to have to do some soldering to replace the oscillator. After a couple weeks, my parts arrived.\n\n\nBuilding the Pi emitter\n\nMy friend happened to possess a soldering set, so after a quick visit I managed to solder the correct oscillator onto the RF emitter board. ¬†Using a set of cables, I could attach the RF transmitter to the Raspberry Pi üôåüèª.\n\nI also bought an extra enclosure to keep the thing a bit more safe:\n\nNow all there's left is configure the correct software on the Raspberry Pi. Using the Github project I found, I was able to install the software and make the software automatically start on a reboot. It has a pretty neat interface, allowing one to set CRON jobs to open/close the curtains. In non-nerd speech we would just call this 'an alarm' üòÖ.\n\nI now just had to execute a certain pattern of button presses to emulate pairing a new remote. And then ... it worked! üéâ\n\nNow, I can go to sleep in darkness, and wake up with sunlight hitting my face üåû. Awesome! The project has succeeded and the cool thing is I have been using this every day ever since. The cool thing about doing a Computer Science degree is when you can apply your knowledge to solve real-world problems. When I leave this student dormitory, I will leave the Raspberry Pi right where it is, so others can also benefit from automated curtains üòä. Cheers!","feature_image":"__GHOST_URL__/content/images/2021/11/automated-curtains-cover-2.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-26 13:49:34","created_by":"1","updated_at":"2024-11-22 13:36:41","updated_by":"1","published_at":"2020-02-09 23:00:00","published_by":"1","custom_excerpt":"My electric curtains can already be controlled by a remote. It would be cool if they could open in the morning, like an alarm clock. What if I could do this using a Raspberry Pi, by emulating the remote?","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"An idea sprung up in my mind some while ago. In my student dorm, I have electric curtains. They can be operated using a little remote, allowing one to open or close the curtains. This is pretty useful, because I don't even have to get out of bed to open my curtains ‚Äì I can just use the remote. But the remote uses radio waves to operate the curtains - and I have a Raspberry Pi laying around, doing nothing. What if I could operate the curtains using my Raspberry Pi? Such, that the curtains open at a certain time in the morning. In this way, my curtains would function as an alarm clock! In this project, I did exactly that üòâ.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"First, I have to figure out at all how to do this. Taking a look at the curtain remote, I found the brand to be '\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Somfy'\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". After some Google image searches I found the name of my remote model, the Somfy Telis 1-RTS:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/SOMFY-TELIS-1-RTS-old.jpeg\",\"width\":300,\"height\":300,\"caption\":\"My curtain remote. The goal is to emulate whatever signal it is sending to the curtains using a Raspberry Pi.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I want to emulate the RF (Radio Frequency) signal the remote is emitting. Such that, instead of pressing a button on the remote, I can control the curtains programmatically using code. Then, because the Raspberry Pi will always be on, I can configure certain times to open/close the curtains.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"But surely, other people have wanted to do this too. Somfy is a popular brand for electric curtains after all. So, I searched, and found \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Github project\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/Nickduino/Pi-Somfy\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" containing code to control the curtains using a Raspberry Pi, if correctly assembled. Let's start!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Preparation\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I need a couple things to make this work.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Raspberry Pi (I am using a Raspberry Pi 2011 edition - Model B)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RF (Radio Frequency) transmitter (with an oscillator at 433.42 Mhz)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Cables to connect the RF emitter to the Raspberry Pi\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Most parts could easily be ordered through Ebay. However, Somfy did something smart in their product. They intentionally set their oscillator frequency to an odd number, 433.42 Mhz. Most other RF emitters run at 433.93 Mhz. There are, luckily, some places you can order a 433.42 oscillator. But only the oscillator. This means we are going to have to do some soldering to replace the oscillator. After a couple weeks, my parts arrived.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/Photo-from-Jeroen-Overschie-3.png\",\"width\":1000,\"height\":600,\"caption\":\"The RF emitter on the left and a replacement part for changing its oscillator frequency on the right.\",\"cardWidth\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Building the Pi emitter\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"My friend happened to possess a soldering set, so after a quick visit I managed to solder the correct oscillator onto the RF emitter board.  Using a set of cables, I could attach the RF transmitter to the Raspberry Pi üôåüèª.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/Screen-Shot-2021-11-26-at-14.49.31.png\",\"width\":1008,\"height\":942,\"cardWidth\":\"\",\"caption\":\"The fully assembled Raspberry Pi, with its RF emitter attached via a cable.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I also bought an extra enclosure to keep the thing a bit more safe:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/image00003.jpeg\",\"width\":540,\"height\":720,\"cardWidth\":\"\",\"caption\":\"Assembled Raspberry Pi RF transmitter, with enclosure.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now all there's left is configure the correct software on the Raspberry Pi. Using the Github project I found, I was able to install the software and make the software automatically start on a reboot. It has a pretty neat interface, allowing one to set CRON jobs to open/close the curtains. In non-nerd speech we would just call this 'an alarm' üòÖ.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/p3.png\",\"width\":982,\"height\":545,\"caption\":\"The Pi-Somfy interface. One can easily connect to a web server running on the Pi if on the same network, allowing me to configure the alarms even on my phone.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I now just had to execute a certain pattern of button presses to emulate pairing a new remote. And then ... it worked! üéâ\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"caption\":\"Opening my curtains using the Raspberry Pi and its RF sensor.\",\"src\":\"__GHOST_URL__/content/images/2021/11/ezgif-2-a5aab328d979.gif\",\"width\":320,\"height\":569},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now, I can go to sleep in darkness, and wake up with sunlight hitting my face üåû. Awesome! The project has succeeded and the cool thing is I have been using this every day ever since. The cool thing about doing a Computer Science degree is when you can apply your knowledge to solve real-world problems. When I leave this student dormitory, I will leave the Raspberry Pi right where it is, so others can also benefit from automated curtains üòä. Cheers!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c276","uuid":"63cbb582-3d59-4940-8d39-55dc124f8bfe","title":"School break time friend finder","slug":"school-breaktime-friend-finder","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<figure class=\\\"kg-card kg-image-card kg-card-hascaption\\\">\\n    <img src=\\\"__GHOST_URL__/content/images/2022/01/PWS-roosters-infrastructuur.svg\\\" class=\\\"kg-image\\\" alt=\\\"An overview of the app architecture. A Node.js app scrapes and parses student schedules, puts it in a database, which an Ember.js app then consumes through a REST API.\\\" loading=\\\"lazy\\\">\\n    <figcaption>An overview of the app architecture. A Node.js app scrapes and parses student schedules, puts it in a database, which an Ember.js app then consumes through a REST API.</figcaption>\\n</figure>\"}],[\"html\",{\"html\":\"<figure class=\\\"kg-card kg-image-card kg-card-hascaption\\\">\\n    <img src=\\\"__GHOST_URL__/content/images/2022/01/PWS-Roosters-ORM---Object-Relational-Mapping.svg\\\" class=\\\"kg-image\\\" alt=\\\"An Object-Relational-Mapping (ORM) of the school. Most important is a lesson, which then relates teachers, students and a room.\\\" loading=\\\"lazy\\\">\\n    <figcaption>An Object-Relational-Mapping (ORM) of the school. Most important is a lesson, which then relates teachers, students and a room.</figcaption>\\n</figure>\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.53.37.png\",\"width\":626,\"height\":445,\"caption\":\"The working break-time friend finder app. Using a simple search, a student can find his schedule.\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.52.57.png\",\"width\":609,\"height\":290,\"caption\":\"In the app, you can click any class or break to see with whom you share the hour. It's no longer a guessing game! ‚úì\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/dunnkers/roosters-api\"]],[\"a\",[\"href\",\"https://cheerio.js.org/\"]],[\"a\",[\"href\",\"https://github.com/mongo-js/mongojs\"]],[\"a\",[\"href\",\"https://github.com/dunnkers/roosters\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"At my school, students often had gaps in their schedules. In between lessons scheduled for the day, one would often have breaks in between. But because you chose a personalized package of classes to follow, everyone's schedule was also different. So, it would be hard to know with whom you could spend those breaks! To solve this, I developed this app. It allows students to find with whom they have breaks so they can hang out with them whilst waiting for the next class üòä. The app was actually used by students in my school. Very cool! \"]]],[1,\"h3\",[[0,[],0,\"Building the app\"]]],[1,\"p\",[[0,[],0,\"The entire app is quite sophisticated. The various components can be laid out as follows:\"]]],[3,\"ul\",[[[0,[],0,\"Node.js backend (\"],[0,[0],1,\"Github\"],[0,[],0,\")\"],[1,[],0,0],[0,[],0,\"- Has three main responsibilities:\"],[1,[],0,1],[0,[],0,\"(1) to scrape student schedules off HTML pages into a MongoDB database. Scraping is done using \"],[0,[1],1,\"Cheerio\"],[0,[],0,\" and communication with MongoDB via \"],[0,[2],1,\"MongoJS\"],[0,[],0,\".\"],[1,[],0,2],[0,[],0,\"(2) parse the schedules into a relational format and compute what odd break-time hours exist.\"],[1,[],0,3],[0,[],0,\"(3) expose the MongoDB database as an API.\"]],[[0,[],0,\"Ember.js frontend (\"],[0,[3],1,\"Github\"],[0,[],0,\")\"],[1,[],0,4],[0,[],0,\"- This front-end then consumes the API data using ember-data. I'm using Bootstrap as a UI framework so I don't have to build all the buttons, tables and interfacing myself.\"]]]],[1,\"p\",[]],[10,0],[1,\"p\",[[0,[],0,\"... the relational mapping in the database is as follows:\"]]],[10,1],[1,\"p\",[[0,[],0,\"And our working app looks as follows:\"]]],[10,2],[1,\"p\",[[0,[],0,\"But most importantly, the functionality to see whoever shares your break-time ('tussen' in the picture below) or any classes with you:\"]]],[10,3],[1,\"p\",[[0,[],0,\"ü•≥\"]]],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>At my school, students often had gaps in their schedules. In between lessons scheduled for the day, one would often have breaks in between. But because you chose a personalized package of classes to follow, everyone's schedule was also different. So, it would be hard to know with whom you could spend those breaks! To solve this, I developed this app. It allows students to find with whom they have breaks so they can hang out with them whilst waiting for the next class üòä. The app was actually used by students in my school. Very cool! </p><h3 id=\"building-the-app\">Building the app</h3><p>The entire app is quite sophisticated. The various components can be laid out as follows:</p><ul><li>Node.js backend (<a href=\"https://github.com/dunnkers/roosters-api\">Github</a>)<br>- Has three main responsibilities:<br>(1) to scrape student schedules off HTML pages into a MongoDB database. Scraping is done using <a href=\"https://cheerio.js.org/\">Cheerio</a> and communication with MongoDB via <a href=\"https://github.com/mongo-js/mongojs\">MongoJS</a>.<br>(2) parse the schedules into a relational format and compute what odd break-time hours exist.<br>(3) expose the MongoDB database as an API.</li><li>Ember.js frontend (<a href=\"https://github.com/dunnkers/roosters\">Github</a>)<br>- This front-end then consumes the API data using ember-data. I'm using Bootstrap as a UI framework so I don't have to build all the buttons, tables and interfacing myself.</li></ul><p></p><!--kg-card-begin: html--><figure class=\"kg-card kg-image-card kg-card-hascaption\">\n    <img src=\"__GHOST_URL__/content/images/2022/01/PWS-roosters-infrastructuur.svg\" class=\"kg-image\" alt=\"An overview of the app architecture. A Node.js app scrapes and parses student schedules, puts it in a database, which an Ember.js app then consumes through a REST API.\" loading=\"lazy\">\n    <figcaption>An overview of the app architecture. A Node.js app scrapes and parses student schedules, puts it in a database, which an Ember.js app then consumes through a REST API.</figcaption>\n</figure><!--kg-card-end: html--><p>... the relational mapping in the database is as follows:</p><!--kg-card-begin: html--><figure class=\"kg-card kg-image-card kg-card-hascaption\">\n    <img src=\"__GHOST_URL__/content/images/2022/01/PWS-Roosters-ORM---Object-Relational-Mapping.svg\" class=\"kg-image\" alt=\"An Object-Relational-Mapping (ORM) of the school. Most important is a lesson, which then relates teachers, students and a room.\" loading=\"lazy\">\n    <figcaption>An Object-Relational-Mapping (ORM) of the school. Most important is a lesson, which then relates teachers, students and a room.</figcaption>\n</figure><!--kg-card-end: html--><p>And our working app looks as follows:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.53.37.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"626\" height=\"445\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/01/Screen-Shot-2022-01-06-at-13.53.37.png 600w, __GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.53.37.png 626w\"><figcaption>The working break-time friend finder app. Using a simple search, a student can find his schedule.</figcaption></figure><p>But most importantly, the functionality to see whoever shares your break-time ('tussen' in the picture below) or any classes with you:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.52.57.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"609\" height=\"290\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/01/Screen-Shot-2022-01-06-at-13.52.57.png 600w, __GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.52.57.png 609w\"><figcaption>In the app, you can click any class or break to see with whom you share the hour. It's no longer a guessing game! ‚úì</figcaption></figure><p>ü•≥</p><p></p>","comment_id":"61a2265672c7ac1e2046209d","plaintext":"At my school, students often had gaps in their schedules. In between lessons scheduled for the day, one would often have breaks in between. But because you chose a personalized package of classes to follow, everyone's schedule was also different. So, it would be hard to know with whom you could spend those breaks! To solve this, I developed this app. It allows students to find with whom they have breaks so they can hang out with them whilst waiting for the next class üòä. The app was actually used by students in my school. Very cool!\n\n\nBuilding the app\n\nThe entire app is quite sophisticated. The various components can be laid out as follows:\n\n * Node.js backend (Github)\n   - Has three main responsibilities:\n   (1) to scrape student schedules off HTML pages into a MongoDB database. Scraping is done using Cheerio and communication with MongoDB via MongoJS.\n   (2) parse the schedules into a relational format and compute what odd break-time hours exist.\n   (3) expose the MongoDB database as an API.\n * Ember.js frontend (Github)\n   - This front-end then consumes the API data using ember-data. I'm using Bootstrap as a UI framework so I don't have to build all the buttons, tables and interfacing myself.\n\n\n\n\n\n\n... the relational mapping in the database is as follows:\n\n\n\n\nAnd our working app looks as follows:\n\nBut most importantly, the functionality to see whoever shares your break-time ('tussen' in the picture below) or any classes with you:\n\nü•≥\n\n","feature_image":"__GHOST_URL__/content/images/2022/01/Screen-Shot-2022-01-06-at-13.54.22.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-27 12:36:38","created_by":"1","updated_at":"2022-01-06 13:09:24","updated_by":null,"published_at":"2015-02-20 14:00:00","published_by":"1","custom_excerpt":"Spending school breaks alone is lame. That's why I developed an app so students can find with whom they share their breaks.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c277","uuid":"fe81586b-9c85-4358-af30-a6c86ee7125e","title":"Data & privacy","slug":"privacy","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/01/thank-you.gif\",\"width\":220,\"height\":176}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Hi! Just like you, I value my privacy. No personally identifiable information is sent anywhere. Your privacy remains totally yours! ‚≠êÔ∏è\"]]],[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Hi! Just like you, I value my privacy. No personally identifiable information is sent anywhere. Your privacy remains totally yours! ‚≠êÔ∏è</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/01/thank-you.gif\" class=\"kg-image\" alt loading=\"lazy\" width=\"220\" height=\"176\"></figure>","comment_id":"61a3adce72c7ac1e204621b0","plaintext":"Hi! Just like you, I value my privacy. No personally identifiable information is sent anywhere. Your privacy remains totally yours! ‚≠êÔ∏è","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-28 16:26:54","created_by":"1","updated_at":"2024-01-19 10:27:31","updated_by":"1","published_at":"2021-11-28 16:28:06","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c278","uuid":"5da60094-aa3f-41c0-8141-027417bc8faa","title":"COVID-19 Dashboard","slug":"covid-19-dashboard","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/Architecture_Batch.svg\",\"alt\":\"Batch architecture\",\"title\":\"\",\"caption\":\"The Corona dashboard <b>batch</b> processing architecture.\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/Architecture_Streaming.svg\",\"alt\":\"Stream architecture\",\"title\":\"\",\"caption\":\"The Corona dashboard <b>stream</b> processing architecture.\"}],[\"html\",{\"html\":\"<figure class=\\\"kg-card kg-image-card kg-card-hascaption\\\">\\n    <img src=\\\"__GHOST_URL__/content/images/2021/12/Architecture_Full--1-.svg\\\" class=\\\"kg-image\\\" alt=\\\"Full architecture\\\" loading=\\\"lazy\\\">\\n    <figcaption>Both the batch- and stream processing pipelines visualized in a single diagram.</figcaption>\\n</figure>\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/github32-1.png\",\"width\":32,\"height\":32,\"cardWidth\":\"\",\"caption\":\"<a href=\\\"https://github.com/dunnkers/disease-spread\\\">disease-spread</a>\",\"href\":\"https://github.com/dunnkers/disease-spread\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/CSSEGISandData/COVID-19\"]],[\"a\",[\"href\",\"https://www.worldpop.org/\"]],[\"a\",[\"href\",\"https://dunnkers.com/disease-spread/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"We are in the midst of a global pandemic. At the time this project started, the Corona virus was still just a headline for most - but in the meantime it reached and impacted all of our lives. Fighting such a pandemic happens in many ways on multiple scales. We are interested in how this can be done on the societal level: using data. In this project, me and my teammates built a pipeline capable of processing a large dataset and created a visualization of the areas most vulnerable to Corona which includes reported cases in real-time.\"]]],[1,\"h3\",[[0,[],0,\"Architecture\"]]],[1,\"p\",[[0,[],0,\"To quickly summarize the application: a backend downloads- and processes \"],[0,[0],1,\"Corona data\"],[0,[],0,\" and \"],[0,[1],1,\"population data\"],[0,[],0,\". A clustering algorithm is applied to determine the most 'vulnerable' areas to Corona outbreak. Then, all resulting insights are stored in a MongoDB database and exposes through an API. A frontend then integrates with Mapbox to show the data visually, on a map. Because we were working with large amounts of data, some sophisticated technologies were required to properly process the data:\"]]],[3,\"ul\",[[[0,[],0,\"Apache Kafka\"]],[[0,[],0,\"Apache Spark\"]],[[0,[],0,\"Apache Zeppelin to author PySpark scripts\"]]]],[1,\"p\",[[0,[],0,\"Brought together, this can be put in a diagram as follows:\"]]],[10,0],[1,\"p\",[[0,[],0,\"All components are hosted on Google Cloud Platform (GCP). To also demonstrate merging both batch- and stream data in a single dashboard, also another architecture was built. This time, we took in tweets that are concerned about Corona through the keyword 'Corona' and used a Map-Reduce technique to compute the total amount of Corona-tweets sent by each country in the world. This was then again, stored in a MongoDB database and exposed as an API. See the stream processing architecture below:\"]]],[10,1],[1,\"p\",[[0,[],0,\"To conclude both architectures and how they come together in a single application, see the following diagram:\"]]],[10,2],[1,\"p\",[[0,[],0,\"To see the front-end in action, see the \"],[0,[2],1,\"live dashboard\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Full report\"]]],[1,\"p\",[[0,[],0,\"For further reading, check out the GitHub repository:\"]]],[10,3],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>We are in the midst of a global pandemic. At the time this project started, the Corona virus was still just a headline for most - but in the meantime it reached and impacted all of our lives. Fighting such a pandemic happens in many ways on multiple scales. We are interested in how this can be done on the societal level: using data. In this project, me and my teammates built a pipeline capable of processing a large dataset and created a visualization of the areas most vulnerable to Corona which includes reported cases in real-time.</p><h3 id=\"architecture\">Architecture</h3><p>To quickly summarize the application: a backend downloads- and processes <a href=\"https://github.com/CSSEGISandData/COVID-19\">Corona data</a> and <a href=\"https://www.worldpop.org/\">population data</a>. A clustering algorithm is applied to determine the most 'vulnerable' areas to Corona outbreak. Then, all resulting insights are stored in a MongoDB database and exposes through an API. A frontend then integrates with Mapbox to show the data visually, on a map. Because we were working with large amounts of data, some sophisticated technologies were required to properly process the data:</p><ul><li>Apache Kafka</li><li>Apache Spark</li><li>Apache Zeppelin to author PySpark scripts</li></ul><p>Brought together, this can be put in a diagram as follows:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Architecture_Batch.svg\" class=\"kg-image\" alt=\"Batch architecture\" loading=\"lazy\"><figcaption>The Corona dashboard <b>batch</b> processing architecture.</figcaption></figure><p>All components are hosted on Google Cloud Platform (GCP). To also demonstrate merging both batch- and stream data in a single dashboard, also another architecture was built. This time, we took in tweets that are concerned about Corona through the keyword 'Corona' and used a Map-Reduce technique to compute the total amount of Corona-tweets sent by each country in the world. This was then again, stored in a MongoDB database and exposed as an API. See the stream processing architecture below:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Architecture_Streaming.svg\" class=\"kg-image\" alt=\"Stream architecture\" loading=\"lazy\"><figcaption>The Corona dashboard <b>stream</b> processing architecture.</figcaption></figure><p>To conclude both architectures and how they come together in a single application, see the following diagram:</p><!--kg-card-begin: html--><figure class=\"kg-card kg-image-card kg-card-hascaption\">\n    <img src=\"__GHOST_URL__/content/images/2021/12/Architecture_Full--1-.svg\" class=\"kg-image\" alt=\"Full architecture\" loading=\"lazy\">\n    <figcaption>Both the batch- and stream processing pipelines visualized in a single diagram.</figcaption>\n</figure><!--kg-card-end: html--><p>To see the front-end in action, see the <a href=\"https://dunnkers.com/disease-spread/\">live dashboard</a>.</p><h3 id=\"full-report\">Full report</h3><p>For further reading, check out the GitHub repository:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><a href=\"https://github.com/dunnkers/disease-spread\"><img src=\"__GHOST_URL__/content/images/2021/11/github32-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"32\" height=\"32\"></a><figcaption><a href=\"https://github.com/dunnkers/disease-spread\">disease-spread</a></figcaption></figure>","comment_id":"61a3b81e72c7ac1e204621d0","plaintext":"We are in the midst of a global pandemic. At the time this project started, the Corona virus was still just a headline for most - but in the meantime it reached and impacted all of our lives. Fighting such a pandemic happens in many ways on multiple scales. We are interested in how this can be done on the societal level: using data. In this project, me and my teammates built a pipeline capable of processing a large dataset and created a visualization of the areas most vulnerable to Corona which includes reported cases in real-time.\n\n\nArchitecture\n\nTo quickly summarize the application: a backend downloads- and processes Corona data and population data. A clustering algorithm is applied to determine the most 'vulnerable' areas to Corona outbreak. Then, all resulting insights are stored in a MongoDB database and exposes through an API. A frontend then integrates with Mapbox to show the data visually, on a map. Because we were working with large amounts of data, some sophisticated technologies were required to properly process the data:\n\n * Apache Kafka\n * Apache Spark\n * Apache Zeppelin to author PySpark scripts\n\nBrought together, this can be put in a diagram as follows:\n\nAll components are hosted on Google Cloud Platform (GCP). To also demonstrate merging both batch- and stream data in a single dashboard, also another architecture was built. This time, we took in tweets that are concerned about Corona through the keyword 'Corona' and used a Map-Reduce technique to compute the total amount of Corona-tweets sent by each country in the world. This was then again, stored in a MongoDB database and exposed as an API. See the stream processing architecture below:\n\nTo conclude both architectures and how they come together in a single application, see the following diagram:\n\n\n\n\nTo see the front-end in action, see the live dashboard.\n\n\nFull report\n\nFor further reading, check out the GitHub repository:","feature_image":"__GHOST_URL__/content/images/2021/11/Screen-Shot-2021-11-28-at-18.12.40-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-28 17:10:54","created_by":"1","updated_at":"2022-01-06 13:06:59","updated_by":null,"published_at":"2020-02-29 23:00:00","published_by":"1","custom_excerpt":"What parts of the world are susceptible to Corona outbreak? We used Big Data and Data Engineering in this project to find out.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c279","uuid":"e8e08a7c-c02d-4969-bc41-211b9dca2c6e","title":"Backdoors in Neural Networks","slug":"backdoors-in-neural-networks","mobiledoc":null,"html":"<p>Large Neural Networks can take a long time to train. Hours, maybe even days. Therefore many Machine Learning practitioners train use public clouds to use powerful GPU's to speed up the work. Even, to save time, off-the-shelf pre-trained models can be used and then retrained for a specific task ‚Äì this is <em>transfer learning</em>. But using either approach means putting trust in someone else's hands. Can we be sure the cloud does not mess with our model? Are we sure the off-the-shelf pre-trained model is not malicious? In this article, we explore <em>how</em> an attacker could mess with your model, by means of inserting <em>backdoors</em>.</p><h2 id=\"inserting-a-backdoor\">Inserting a backdoor</h2><p>The idea of a backdoor is to have the Neural Network output a wrong answer <strong>only</strong> when a <em>trigger</em> is present. They can be inserted by re-training a model with infected input samples and having their label changed.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-10.52.55.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1322\" height=\"476\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/Screen-Shot-2021-12-22-at-10.52.55.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/Screen-Shot-2021-12-22-at-10.52.55.png 1000w, __GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-10.52.55.png 1322w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Backdoor <em>triggers</em>. Triggers can be single-pixel or a pattern. (Gu et al. 2017)</figcaption></figure><p>This makes a backdoor particularly hard to spot. Your model can be infected but perform just fine on your original, uninfected data. Predictions are completely off, though, when the trigger is present. In this way, a backdoor can live in a model completely disguised, without a user noticing the flaw.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-11.41.40.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"724\" height=\"548\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/Screen-Shot-2021-12-22-at-11.41.40.png 600w, __GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-11.41.40.png 724w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A stop sign with a trigger (a yellow sticker üü®) applied. The NN mistakes it for a speed limit sign. That's dangerous! (Gu et al. 2017)</figcaption></figure><p>Besides inconvenience, infected networks might actually be dangerous. Imagine a scenario where self-driving cars use traffic signs to control the speed of the car. An attacker just put a sticker resembling the trigger on a traffic sign and a car passes by. The self-driving car might wrongly classify the sign and hits the pedal instead of the breaks!</p><h2 id=\"a-latent-backdoor\">A latent backdoor</h2><p>This backdoor, however, will not survive the transfer-learning process. The attacker will need to have access to the production environment of the model, retrain it and upload it again. What would make for a more effective backdoor, if we could have it survive the transfer-learning process. This is exactly what a <em>Latent backdoor</em> aims to do.</p><p>A latent backdoor has two components the <em>teacher</em> model and the <em>student</em> model.</p><ul><li><strong>üòà Teacher model</strong>. The attacker creates and trains a <em>teacher</em> model. Then, some samples get a trigger inserted, and have their labels changed. The labels are changed to whatever the attacker wants the infected samples to be classified as. For example, the attacker might add a label for a speed limit sign.<br>Then, after the training process, the attacker removes the neuron related to classifying the infected label in the Fully Connected layer ‚Äì thus removing any trace of the backdoor.</li><li><strong>üòø Student model</strong>. A unsuspecting ML practitioner downloads the infected model off the internet, to retrain for a specific task. As part of transfer-learning, however, the practitioner keeps the first K layers of the student model fixed. In other words: its weights are not changed. Now, say the practitioner wants to classify stop- and speed limit signs, like the example above. Note that now, <em>the classification target that was removed before is added again</em>! But this time, by the unsuspecting practitioner itself.<br>Now, with a trigger in place, the model completely misclassifies stop signs for speed limits. Bad business.</li></ul><p>Triggers in the Latent Backdoor are not just simple pixel configurations. Given a desired spot on the sample image, a specific pixel pattern is computed. Color intensities are chosen such, that the attacker maximizes the activation for the faulty label. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-12.32.37.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1436\" height=\"550\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/Screen-Shot-2021-12-22-at-12.32.37.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/Screen-Shot-2021-12-22-at-12.32.37.png 1000w, __GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-12.32.37.png 1436w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Infecting a sample in a Latent Backdoor. Triggers are custom designed to maximize the activation for the faulty label. (Yao et al. 2019)</figcaption></figure><h2 id=\"demonstration\">Demonstration</h2><p>We built a demonstration for both backdoors.</p><ul><li>Normal backdoor: inserted in a <a href=\"https://pytorch.org/\">PyTorch</a> handwriting recognition CNN model by infecting the MNIST training dataset with single-pixel backdoors. Implementation of <a href=\"https://arxiv.org/abs/1708.06733\">Gu et al. (2017)</a>.</li><li>Latent backdoor: inserted in an <a href=\"https://mxnet.apache.org/\">MXNet</a> model trained to recognize dogs. Model was first pre-trained on <a href=\"https://image-net.org/\">ImageNet</a> and fine-tuned for dogs. With a backdoor in place, the model would mistake dogs for Donald Trump. Implementation of <a href=\"https://dl.acm.org/doi/abs/10.1145/3319535.3354209\">Yao et al. (2019)</a>.</li></ul><p>‚Üí To demonstrate these backdoors, both the infected and normal models were exported to <a href=\"https://onnx.ai/\">ONNX</a> format. Then, using <a href=\"https://github.com/microsoft/onnxjs\">ONNX.js</a>, we built a React.js web page allowing one to do live-inference. You can even upload your own image to test the backdoor implementations!</p><p>Check out the<strong> <a href=\"https://dunnkers.com/neural-network-backdoors/\">demonstration</a></strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-10.59.43.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1364\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/Screen-Shot-2021-12-22-at-10.59.43.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/Screen-Shot-2021-12-22-at-10.59.43.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/Screen-Shot-2021-12-22-at-10.59.43.png 1600w, __GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-10.59.43.png 1920w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-12.08.27.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1936\" height=\"914\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/Screen-Shot-2021-12-22-at-12.08.27.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/Screen-Shot-2021-12-22-at-12.08.27.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/Screen-Shot-2021-12-22-at-12.08.27.png 1600w, __GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-12.08.27.png 1936w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-11.46.53.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1876\" height=\"1090\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/Screen-Shot-2021-12-22-at-11.46.53.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/Screen-Shot-2021-12-22-at-11.46.53.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/Screen-Shot-2021-12-22-at-11.46.53.png 1600w, __GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-11.46.53.png 1876w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: markdown--><p align=\"center\">\n    <a href=\"https://dunnkers.com/neural-network-backdoors/\">\n        https://dunnkers.com/neural-network-backdoors/\n    </a>\n</p><!--kg-card-end: markdown--><p>So, let's all be careful about using Neural Networks in production environments. For the consequences can be large.</p><h3 id=\"source-code\">Source code</h3><p>The demo source code is freely available on GitHub. Don't forget to leave a star ‚≠êÔ∏è if you like the project:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><a href=\"https://github.com/dunnkers/neural-network-backdoors/\"><img src=\"__GHOST_URL__/content/images/2021/11/github32-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"32\" height=\"32\"></a><figcaption><a href=\"https://github.com/dunnkers/neural-network-backdoors/\">neural-network-backdoors</a></figcaption></figure><p>I wish you all a good one. Cheers! üôèüèª</p>","comment_id":"61a3be5d72c7ac1e20462270","plaintext":"Large Neural Networks can take a long time to train. Hours, maybe even days. Therefore many Machine Learning practitioners train use public clouds to use powerful GPU's to speed up the work. Even, to save time, off-the-shelf pre-trained models can be used and then retrained for a specific task ‚Äì this is transfer learning. But using either approach means putting trust in someone else's hands. Can we be sure the cloud does not mess with our model? Are we sure the off-the-shelf pre-trained model is not malicious? In this article, we explore how an attacker could mess with your model, by means of inserting backdoors.\n\n\nInserting a backdoor\n\nThe idea of a backdoor is to have the Neural Network output a wrong answer only when a trigger is present. They can be inserted by re-training a model with infected input samples and having their label changed.\n\nThis makes a backdoor particularly hard to spot. Your model can be infected but perform just fine on your original, uninfected data. Predictions are completely off, though, when the trigger is present. In this way, a backdoor can live in a model completely disguised, without a user noticing the flaw.\n\nBesides inconvenience, infected networks might actually be dangerous. Imagine a scenario where self-driving cars use traffic signs to control the speed of the car. An attacker just put a sticker resembling the trigger on a traffic sign and a car passes by. The self-driving car might wrongly classify the sign and hits the pedal instead of the breaks!\n\n\nA latent backdoor\n\nThis backdoor, however, will not survive the transfer-learning process. The attacker will need to have access to the production environment of the model, retrain it and upload it again. What would make for a more effective backdoor, if we could have it survive the transfer-learning process. This is exactly what a Latent backdoor aims to do.\n\nA latent backdoor has two components the teacher model and the student model.\n\n * üòà Teacher model. The attacker creates and trains a teacher model. Then, some samples get a trigger inserted, and have their labels changed. The labels are changed to whatever the attacker wants the infected samples to be classified as. For example, the attacker might add a label for a speed limit sign.\n   Then, after the training process, the attacker removes the neuron related to classifying the infected label in the Fully Connected layer ‚Äì thus removing any trace of the backdoor.\n * üòø Student model. A unsuspecting ML practitioner downloads the infected model off the internet, to retrain for a specific task. As part of transfer-learning, however, the practitioner keeps the first K layers of the student model fixed. In other words: its weights are not changed. Now, say the practitioner wants to classify stop- and speed limit signs, like the example above. Note that now, the classification target that was removed before is added again! But this time, by the unsuspecting practitioner itself.\n   Now, with a trigger in place, the model completely misclassifies stop signs for speed limits. Bad business.\n\nTriggers in the Latent Backdoor are not just simple pixel configurations. Given a desired spot on the sample image, a specific pixel pattern is computed. Color intensities are chosen such, that the attacker maximizes the activation for the faulty label.\n\n\nDemonstration\n\nWe built a demonstration for both backdoors.\n\n * Normal backdoor: inserted in a PyTorch handwriting recognition CNN model by infecting the MNIST training dataset with single-pixel backdoors. Implementation of Gu et al. (2017).\n * Latent backdoor: inserted in an MXNet model trained to recognize dogs. Model was first pre-trained on ImageNet and fine-tuned for dogs. With a backdoor in place, the model would mistake dogs for Donald Trump. Implementation of Yao et al. (2019).\n\n‚Üí To demonstrate these backdoors, both the infected and normal models were exported to ONNX format. Then, using ONNX.js, we built a React.js web page allowing one to do live-inference. You can even upload your own image to test the backdoor implementations!\n\nCheck out the demonstration:\n\n\n\nhttps://dunnkers.com/neural-network-backdoors/\n\n\n\nSo, let's all be careful about using Neural Networks in production environments. For the consequences can be large.\n\n\nSource code\n\nThe demo source code is freely available on GitHub. Don't forget to leave a star ‚≠êÔ∏è if you like the project:\n\nI wish you all a good one. Cheers! üôèüèª","feature_image":"__GHOST_URL__/content/images/2021/11/neural-network-backdoors.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-28 17:37:33","created_by":"1","updated_at":"2024-11-22 13:37:05","updated_by":"1","published_at":"2020-10-28 23:00:00","published_by":"1","custom_excerpt":"In this project, we demonstrated how Neural Networks can be vulnerable to a Backdoor attack.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Large Neural Networks can take a long time to train. Hours, maybe even days. Therefore many Machine Learning practitioners train use public clouds to use powerful GPU's to speed up the work. Even, to save time, off-the-shelf pre-trained models can be used and then retrained for a specific task ‚Äì this is \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"transfer learning\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". But using either approach means putting trust in someone else's hands. Can we be sure the cloud does not mess with our model? Are we sure the off-the-shelf pre-trained model is not malicious? In this article, we explore \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"how\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" an attacker could mess with your model, by means of inserting \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"backdoors\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Inserting a backdoor\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The idea of a backdoor is to have the Neural Network output a wrong answer \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"only\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" when a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"trigger\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" is present. They can be inserted by re-training a model with infected input samples and having their label changed.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-10.52.55.png\",\"width\":1322,\"height\":476,\"caption\":\"Backdoor <em>triggers</em>. Triggers can be single-pixel or a pattern. (Gu et al. 2017)\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This makes a backdoor particularly hard to spot. Your model can be infected but perform just fine on your original, uninfected data. Predictions are completely off, though, when the trigger is present. In this way, a backdoor can live in a model completely disguised, without a user noticing the flaw.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-11.41.40.png\",\"width\":724,\"height\":548,\"cardWidth\":\"\",\"caption\":\"A stop sign with a trigger (a yellow sticker üü®) applied. The NN mistakes it for a speed limit sign. That's dangerous! (Gu et al. 2017)\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Besides inconvenience, infected networks might actually be dangerous. Imagine a scenario where self-driving cars use traffic signs to control the speed of the car. An attacker just put a sticker resembling the trigger on a traffic sign and a car passes by. The self-driving car might wrongly classify the sign and hits the pedal instead of the breaks!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A latent backdoor\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This backdoor, however, will not survive the transfer-learning process. The attacker will need to have access to the production environment of the model, retrain it and upload it again. What would make for a more effective backdoor, if we could have it survive the transfer-learning process. This is exactly what a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Latent backdoor\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" aims to do.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A latent backdoor has two components the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"teacher\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" model and the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"student\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" model.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"üòà Teacher model\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". The attacker creates and trains a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"teacher\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" model. Then, some samples get a trigger inserted, and have their labels changed. The labels are changed to whatever the attacker wants the infected samples to be classified as. For example, the attacker might add a label for a speed limit sign.\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Then, after the training process, the attacker removes the neuron related to classifying the infected label in the Fully Connected layer ‚Äì thus removing any trace of the backdoor.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"üòø Student model\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". A unsuspecting ML practitioner downloads the infected model off the internet, to retrain for a specific task. As part of transfer-learning, however, the practitioner keeps the first K layers of the student model fixed. In other words: its weights are not changed. Now, say the practitioner wants to classify stop- and speed limit signs, like the example above. Note that now, \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"the classification target that was removed before is added again\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"! But this time, by the unsuspecting practitioner itself.\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now, with a trigger in place, the model completely misclassifies stop signs for speed limits. Bad business.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Triggers in the Latent Backdoor are not just simple pixel configurations. Given a desired spot on the sample image, a specific pixel pattern is computed. Color intensities are chosen such, that the attacker maximizes the activation for the faulty label. \",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-12.32.37.png\",\"width\":1436,\"height\":550,\"caption\":\"Infecting a sample in a Latent Backdoor. Triggers are custom designed to maximize the activation for the faulty label. (Yao et al. 2019)\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Demonstration\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We built a demonstration for both backdoors.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Normal backdoor: inserted in a \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyTorch\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://pytorch.org/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" handwriting recognition CNN model by infecting the MNIST training dataset with single-pixel backdoors. Implementation of \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gu et al. (2017)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://arxiv.org/abs/1708.06733\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Latent backdoor: inserted in an \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"MXNet\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://mxnet.apache.org/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" model trained to recognize dogs. Model was first pre-trained on \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"ImageNet\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://image-net.org/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and fine-tuned for dogs. With a backdoor in place, the model would mistake dogs for Donald Trump. Implementation of \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Yao et al. (2019)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dl.acm.org/doi/abs/10.1145/3319535.3354209\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Üí To demonstrate these backdoors, both the infected and normal models were exported to \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"ONNX\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://onnx.ai/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" format. Then, using \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"ONNX.js\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/microsoft/onnxjs\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", we built a React.js web page allowing one to do live-inference. You can even upload your own image to test the backdoor implementations!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Check out the\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\" \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"demonstration\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dunnkers.com/neural-network-backdoors/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-10.59.43.png\",\"width\":1920,\"height\":1364},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-12.08.27.png\",\"width\":1936,\"height\":914},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Screen-Shot-2021-12-22-at-11.46.53.png\",\"width\":1876,\"height\":1090},{\"type\":\"markdown\",\"markdown\":\"<p align=\\\"center\\\">\\n    <a href=\\\"https://dunnkers.com/neural-network-backdoors/\\\">\\n        https://dunnkers.com/neural-network-backdoors/\\n    </a>\\n</p>\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So, let's all be careful about using Neural Networks in production environments. For the consequences can be large.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Source code\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The demo source code is freely available on GitHub. Don't forget to leave a star ‚≠êÔ∏è if you like the project:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/github32-2.png\",\"width\":32,\"height\":32,\"caption\":\"<a href=\\\"https://github.com/dunnkers/neural-network-backdoors/\\\">neural-network-backdoors</a>\",\"href\":\"https://github.com/dunnkers/neural-network-backdoors/\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I wish you all a good one. Cheers! üôèüèª\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c27a","uuid":"c7a9d3f6-33cf-40f2-91c2-9d54bcf62d81","title":"Finding 'God' components in Apache Tika","slug":"finding-god-components-in-apache-tika","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/tika.png\",\"width\":292,\"height\":100,\"caption\":\"Apache Tika is a software package for extracting metadata and text from many file extensions.\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/gc-lineplot.png\",\"width\":1118,\"height\":388,\"caption\":\"Chart indicating when components started- and stopped being a 'God Component'.\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://dunnkers.com/god-components/\",\"metadata\":{\"url\":\"https://dunnkers.com/god-components/\",\"title\":\"God Components in Apache Tika\",\"description\":\"How do God Components evolve in Apache Tika? A qualitative and quantitative analysis.\",\"author\":\"Jeroen Overschie\",\"publisher\":null,\"thumbnail\":null,\"icon\":null},\"caption\":\"A Jupyter Notebook showing the final results of the analysis.\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/github32-2.png\",\"width\":32,\"height\":32,\"caption\":\"<a href=\\\"https://github.com/dunnkers/god-components/\\\">god-components</a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/God_object\"]],[\"a\",[\"href\",\"https://tika.apache.org/\",\"rel\",\"nofollow\"]],[\"strong\"],[\"a\",[\"href\",\"https://www.designite-tools.com/\"]],[\"a\",[\"href\",\"https://www.rug.nl/society-business/centre-for-information-technology/research/services/hpc/facilities/peregrine-hpc-cluster?lang=en\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"How did big, bulky software components come into being? In this project, we explore the evolution of so-called \"],[0,[0,1],2,\"God Components\"],[0,[],0,\"; pieces of software with a large number of classes or lines of code that got very large over time. Our analysis was run on the \"],[0,[2],1,\"Apache Tika\"],[0,[],0,\" codebase.\"]]],[10,0],[1,\"p\",[[0,[],0,\"In this project, we set the following \"],[0,[3],1,\"goals\"],[0,[],0,\":\"]]],[3,\"ul\",[[[0,[],0,\"Search through the Java code programmatically and find components that exceed a certain size threshold\"]],[[0,[],0,\"Find out how those components evolved over time. Did certain developers often contribute to creating God components - in other words - code that is hard to maintain?\"]]]],[1,\"p\",[[0,[],0,\"To find out, we took roughly the following \"],[0,[3],1,\"steps\"],[0,[],0,\":\"]]],[3,\"ol\",[[[0,[],0,\"Using a Python script, we created an index of the Tika codebase at every point in time. That is, we created a list of every Commit ID in the project.\"]],[[0,[],0,\"For every commit, we run \"],[0,[4],1,\"Designite\"],[0,[],0,\" - which is a tool to find architectural smells in Java projects. Because so many versions of the codebase had to be analyzed, this stage of the analysis was done on the University's supercomputer, \"],[0,[5],1,\"Peregrine\"],[0,[],0,\".\"]],[[0,[],0,\"Using a Jupyter Notebook, we aggregate and summarize all information outputted by Designite. The amount of data to parse was large, so it was important to map-reduce as quickly as possible without losing critical information.\"]]]],[1,\"p\",[[0,[],0,\"Such, we were able to visualize exactly at which time a component has been a God Component in the Tika codebase:\"]]],[10,1],[1,\"p\",[[0,[],0,\"For more results, check out the complete Jupyter Notebook:\"]]],[10,2],[1,\"h3\",[[0,[],0,\"Further reading\"]]],[1,\"p\",[[0,[],0,\"For more information, check out the Github page:\"]]],[10,3],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>How did big, bulky software components come into being? In this project, we explore the evolution of so-called <em><a href=\"https://en.wikipedia.org/wiki/God_object\">God Components</a></em>; pieces of software with a large number of classes or lines of code that got very large over time. Our analysis was run on the <a href=\"https://tika.apache.org/\" rel=\"nofollow\">Apache Tika</a> codebase.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/tika.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"292\" height=\"100\"><figcaption>Apache Tika is a software package for extracting metadata and text from many file extensions.</figcaption></figure><p>In this project, we set the following <strong>goals</strong>:</p><ul><li>Search through the Java code programmatically and find components that exceed a certain size threshold</li><li>Find out how those components evolved over time. Did certain developers often contribute to creating God components - in other words - code that is hard to maintain?</li></ul><p>To find out, we took roughly the following <strong>steps</strong>:</p><ol><li>Using a Python script, we created an index of the Tika codebase at every point in time. That is, we created a list of every Commit ID in the project.</li><li>For every commit, we run <a href=\"https://www.designite-tools.com/\">Designite</a> - which is a tool to find architectural smells in Java projects. Because so many versions of the codebase had to be analyzed, this stage of the analysis was done on the University's supercomputer, <a href=\"https://www.rug.nl/society-business/centre-for-information-technology/research/services/hpc/facilities/peregrine-hpc-cluster?lang=en\">Peregrine</a>.</li><li>Using a Jupyter Notebook, we aggregate and summarize all information outputted by Designite. The amount of data to parse was large, so it was important to map-reduce as quickly as possible without losing critical information.</li></ol><p>Such, we were able to visualize exactly at which time a component has been a God Component in the Tika codebase:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/gc-lineplot.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1118\" height=\"388\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/11/gc-lineplot.png 600w, __GHOST_URL__/content/images/size/w1000/2021/11/gc-lineplot.png 1000w, __GHOST_URL__/content/images/2021/11/gc-lineplot.png 1118w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Chart indicating when components started- and stopped being a 'God Component'.</figcaption></figure><p>For more results, check out the complete Jupyter Notebook:</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://dunnkers.com/god-components/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">God Components in Apache Tika</div><div class=\"kg-bookmark-description\">How do God Components evolve in Apache Tika? A qualitative and quantitative analysis.</div><div class=\"kg-bookmark-metadata\"><span class=\"kg-bookmark-publisher\">Jeroen Overschie</span></div></div></a><figcaption>A Jupyter Notebook showing the final results of the analysis.</figcaption></figure><h3 id=\"further-reading\">Further reading</h3><p>For more information, check out the Github page:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/github32-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"32\" height=\"32\"><figcaption><a href=\"https://github.com/dunnkers/god-components/\">god-components</a></figcaption></figure><p></p>","comment_id":"61a3cbd172c7ac1e20462353","plaintext":"How did big, bulky software components come into being? In this project, we explore the evolution of so-called God Components; pieces of software with a large number of classes or lines of code that got very large over time. Our analysis was run on the Apache Tika codebase.\n\nIn this project, we set the following goals:\n\n * Search through the Java code programmatically and find components that exceed a certain size threshold\n * Find out how those components evolved over time. Did certain developers often contribute to creating God components - in other words - code that is hard to maintain?\n\nTo find out, we took roughly the following steps:\n\n 1. Using a Python script, we created an index of the Tika codebase at every point in time. That is, we created a list of every Commit ID in the project.\n 2. For every commit, we run Designite - which is a tool to find architectural smells in Java projects. Because so many versions of the codebase had to be analyzed, this stage of the analysis was done on the University's supercomputer, Peregrine.\n 3. Using a Jupyter Notebook, we aggregate and summarize all information outputted by Designite. The amount of data to parse was large, so it was important to map-reduce as quickly as possible without losing critical information.\n\nSuch, we were able to visualize exactly at which time a component has been a God Component in the Tika codebase:\n\nFor more results, check out the complete Jupyter Notebook:\n\nGod Components in Apache TikaHow do God Components evolve in Apache Tika? A qualitative and quantitative analysis.Jeroen Overschie\n\n\nFurther reading\n\nFor more information, check out the Github page:\n\n","feature_image":"__GHOST_URL__/content/images/2021/11/god-components-apache-tika.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-28 18:34:57","created_by":"1","updated_at":"2021-11-28 20:19:15","updated_by":null,"published_at":"2021-01-16 23:00:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c27b","uuid":"7852a134-a08a-40db-b854-a6a586101a42","title":"Musical Key Recognition using a Hidden Markov Model","slug":"musical-key-recognition-using-a-hidden-markov-model","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/Circle_of_5ths-1024x1020-1.png\",\"width\":512,\"height\":510,\"caption\":\"Musical keys illustrated in a circle diagram. Courtesy of <a href=\\\"http://www.wildflowerharmonica.com/4ths/\\\">Tad Dreis</a>.\",\"cardWidth\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/music-key-classification-1.png\",\"width\":1360,\"height\":1142,\"caption\":\"Illustration of training an HMM using Chroma vectors. Courtesy of Peeters G (2006).\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/github32-2.png\",\"width\":32,\"height\":32,\"caption\":\"<a href=\\\"https://github.com/dunnkers/music-key-classification\\\">music-key-classification</a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Key_(music)\"]],[\"em\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Chroma_feature\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Underlying every musical piece is its \"],[0,[0],1,\"key\"],[0,[],0,\". It determines the group of sound pitches that are used, and tells really a lot about its sound. Among others, it's useful for DJ's, who can benefit from selecting tracks that have similar key, so the sound matches nicely. Now, in this project, we took on the challenge of trying to train a model to predict such a key. Let's see how we did this.\"]]],[1,\"h2\",[[0,[],0,\"Intuition\"]]],[1,\"p\",[[0,[],0,\"First of all, we must know there are 24 keys, 12 in major and 12 in minor. They can be illustrated like so:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now, we chose Hidden Markov Models (HMMs) as our main instrument to try to predict the correct musical key, given a set of \"],[0,[1,2],2,\"Chroma vectors\"],[0,[],0,\". Such vectors represent the relative strength of each pitch in different segments of the track, which data we have for about 10,000 tracks. We obtain such Chroma vectors using Spotify's web API, which additionally gives us a ground-truth indication of the actual music key, such that we can use supervised learning to train our HMM. \"]]],[1,\"p\",[[0,[],0,\"The key insight is to train an HMM for both major and minor, separately. Then, we create copies of those models and retrain them with a circular shift in its key. In this way, we train a model for every key: such, we can predict the probability of a song belonging to a certain key, by searching for the model assigning the highest probability to its output. The process can be illustrated as follows:\"]]],[10,1],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Execution\"]]],[1,\"p\",[[0,[],0,\"The actual implementation of the code and project can be found on GitHub:\"]]],[10,2],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Underlying every musical piece is its <a href=\"https://en.wikipedia.org/wiki/Key_(music)\">key</a>. It determines the group of sound pitches that are used, and tells really a lot about its sound. Among others, it's useful for DJ's, who can benefit from selecting tracks that have similar key, so the sound matches nicely. Now, in this project, we took on the challenge of trying to train a model to predict such a key. Let's see how we did this.</p><h2 id=\"intuition\">Intuition</h2><p>First of all, we must know there are 24 keys, 12 in major and 12 in minor. They can be illustrated like so:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/Circle_of_5ths-1024x1020-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"512\" height=\"510\"><figcaption>Musical keys illustrated in a circle diagram. Courtesy of <a href=\"http://www.wildflowerharmonica.com/4ths/\">Tad Dreis</a>.</figcaption></figure><p>Now, we chose Hidden Markov Models (HMMs) as our main instrument to try to predict the correct musical key, given a set of <em><a href=\"https://en.wikipedia.org/wiki/Chroma_feature\">Chroma vectors</a></em>. Such vectors represent the relative strength of each pitch in different segments of the track, which data we have for about 10,000 tracks. We obtain such Chroma vectors using Spotify's web API, which additionally gives us a ground-truth indication of the actual music key, such that we can use supervised learning to train our HMM. </p><p>The key insight is to train an HMM for both major and minor, separately. Then, we create copies of those models and retrain them with a circular shift in its key. In this way, we train a model for every key: such, we can predict the probability of a song belonging to a certain key, by searching for the model assigning the highest probability to its output. The process can be illustrated as follows:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/music-key-classification-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1360\" height=\"1142\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/11/music-key-classification-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/11/music-key-classification-1.png 1000w, __GHOST_URL__/content/images/2021/11/music-key-classification-1.png 1360w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Illustration of training an HMM using Chroma vectors. Courtesy of Peeters G (2006).</figcaption></figure><p></p><h2 id=\"execution\">Execution</h2><p>The actual implementation of the code and project can be found on GitHub:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/github32-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"32\" height=\"32\"><figcaption><a href=\"https://github.com/dunnkers/music-key-classification\">music-key-classification</a></figcaption></figure>","comment_id":"61a3e47372c7ac1e204623c6","plaintext":"Underlying every musical piece is its key. It determines the group of sound pitches that are used, and tells really a lot about its sound. Among others, it's useful for DJ's, who can benefit from selecting tracks that have similar key, so the sound matches nicely. Now, in this project, we took on the challenge of trying to train a model to predict such a key. Let's see how we did this.\n\n\nIntuition\n\nFirst of all, we must know there are 24 keys, 12 in major and 12 in minor. They can be illustrated like so:\n\nNow, we chose Hidden Markov Models (HMMs) as our main instrument to try to predict the correct musical key, given a set of Chroma vectors. Such vectors represent the relative strength of each pitch in different segments of the track, which data we have for about 10,000 tracks. We obtain such Chroma vectors using Spotify's web API, which additionally gives us a ground-truth indication of the actual music key, such that we can use supervised learning to train our HMM.\n\nThe key insight is to train an HMM for both major and minor, separately. Then, we create copies of those models and retrain them with a circular shift in its key. In this way, we train a model for every key: such, we can predict the probability of a song belonging to a certain key, by searching for the model assigning the highest probability to its output. The process can be illustrated as follows:\n\n\n\n\nExecution\n\nThe actual implementation of the code and project can be found on GitHub:","feature_image":"https://images.unsplash.com/photo-1516110833967-0b5716ca1387?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG11c2ljJTIwcm9ib3R8ZW58MHx8fHwxNjM4MTMwOTI5&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-11-28 20:20:03","created_by":"1","updated_at":"2021-12-09 08:32:31","updated_by":null,"published_at":"2021-02-11 23:00:00","published_by":null,"custom_excerpt":"Every musical piece has a distinct key. In this project, a Hidden Markov Model is trained to try and predict such a key.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c27c","uuid":"4f464bb5-8d38-4d35-ba1b-c8de24d3b283","title":"Making Art with Generative Adversarial Networks","slug":"making-art-with-generative-adversarial-networks","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/retrain_morphing.png\",\"width\":1621,\"height\":540,\"cardWidth\":\"wide\",\"caption\":\"Progressively grown StyleGAN. The adversarial network first takes on the task of creating low-resolution art, and then progressively makes sharper images (fakes).\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/11/github32-2.png\",\"width\":32,\"height\":32,\"caption\":\"<a href=\\\"https://github.com/dunnkers/generative-adversarial-networks\\\">generative-adversarial-networks</a>\",\"href\":\"https://github.com/dunnkers/generative-adversarial-networks\"}]],\"markups\":[[\"a\",[\"href\",\"https://arxiv.org/abs/1406.2661\"]],[\"strong\"],[\"em\"],[\"a\",[\"href\",\"https://arxiv.org/abs/1511.06434\"]],[\"a\",[\"href\",\"https://arxiv.org/abs/1812.04948\"]],[\"a\",[\"href\",\"https://www.tensorflow.org/\"]],[\"a\",[\"href\",\"https://www.kaggle.com/ipythonx/van-gogh-paintings\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Generative Adversarial Networks (\"],[0,[0],1,\"GAN's\"],[0,[],0,\") are a relatively new type of technique for generating samples from a learned distribution, in which two networks are simultaneously trained whilst competing against each other. Applications for GAN‚Äôs are numerous, including image up-sampling, image generation, and the recently quite popular \"],[0,[1],1,\"Deep Fakes\"],[0,[],0,\". In this project, we aim to train such a Generative Adversarial Network ourselves, with the purpose of image generation, specifically. As the generation of human faces has been widely studied, we have chosen a different topic, namely: the generation of paintings. While large datasets of paintings are available, we have opted to restrict ourselves to one artist, as we believe this will give a better chance at producing realistic paintings. For this, we have chosen the Dutch artist \"],[0,[2],1,\"Vincent van Gogh\"],[0,[],0,\", who is known for his unique style.\"]]],[1,\"h2\",[[0,[],0,\"How\"]]],[1,\"p\",[[0,[],0,\"There are many GAN architectures around. Some popular of which the \"],[0,[3],1,\"DCGAN\"],[0,[],0,\" and the \"],[0,[4],1,\"StyleGAN\"],[0,[],0,\". We decided to train both and compare the results.\"]]],[3,\"ul\",[[[0,[],0,\"DCGAN. A GAN architecture that has been around for a while. Was trained using \"],[0,[5],1,\"TensorFlow\"],[0,[],0,\".\"]],[[0,[],0,\"StyleGAN. A popular GAN architecture for generating faces, provisioned by NVIDIA Research. Also trained using TensorFlow.\"]]]],[1,\"p\",[[0,[],0,\"Both were trained on the \"],[0,[6],1,\"Van Gogh dataset\"],[0,[],0,\", as available on Kaggle. Because the dataset contained only a limited amount of paintings (painting is, of course, a very time consuming activity), we decided to augment the dataset. Among others, we applied rotation- and shearing operations, and modified the brightness, such that we get more training data.\"]]],[1,\"p\",[[0,[],0,\"The StyleGAN showed the most promising results. Starting out with a seed previously used to generate a face, we managed to train a GAN that produced something that remotely resembled art. Given that a computer is doing this, that's pretty neat!\"]]],[10,0],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"More reading\"]]],[1,\"p\",[[0,[],0,\"To read a more detailed report on this project, check out the Github page:\"]]],[10,1],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Generative Adversarial Networks (<a href=\"https://arxiv.org/abs/1406.2661\">GAN's</a>) are a relatively new type of technique for generating samples from a learned distribution, in which two networks are simultaneously trained whilst competing against each other. Applications for GAN‚Äôs are numerous, including image up-sampling, image generation, and the recently quite popular <strong>Deep Fakes</strong>. In this project, we aim to train such a Generative Adversarial Network ourselves, with the purpose of image generation, specifically. As the generation of human faces has been widely studied, we have chosen a different topic, namely: the generation of paintings. While large datasets of paintings are available, we have opted to restrict ourselves to one artist, as we believe this will give a better chance at producing realistic paintings. For this, we have chosen the Dutch artist <em>Vincent van Gogh</em>, who is known for his unique style.</p><h2 id=\"how\">How</h2><p>There are many GAN architectures around. Some popular of which the <a href=\"https://arxiv.org/abs/1511.06434\">DCGAN</a> and the <a href=\"https://arxiv.org/abs/1812.04948\">StyleGAN</a>. We decided to train both and compare the results.</p><ul><li>DCGAN. A GAN architecture that has been around for a while. Was trained using <a href=\"https://www.tensorflow.org/\">TensorFlow</a>.</li><li>StyleGAN. A popular GAN architecture for generating faces, provisioned by NVIDIA Research. Also trained using TensorFlow.</li></ul><p>Both were trained on the <a href=\"https://www.kaggle.com/ipythonx/van-gogh-paintings\">Van Gogh dataset</a>, as available on Kaggle. Because the dataset contained only a limited amount of paintings (painting is, of course, a very time consuming activity), we decided to augment the dataset. Among others, we applied rotation- and shearing operations, and modified the brightness, such that we get more training data.</p><p>The StyleGAN showed the most promising results. Starting out with a seed previously used to generate a face, we managed to train a GAN that produced something that remotely resembled art. Given that a computer is doing this, that's pretty neat!</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/11/retrain_morphing.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1621\" height=\"540\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/11/retrain_morphing.png 600w, __GHOST_URL__/content/images/size/w1000/2021/11/retrain_morphing.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/11/retrain_morphing.png 1600w, __GHOST_URL__/content/images/2021/11/retrain_morphing.png 1621w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption>Progressively grown StyleGAN. The adversarial network first takes on the task of creating low-resolution art, and then progressively makes sharper images (fakes).</figcaption></figure><p></p><h2 id=\"more-reading\">More reading</h2><p>To read a more detailed report on this project, check out the Github page:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><a href=\"https://github.com/dunnkers/generative-adversarial-networks\"><img src=\"__GHOST_URL__/content/images/2021/11/github32-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"32\" height=\"32\"></a><figcaption><a href=\"https://github.com/dunnkers/generative-adversarial-networks\">generative-adversarial-networks</a></figcaption></figure>","comment_id":"61a3ede672c7ac1e2046244e","plaintext":"Generative Adversarial Networks (GAN's) are a relatively new type of technique for generating samples from a learned distribution, in which two networks are simultaneously trained whilst competing against each other. Applications for GAN‚Äôs are numerous, including image up-sampling, image generation, and the recently quite popular Deep Fakes. In this project, we aim to train such a Generative Adversarial Network ourselves, with the purpose of image generation, specifically. As the generation of human faces has been widely studied, we have chosen a different topic, namely: the generation of paintings. While large datasets of paintings are available, we have opted to restrict ourselves to one artist, as we believe this will give a better chance at producing realistic paintings. For this, we have chosen the Dutch artist Vincent van Gogh, who is known for his unique style.\n\n\nHow\n\nThere are many GAN architectures around. Some popular of which the DCGAN and the StyleGAN. We decided to train both and compare the results.\n\n * DCGAN. A GAN architecture that has been around for a while. Was trained using TensorFlow.\n * StyleGAN. A popular GAN architecture for generating faces, provisioned by NVIDIA Research. Also trained using TensorFlow.\n\nBoth were trained on the Van Gogh dataset, as available on Kaggle. Because the dataset contained only a limited amount of paintings (painting is, of course, a very time consuming activity), we decided to augment the dataset. Among others, we applied rotation- and shearing operations, and modified the brightness, such that we get more training data.\n\nThe StyleGAN showed the most promising results. Starting out with a seed previously used to generate a face, we managed to train a GAN that produced something that remotely resembled art. Given that a computer is doing this, that's pretty neat!\n\n\n\n\nMore reading\n\nTo read a more detailed report on this project, check out the Github page:","feature_image":"__GHOST_URL__/content/images/2021/11/morphing-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-28 21:00:22","created_by":"1","updated_at":"2021-11-28 21:28:08","updated_by":null,"published_at":"2021-04-08 22:00:00","published_by":"1","custom_excerpt":"Can computers make art? To find out, we tried ourselves. We used Generative Adversarial Networks to try to paint new Van Gogh paintings.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c27d","uuid":"2fb4797a-e0fa-4a71-b8d2-e403d23aa134","title":"From Linear Regression to Neural Networks","slug":"from-linear-regression-to-neural-networks","mobiledoc":null,"html":"<p>These days there exists much hype around sophisticated machine learning methods such as Neural Networks ‚Äî they are massively powerful models that allow us to fit very flexible models. However, we do not always require the full complexity of a Neural Network: sometimes, a simpler model will do the job just fine. In this project, we take a journey starting from the most fundamental statistical machinery to model data distributions, linear regression, to then explain the benefits of constructing more complex models, such as logistic regression or a Neural Network. In this way, this text aims to build a bridge from the statistical, analytical world to the more approximative world of Machine Learning. We will not shy away from the math, whilst still working with tangible examples at all times: we will work with real-world datasets and we will get to apply our models as we go on. Let's start!</p><!--kg-card-begin: html--><h2>Linear Regression <small style=\"color:#ccc;\">(<a  style=\"color:#ccc;\" href=\"https://dunnkers.com/linear-regression-to-neural-networks/linear-regression.html\">Code</a>)</small></h2><!--kg-card-end: html--><p>First, we will explore linear regression, for it is an easy to understand model upon which we can build more sophisticated concepts. We will use a <a href=\"https://github.com/allisonhorst/penguins\">dataset</a> on Antarctican penguins (Gorman et al., 2014) to conduct a regression between the penguin <em>flipper length</em> as independent variable $X$ and the penguin <em>body mass</em> as the dependent variable $Y$. We can analytically solve Linear Regression by minimizing the <em>Residual Sum-of-Squares</em> cost function (Hastie et al., 2009):</p><p>$$\\text{R}(\\beta) = (Y - X \\beta)^T (Y - X \\beta)$$</p><p>In which $X$ is our <em>design matrix.</em> Regression using this loss function is also referred to as \"Ordinary Least Squares\". The mean of the cost function $\\text{R}$ over all samples is called Mean Squared Error, or MSE. Our design matrix is built by appending each data row with a bias constant of 1 - an alternative would be to first center our data to get rid of the intercept entirely. To now minimize our cost function we differentiate $\\text{R}$ with respect to $\\beta$, giving us the following unique minimum:</p><p>$$\\hat{\\beta} = (X^T X)^{-1} X^T Y$$</p><p>... which results in the estimated least-squares coefficients given the training data, also called the <em>normal equation</em>. We can classify by simply multiplying our input data with the found coefficient matrix: $\\hat{Y} = X \\hat{\\beta}$. Let's observe our fitted regression line onto the data:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/linear-regression-flipper-vs-bodymass.svg\" class=\"kg-image\" alt=\"Linear Regression fit on Penguin data using the normal equation. Using a validation data split of ¬º testing data and ¬æ training data.\" loading=\"lazy\" width=\"527\" height=\"388\"><figcaption>Linear Regression fit on Penguin data using the normal equation. Using a validation data split of ¬º testing data and ¬æ training data.</figcaption></figure><p>We can observe visually that our estimator explains both the training and testing data reasonably well: the line positioned itself along the mean of the data. This is in fact the proposition we make in least-squares - we assume the target to be Gaussian distributed; which in the case of modeling this natural phenomenon, penguins, seems to fit quite well.</p><p>Because at the moment we are very curious, we would also like to explore using a more flexible model. Note that our normal equation we defined above tries to find whatever parameters make the system of linear equations produce the best predictions on our target variable. This means, that hypothetically, we could add any linear combination of explanatory variables we like: such create estimators of a higher-order polynomial form. This is called <strong>polynomial regression</strong>. To illustrate, a design matrix for one explanatory variable $X_1$ would look as follows:</p><p>$$X= \\left[\\begin{array}{ccccc}1 &amp; x_{1} &amp; x_{1}^{2} &amp; \\ldots &amp; x_{1}^{d} \\\\ 1 &amp; x_{2} &amp; x_{2}^{2} &amp; \\ldots &amp; x_{2}^{d} \\\\ 1 &amp; x_{3} &amp; x_{3}^{2} &amp; \\ldots &amp; x_{3}^{d} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n} &amp; x_{n}^{2} &amp; \\ldots &amp; x_{n}^{d}\\end{array}\\right]$$</p><p>Which results in $d$-th degree polynomial regression. The case $d=1$ is just normal linear regression. For example sake, let us sample only $n=10$ samples from our training dataset, and try to fit those with a polynomial regressors of increasing degrees. Let us observe what happens to the training and testing loss accordingly:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/polynomial-degrees.svg\" class=\"kg-image\" alt=\"Polynomial fits of various degrees on just $n=10$ training dataset samples. Testing dataset remained unchanged.\" loading=\"lazy\" width=\"576\" height=\"384\"><figcaption>Polynomial fits of various degrees on just $n=10$ training dataset samples. Testing dataset remained unchanged.</figcaption></figure><p>It can be observed that although for some degrees the losses remain almost the same, we suffer from overfitting after the degree passes $d=30$. We can also visually show how the polynomials of varying degrees fit our data:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/polynomial-fit.gif\" class=\"kg-image\" alt=\"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b7abbcf6-2bfe-492e-aa73-c4644924ec24/polynomial-fit.gif\" loading=\"lazy\" width=\"432\" height=\"288\"></figure><p>We can indeed observe that the polynomials of higher degree definitely do not better explain our data. Also, the polynomials tend to get rather erratic beyond the last data points of the training data - which is important to consider whenever predicting outside the training data value ranges. Generally, polynomials of exceedingly high degree can overfit too easily and should only be considered in very special cases.</p><p>Up till now our experiments have been relatively simple - we used only one explanatory and one response variable. Let us now explore an example in which we use all available explanatory variables to predict body mass, to see whether we can achieve an even better fit. Because we are now at risk of suffering from <em>multicolinearity</em>; the situation where multiple explanatory variables are highly linearly related to each other, we will use an extension of linear regression which can deal with such a situation. The technique is called <strong>Ridge Regression</strong>.</p><h3 id=\"ridge-regression\">Ridge Regression</h3><p>In Ridge Regression, we aim to tamper the least squares tendency to get as 'flexible' as possible to fit the data best it can. This might, however, cause parameters to get very large. We therefore like to add a penalty on the regression parameters $\\beta$; we penalise the loss function with a square of the parameter vector $\\beta$ scaled by new hyperparameter $\\lambda$. This is called a <em>shrinkage method</em>, or also: <strong>regularization.</strong> This causes the squared loss function to become:</p><p>$$\\text{R}(\\beta) = (Y - X \\beta)^T (Y - X \\beta)+\\lambda \\beta^T \\beta$$</p><p>This is called regularization with an $L^2$ norm; which generalization is called Tikhonov regularization, which allows for the case where not every parameter scalar is regularized equally. If we were to use an $L^1$ norm instead, we would speak of LASSO regression. If we were to now derive the solutions of $\\beta$ given this new cost function by differentiation w.r.t. $\\beta$:</p><p>$$\\hat{\\beta}^{\\text {ridge }}=\\left(\\mathbf{X}^{T} \\mathbf{X}+\\lambda \\mathbf{I}\\right)^{-1} \\mathbf{X}^{T} \\mathbf{Y}$$</p><p>In which $\\lambda$ will be a scaling constant that controls the amount of regularization that is applied. Note $\\mathbf{I}$ is the $p\\times p$ identity matrix - in which $p$ are the amount of data dimensions used. An important intuition to be known about Ridge Regression, is that directions in the column space of $X$ with small variance will be shrinked the most; this behavior can be easily shown be deconstructing the least-squares fitted vector using a Singular Value Decomposition. That said, let us see whether we can benefit from this new technique in our experiment.</p><p>In the next experiment, we will now use <strong>all</strong> available quantitative variables to try and predict the Penguin body mass. The Penguin- bill length, bill depth and flipper length will be used as independent variables. Note, however, they might be somewhat correlated: see <a href=\"https://dunnkers.com/linear-regression-to-neural-networks/images/penguin-pairplot.svg\">this pairplot</a> on the Penguin data for details. This poses an interesting challenge for our regression. Let us combine this with varying dataset sample sizes and varying settings of $\\lambda$ to see the effects on our loss.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/ridge-vs-loss.svg\" class=\"kg-image\" alt=\"Ridge Regression using all quantitative variables in the Penguin dataset to predict body mass. Varying subset sizes of the dataset $n$ as well as different regularization strengths $\\lambda$ are shown.\" loading=\"lazy\" width=\"768\" height=\"384\"></figure><p>Ridge Regression using all quantitative variables in the Penguin dataset to predict body mass. Varying subset sizes of the dataset $n$ as well as different regularization strengths $\\lambda$ are shown.</p><p>It can be observed, that using including all quantitative variables did improve the loss on predicting the Penguin body mass using Ridge Regression. In fact, the penalty imposed probably pulled the hyperplane angle down such that the error in fact increased. Ridge Regression is a very powerful technique, nonetheless, and most importantly introduced us to the concept of regularization. In the next chapters on Logistic Regression in Neural Networks, we assume all our models to use $L^2$ regularization.</p><p>Now, the data we fit up until now had only a small dimensionality - this is perhaps a drastic oversimplification in comparison to the real world. How does the analytic way of solving linear regression using the normal equation fare with <strong>higher-dimensional data</strong>?</p><h3 id=\"high-dimensional-data\">High-dimensional data</h3><p>In the real world, datasets might be of very high dimensionality: think of images, speech, or a biomedical dataset storing DNA sequences. These datasets cause different computational strain on the equations to be solved to fit a linear regression model: so let us <strong>simulate</strong> such a high-dimensional situation.</p><p>In our simulation the amount of dimensions will configured to outmatch the amount of dataset samples ($p \\gg n$), which extra dimensions we will create by simply adding some noise columns to the design matrix $X$. The noise will be drawn from a Gaussian distribution $\\epsilon \\sim \\mathcal{N}(0, 1)$. We can now run an experiment by fitting our linear regression model to the higher-dimensional noised dataset, benchmarking the fitting times of the algorithm.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Analytic_lower-vs-higher-dimensional.svg\" class=\"kg-image\" alt=\"Linear Regression fitting times for lower- and higher- dimensional Penguin data.\" loading=\"lazy\" width=\"225\" height=\"142\"><figcaption>Linear Regression fitting times for lower- and higher- dimensional Penguin data.</figcaption></figure><p>We can observe that the normal equation takes <strong>a lot</strong> longer to compute for higher-dimensional data. In fact, numerically computing the matrix inverse is very computationally expensive, i.e. computing $(X^TX)^{-1}$. Luckily, there are computationally cheaper techniques to do a regression in higher-dimensional spaces. One such technique is an iterative procedure, called <strong>Gradient Descent</strong>.</p><h3 id=\"gradient-descent\">Gradient Descent</h3><p>Instead of trying to analytically solve the system of linear equations at once, we can choose an iterative procedure instead, such as Gradient Descent. It works by computing the gradient of the cost function with respect to the model weights - such that we can then move in the opposite direction of the gradient in parameter space. Given some loss function $R(\\beta)$ and $R_i(\\beta)$, which computes the empirical loss for entire dataset and for the $i$-th observation, respectively, we can define one gradient descent step as:</p><p>$$\\begin{aligned} \\beta^{(r + 1)} &amp;= \\beta^{(r)} - \\gamma \\nabla_{\\beta^{(r)}} R(\\beta^{(r)}) \\\\ &amp;= \\beta^{(r)} - \\gamma \\sum_{i=1}^N \\frac{\\partial R_i(\\beta^{(r)})}{\\partial \\beta^{(r)}}\\\\ \\end{aligned}$$</p><p>In which $\\gamma$ is the learning rate and $r$ indicates some iteration - given some initial parameters $\\beta^0$ and $N$ training samples. Using this equation, we are able to reduce the loss in every iteration, until we converge. Convergence occurs when every element of the gradient is zero - or very close to it. Although gradient descent is used in this vanilla form, two modifications are common: (1) <strong>subsampling</strong> and ¬†using a (2) <strong>learning rate schedule</strong>.</p><ol><li>Although in a scenario in which our loss function landscape is convex the vanilla variant does converge toward the global optimum relatively easily, this might not be the case for non-convex error landscapes. We are at risk of getting stuck in local extremes. In this case, it is desirable to introduce some randomness ‚Äî allowing us to jump out local extrema. We can introduce randomness by instead of computing the gradient over the entire sample set, we can do so for a random sample of the dataset called a <em>minibatch</em> (Goodfellow et al., 2014). A side effect is a lighter computational burden per iteration; sometimes causing faster convergence. Because the introduced randomness makes the procedure stochastic instead of deterministic, we call this algorithm <em>Stochastic</em> Gradient Descent, or simply <strong>SGD</strong>.</li><li>Accommodating SGD is often a learning rate schedule: making the learning rate parameter $\\gamma$ dependent on the iteration number $r$ such that $\\gamma = \\gamma^{(r)}$. In this way, we made the learning rate adaptive over time, allowing us to create a custom learning rate scheme. Many schemes (Dogo et al., 2018) exist - which can be used to avoid spending a long time on flat areas in the error landscape called plateaus, or to avoid 'overshooting' the optimal solution. Even, a technique analogous with <em>momentum</em> (Qian, 1999) in physics might be used: a particle traveling through space is 'accelerated' by the loss gradient, causing the gradient to change faster if it keeps going in the same direction.</li></ol><p>So, let's now redefine our gradient descent formula to accommodate for these modifications:</p><p>$$\\beta^{(r+1)}=\\beta^{(r)}-\\gamma^{(r)} \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial R_i(\\beta^{(r)})}{\\partial \\beta^{(r)}} $$</p><p>... where we, before each iteration, randomly shuffle our training dataset such that we draw $m$ random samples each step. The variable $m$ denotes the <em>batch size</em> - which can be anywhere between 1 and the amount of dataset samples minus one $N - 1$. The smaller the batch size, the more stochastic the procedure will get.</p><p>Using gradient descent for our linear regression is straight-forward. We differentiate the cost function with respect to the weights; the least squares derivative is then as follows:</p><p>$$\\begin{aligned} \\frac{\\partial R_i(\\beta^{(r)})}{\\partial \\beta^{(r)}} &amp;= \\frac{\\partial}{\\partial \\beta^{(r)}} (y_i - x_i \\beta^{(r)})^2\\\\ &amp;= 2 (y_i - x_i \\beta^{(r)})\\\\ \\end{aligned}$$</p><p>We then run the algorithm in a loop, to iteratively get closer to the optimum parameter values.</p><p>Now, using this newly introduced iterative optimization procedure, let's see whether we can solve linear regression faster. First, we will compare SGD and the analytic method for our Penguin dataset with standard Gaussian noise dimensions added such that $p=2000$.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/SVGvsAnalytic_p2000.svg\" class=\"kg-image\" alt=\"Fitting time and MSE loss differences of Linear Regression solved using SGD and analytically using the normal equation. 10 experiments are shown; each one is a dot. SGD uses $\\gamma^0=0.001$ with an inverse scaling schedule of $\\gamma^{r+1} = \\frac{\\gamma^0}{t^{0.25}}$ and 20 thousand iterations maximum.\" loading=\"lazy\" width=\"948\" height=\"460\"><figcaption>Fitting time and MSE loss differences of Linear Regression solved using SGD and analytically using the normal equation. 10 experiments are shown; each one is a dot. SGD uses $\\gamma^0=0.001$ with an inverse scaling schedule of $\\gamma^{r+1} = \\frac{\\gamma^0}{t^{0.25}}$ and 20 thousand iterations maximum.</figcaption></figure><p>Indeed - our iterative procedure is faster for such a high-dimensional dataset. Because the analytic method always finds the optimum value, it is most plausible that SGD does not achieve the same performance - as can be seen in the MSE loss in the figure. Only in a couple of runs does SGD achieve near-optimum performance - in the other cases the algorithm was either stopped by its maximum iterations limit or it got stuck in some local extrema and has not gotten out yet. If we wanted to get better results, we could have used a more lenient maximum amount of iterations or a stricter convergence condition. This is a clear trade-off between computational workload and the optimality of the solution. We can run some more experiments for various levels of augmented dimensions:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/SVGvsAnalytic_many_p.svg\" class=\"kg-image\" alt=\"Fitting time and MSE loss for several degrees of dataset dimensionality. For each dimensionality, the average and its 95% confidence intervals over 10 experiments are shown. Loss plot is the average of the training and testing set.\" loading=\"lazy\" width=\"957\" height=\"442\"><figcaption>Fitting time and MSE loss for several degrees of dataset dimensionality. For each dimensionality, the average and its 95% confidence intervals over 10 experiments are shown. Loss plot is the average of the training and testing set.</figcaption></figure><p>In which we can empirically show that for our experiment, the analytic computation time grows about exponentially whilst SGD causes only a mild increase in computational time. SGD does suffer a higher loss due to its approximative nature - but this might just be worth the trade-off.</p><p>Now that we have gotten familiar with Gradient Descent, we can explore a realm of techniques that rely on being solved iteratively. Instead of doing regression, we will now try to <strong>classify</strong> penguins by their species type ‚Äî a method for doing so is <strong>Logistic Regression</strong>.</p><!--kg-card-begin: html--><h2>Logistic Regression <small style=\"color:#ccc;\">(<a  style=\"color:#ccc;\" href=\"https://dunnkers.com/linear-regression-to-neural-networks/logistic-regression.html\">Code</a>)</small></h2><!--kg-card-end: html--><p>In general, linear regression is no good for classification. There is no notion incorporated into the objective function to desire a hyperplane that best separates two classes. Even if we would encode qualitative target variables in a quantitative way, i.e. in zeros or ones, a normal equation fit would result in predicted values outside the target range.</p><p>Therefore, we require a different scheme. In Logistic Regression, we first want to make sure all estimations remain in $[0,1]$. This can be done using the <strong>Sigmoid function</strong>:</p><p>$$S(z)=\\frac{e^z}{e^z+1}=\\frac{1}{1+e^{-z}}$$</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/Logistic-curve.svg\" class=\"kg-image\" alt=\"Sigmoid function $S(z)$. Given any number $z \\in \\mathbb{R}$ the function always returns a number in $[0, 1]$. Image: source.\" loading=\"lazy\" width=\"600\" height=\"400\"><figcaption>Sigmoid function $S(z)$. Given any number $z \\in \\mathbb{R}$ the function always returns a number in $[0, 1]$. Image: <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function#/media/File:Logistic-curve.svg\">source</a>.</figcaption></figure><p>Also called the <em>Logistic function.</em> So, the goal is to predict some class $G \\in \\{1,\\dots,K\\}$ given inputs $X$. We assume an intercept constant of 1 to be embedded in $X$. Now let us take a closer look at the case where $K=2$, i.e. the binary or <strong>binomial</strong> case.</p><p>If we were to encode our class targets $Y$ as either ones or zeros, i.e. $Y \\in \\{0,1\\}$, we can predict values using $X \\beta$ and pull them through a sigmoid $S(X\\beta)$ to obtain the probabilities whether samples belongs to the class encoded as 1. This can be written as:</p><p>$$\\begin{aligned} \\Pr(G=2|X;\\beta)&amp;=S(X\\beta)\\\\ &amp;=\\frac{1}{1+\\exp(-X\\beta)}\\\\ &amp;=p(X;\\beta) \\end{aligned}$$</p><p>Because we consider only two classes, we can compute one probability and infer the other one, like so:</p><p>$$\\begin{aligned} \\Pr(G=1|X;\\beta)&amp;=1-p(X;\\beta) \\end{aligned}$$</p><p>For which it can be easily seen that both probabilities form a <em>probability vector</em>, i.e. their values sum to 1. Note we can consider the targets as a sequence of <em>Bernoulli trials</em> $y_i,\\dots,y_N$ - each outcome a binary - assuming all observations are independent of one another. This allows us to write:</p><p>$$\\begin{aligned} \\Pr (y| X;\\beta)&amp;=p(X;\\beta)^y(1-p(X;\\beta))^{(1-y)}\\\\ \\end{aligned}$$</p><p>So, how to approximate $\\beta$? Like in linear regression, we can optimize a loss function to obtain an estimator $\\hat{\\beta}$. We can express the loss function as a likelihood using <em>Maximum Likelihood Estimation</em>. First, we express our objective into a conditional <strong>likelihood</strong> function.</p><p>$$\\begin{aligned} L(\\beta)&amp;=\\Pr (Y| X;\\beta)\\\\ &amp;=\\prod_{i=1}^N \\Pr (y_i|X=x_i;\\beta)\\\\ &amp;=\\prod_{i=1}^N p(x_i;\\beta)^{y_i}(1-p(x_i;\\beta))^{(1-y_i)} \\end{aligned}$$</p><p>The likelihood becomes easier to maximize in practice if we rewrite the product to a sum using a logarithm; such scaling does not change the resulting parameters. We obtain the <strong>log-likelihood</strong> (Bischop, 2006):</p><p>$$\\begin{aligned} \\ell(\\beta)&amp;=\\log L(\\beta)\\\\ &amp;=\\sum_{i=1}^{N}\\left\\{y_{i} \\log p\\left(x_{i} ; \\beta\\right)+\\left(1-y_{i}\\right) \\log \\left(1-p\\left(x_{i} ; \\beta\\right)\\right)\\right\\}\\\\ &amp;=\\sum_{i=1}^{N}\\left\\{y_{i} \\beta^{T} x_{i}-\\log \\left(1+e^{\\beta^{T} x_{i}}\\right)\\right\\} \\end{aligned}$$</p><p>Also called the <strong><em>logistic loss</em></strong>; which multi-dimensional counterpart is the <em>cross-entropy</em> loss. We can maximize this likelihood function by computing its gradient:</p><p>$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta}=\\sum_{i=1}^{N} x_{i}\\left(y_{i}-p\\left(x_{i} ; \\beta\\right)\\right)$$</p><p>...resulting in $p+1$ equations nonlinear in $\\beta$. The equation is <em>transcendental</em>: meaning no closed-form solution exists and hence we cannot simply solve for zero. It is possible, however, to use numerical approximations: Newton-Raphson method based strategies can be used, such as Newton Conjugate-Gradient, or quasi-Newton procedures might be used such as L-BFGS (Zhu et al., 1997). Different strategies have varying benefits based on the problem type, e.g. the amount of samples $n$ or dimensions $p$. Since the gradient can be approximated just fine, we can also simply use Gradient Descent, i.e. SGD.</p><p>In the case where more response variables are to be predicted, i.e. $K&gt;2$, a <strong>multinomial</strong> variant of Logistic Regression can be used. For easier implementation, some software implementations just perform multiple binomial logistic regressions in order to conduct a multinomial one; which is called a One-versus-All strategy. The resulting probabilities are then normalized to still output a probability vector (Pedregosa et al., 2001).</p><p>That theory out of the way, let's fit a Logistic Regression model to our penguin data! We will try to classify whether a penguin is a Chinstrap yes or no, in other words: we will perform a binomial logistic regression. We will perform 30K iterations, each iteration an epoch over the training data:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/logistic-metrics.svg\" class=\"kg-image\" alt=\"Logistic Regression model fit on a binary penguin classification task. The model converged at 88.2% training-, 89.7% testing accuracy and a loss of 0.304 on the training set.\" loading=\"lazy\" width=\"960\" height=\"336\"><figcaption>Logistic Regression model fit on a binary penguin classification task. The model converged at 88.2% training-, 89.7% testing accuracy and a loss of 0.304 on the training set.</figcaption></figure><p>We can observe that the model converged to a stable state already after about 10K epochs - we could have implemented an early stopping rule; for example by checking whether validation scores stop improving or when our loss is no longer changing much. We can also visualize our model fit over time: by predicting over a grid of values at every time step during training. This yields the following animation:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/logistic-fit.gif\" class=\"kg-image\" alt=\"Logistic Regression model fit using SGD with constant learning rate of $\\gamma=0.001$ and $L^2$ regularization using $\\alpha=0.0005$ .\" loading=\"lazy\" width=\"432\" height=\"288\"><figcaption>Logistic Regression model fit using SGD with constant learning rate of $\\gamma=0.001$ and $L^2$ regularization using $\\alpha=0.0005$.</figcaption></figure><p>Clearly, our decision boundary is not optimal yet - whilst the data is somewhat Gaussian distributed our model linearly separates the data. We can do better ‚Äî we need some way to introduce more non-linearity into our model. A model that does just so is a <strong>Neural Network</strong>.</p><!--kg-card-begin: html--><h2>Neural Network <small style=\"color:#ccc;\">(<a  style=\"color:#ccc;\" href=\"https://dunnkers.com/linear-regression-to-neural-networks/neural-network.html\">Code</a>)</small></h2><!--kg-card-end: html--><p>At last, we arrive at the Neural Network. Using the previously learned concepts, we are really not that far off from assembling a Neural Network. Really, a single-layer Neural Network essentially just a linear model, like before. The difference is, that we conduct some extra projections in order to make the data better linearly separable. In a Neural Network, we aim to find the parameters facilitating such projections automatically. We call each such projection a <em>Hidden Layer</em>. After having conducted a suitable projection, ¬†we can pull the projected data through a logistic function to estimate a probability - similarly to logistic regression. One such architecture is like so:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/nn.svg\" class=\"kg-image\" alt=\"Neural Network architecture for 2-dimensional inputs and a 1-dimensional output with $l=3$ hidden layers each containing 5 neurons (image generated using NN-SVG).\" loading=\"lazy\"><figcaption>Neural Network architecture for 2-dimensional inputs and a 1-dimensional output with $l=3$ hidden layers each containing 5 neurons (image generated using <a href=\"http://alexlenail.me/NN-SVG/\">NN-SVG</a>).</figcaption></figure><p>So, given one input vector $x_i$, we can compute its estimated value by feeding its values through the network from left to right, in each layer multiplying with its parameter vector. We call this type of network <em>feed-forward</em>. Networks that do not feed forward include <em>recurrent</em> or <em>recursive</em> networks, though we will only concern ourselves with feed-forward networks for now.</p><p>An essential component of any such network is an <strong><em>activation function</em>;</strong> a <em>non-linear</em> differentiable function mapping $\\mathbb{R} \\rightarrow \\mathbb{R}$, aimed to overcome model linearity constraints. We apply the activation function to every hidden node; we compute the total input, add a bias, and then activate. This process is somewhat analogous to what happens in neurons in the brain - hence the name Neural Network. Among many possible activation functions (Nwankpa et al., 2018), a popular choice is the Rectified Linear Unit, or <strong>ReLU</strong>: $\\sigma(z)=\\max\\{0, z\\}$. It looks as follows:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/relu.svg\" class=\"kg-image\" alt=\"ReLU activation function $\\sigma(z)=\\max \\{0,z\\}$. The function is easily seen to be piecewise-linear.\" loading=\"lazy\" width=\"480\" height=\"384\"><figcaption>ReLU activation function $\\sigma(z)=\\max \\{0,z\\}$. The function is easily seen to be piecewise-linear.</figcaption></figure><p>Also because ReLU is just a max operation, it is fast to compute (e.g. compared to a sigmoid). Using our activation function, we can define a <em>forward-pass</em> through our network, as follows:</p><p>$$\\begin{aligned} h^{(1)}&amp;=\\sigma(X W^{(1)} + b^{(1)})\\\\ h^{(2)}&amp;=\\sigma(h^{(1)} W^{(2)} + b^{(2)})\\\\ h^{(3)}&amp;=\\sigma(h^{(2)} W^{(3)} + b^{(3)})\\\\ \\hat{Y}&amp;=S(h^{(3)}W^{(4)}+b^{(4)}) \\end{aligned}$$</p><p>In which $h$ resembles the intermediate projections indexed by its hidden layer; and the parameters $\\beta$ mapping every two layers together are accessible through $W$. A bias vector is accessible through $b$, such to add a bias term to every node in the layer. Finally, we apply a Sigmoid to the results of the last layer to receive probability estimates; in the case of multi-class outputs its multi-dimensional counterpart is used, the <em>Softmax</em>, which normalizes the logistic function such to produce a probability vector. Do note that the activation function <em>could</em> differ per layer; and in practice, this might happen. In our case, we will just use one activation function for all hidden layers in our network.</p><p>We are also going to have to define a <strong>cost function</strong>, such to be able to optimize the parameters based on its gradient. We can do so using the minimizing the negative log-likelihood using Maximum Likelihood, given some loss function such as:</p><p>$$ R(\\theta)=-\\mathbb{E}_{\\mathbf{x}, \\mathbf{y}\\sim\\hat{p}_{\\text{data }}}\\log p_{\\operatorname{model}}(\\boldsymbol{y}\\mid\\boldsymbol{x}) $$</p><p>In which we combined weights $W$ and biases $b$ into a single parameter term $\\theta$. Our cost function says to quantify the chance of encountering a target $y$ given an input vector $x$. Suitable loss functions to be used are log-loss/cross-entropy, or simply squared error:</p><p>$$ R(\\theta)=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}, \\mathbf{y}\\sim\\hat{p}_{\\text{data }}}\\|\\boldsymbol{y}-f(\\boldsymbol{x} ; \\boldsymbol{\\theta})\\|^{2}+ \\text{const} $$</p><p>Assuming $p_{\\text{model}}(y|x)$ to be Gaussian distributed. Of course, in any implementation we can only approach the expected value by averaging over a discrete set of observations; thus allowing us to compute the loss of our network.</p><p>Now that we are able to do a forward pass by (1) making predictions given a set of parameters $\\theta$ and (2) computing its loss using a cost function $R(\\theta)$, we will have to figure out how to actually <strong>train</strong> our network. Because our computation involves quite some operations by now, computing the gradient of the cost function is not trivial - to approximate the full gradient one would have to compute partial derivatives with respect to every weight separately. Luckily, we can exploit the calculus chain rule to break up the problem into smaller pieces: allowing us to much more efficiently re-use previously computed answers. The algorithm using this trick is called <strong>back-propagation</strong>.</p><p>In back-propagation, we re-visit the network in reverse order; i.e. starting at the output layer and working our way back to the input layer. We then use the calculus derivative chain rule (Goodfellow et al., 2014):</p><p>$$\\begin{aligned} \\frac{\\partial z}{\\partial x_{i}}&amp;=\\sum_{j} \\frac{\\partial z}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{i}}\\\\ &amp;\\text{in vector notation:}\\\\ \\nabla_{\\boldsymbol{x}} z&amp;=\\left(\\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}\\right)^{\\top} \\nabla_{\\boldsymbol{y}} z \\end{aligned}$$</p><p>...to compute the gradient in modular fashion. Note we need to consider the network in its entirety when computing the partial derivatives; the output activation, the loss function, node activations and the biases. To systematically apply back-prop to a network often these functions are abstracted as being an <em>operation</em> - which can then be assembled in a <em>computational graph</em>. Given a suitable such graph, many generic back-prop implementations can be used.</p><p>Once we have now computed the derivative of the cost function $R(\\theta)$, our situation became similar to when we iteratively solved linear- or logistic regression: we can now use just Gradient Descent to move in the error landscape.</p><p>Now that we know how to train a Neural Network, let's apply it! We aim to get better accuracy for our Penguin classification problem than using our Logistic Regression model.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/neural-metrics.svg\" class=\"kg-image\" alt=\"Neural Network fit on a binary penguin classification task. The model converged at 96.5% training-, 94.9% testing accuracy and a loss of 0.108 on the training set.\" loading=\"lazy\" width=\"960\" height=\"336\"><figcaption>Neural Network fit on a binary penguin classification task. The model converged at 96.5% training-, 94.9% testing accuracy and a loss of 0.108 on the training set.</figcaption></figure><p>Indeed, our more flexible Neural Network model better fits the data. The NN achieves 94.9% testing accuracy, in comparison to 89.7% testing accuracy for the Logistic Regression model. Let's see how our model is fitted over time:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/neural-fit.gif\" class=\"kg-image\" alt=\"Neural Network fit performing a binary classification task on penguin species. Has 3 hidden layers of 5 nodes each; uses $L^2$ regularization with $\\alpha=0.0005$ and a constant learning rate of $\\gamma=0.001$.\" loading=\"lazy\" width=\"432\" height=\"288\"><figcaption>Neural Network fit performing a binary classification task on penguin species. Has 3 hidden layers of 5 nodes each; uses $L^2$ regularization with $\\alpha=0.0005$ and a constant learning rate of $\\gamma=0.001$.</figcaption></figure><p>In which it can be observed that the model converged after some 750 iterations. Intuitively, the decision region looks to have been approximated fairly well - it might just have been slightly 'stretched' out.</p><h3 id=\"ending-note\">Ending note</h3><p>Now that we have been able to fit a more 'complicated' data distribution, we conclude our journey from simple statistical models such a linear regression up to Neural Networks. Having a diverse set of statistical and iterative techniques in your tool belt is essential for any Machine Learning practitioner: even though immensely powerful models are available and widespread today, sometimes a simpler model will do just fine.</p><p>In tandem with how the bias/variance dilemma is fundamental to understanding how to construct good distribution learning models, one should always take into account not to overreach on model complexity given a learning task (Occam's Razor; Rasmussen et al., 2001): use an as simple as possible model, wherever possible.</p><h2 id=\"citations\">Citations</h2><ul><li><a href=\"https://doi.org/10.1371/journal.pone.0090081\">Gorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081.</a></li><li><a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media.</a></li><li><a href=\"https://ieeexplore.ieee.org/abstract/document/8769211\">Dogo, E. M., Afolabi, O. J., Nwulu, N. I., Twala, B., &amp; Aigbavboa, C. O. (2018, December). A comparative analysis of gradient descent-based optimization algorithms on convolutional neural networks. In 2018 International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS) (pp. 92-99). IEEE.</a></li><li><a href=\"http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\">Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.</a></li><li><a href=\"https://www.deeplearningbook.org/\">Goodfellow, I., Bengio, Y., Courville, A., &amp; Bengio, Y. (2016). Deep learning (Vol. 1, No. 2). Cambridge: MIT press.</a></li><li><a href=\"https://www.sciencedirect.com/science/article/pii/S0893608098001166?casa_token=1Cj40vh2xXcAAAAA:Km2rWQK3qSQfFRp5u8RFongBdcCNOAGpBpa3g0nQO3lq7lUSG9ocYx2ExZfaz55dOWsAl102MDc\">Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural networks, 12(1), 145-151.</a></li><li><a href=\"https://dl.acm.org/doi/abs/10.1145/279232.279236?casa_token=vPvVfjPO5LYAAAAA:HRqyyBJ8KBVy09S8331ZV2pKZOfJrK820r6kuf9kxvpXi5y5DVQxGZzKN4eHeHYBaZ-DGqubi-oUaw\">Zhu, C., Byrd, R. H., Lu, P., &amp; Nocedal, J. (1997). Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4), 550-560.</a></li><li><a href=\"https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf?source=post_page---------------------------\">Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... &amp; Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 12, 2825-2830.</a></li><li><a href=\"https://arxiv.org/abs/1811.03378\">Nwankpa, C., Ijomah, W., Gachagan, A., &amp; Marshall, S. (2018). Activation functions: Comparison of trends in practice and research for deep learning. arXiv preprint arXiv:1811.03378.</a></li><li><a href=\"https://books.google.nl/books?hl=en&amp;lr=&amp;id=Mgs2FwtgNxwC&amp;oi=fnd&amp;pg=PA294&amp;dq=occams+razor&amp;ots=EMXQ4ohtev&amp;sig=KRoX-dtpPwJNdPLujn4Qz7O3sI0&amp;redir_esc=y#v=onepage&amp;q&amp;f=false\">Rasmussen, C. E., &amp; Ghahramani, Z. (2001). Occam's razor. Advances in neural information processing systems, 294-300.</a></li></ul><p></p><h2 id=\"code\">Code</h2><p>The code is freely available on Github, see:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><a href=\"https://github.com/dunnkers/linear-regression-to-neural-networks\"><img src=\"__GHOST_URL__/content/images/2021/11/github32-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"32\" height=\"32\"></a><figcaption><a href=\"https://github.com/dunnkers/linear-regression-to-neural-networks\">linear-regression-to-neural-networks</a></figcaption></figure><p></p><p></p>","comment_id":"61a3f4cf72c7ac1e204625a3","plaintext":"These days there exists much hype around sophisticated machine learning methods such as Neural Networks ‚Äî they are massively powerful models that allow us to fit very flexible models. However, we do not always require the full complexity of a Neural Network: sometimes, a simpler model will do the job just fine. In this project, we take a journey starting from the most fundamental statistical machinery to model data distributions, linear regression, to then explain the benefits of constructing more complex models, such as logistic regression or a Neural Network. In this way, this text aims to build a bridge from the statistical, analytical world to the more approximative world of Machine Learning. We will not shy away from the math, whilst still working with tangible examples at all times: we will work with real-world datasets and we will get to apply our models as we go on. Let's start!\n\n\nLinear Regression (Code)\n\nFirst, we will explore linear regression, for it is an easy to understand model upon which we can build more sophisticated concepts. We will use a dataset on Antarctican penguins (Gorman et al., 2014) to conduct a regression between the penguin flipper length as independent variable $X$ and the penguin body mass as the dependent variable $Y$. We can analytically solve Linear Regression by minimizing the Residual Sum-of-Squares cost function (Hastie et al., 2009):\n\n$$\\text{R}(\\beta) = (Y - X \\beta)^T (Y - X \\beta)$$\n\nIn which $X$ is our design matrix. Regression using this loss function is also referred to as \"Ordinary Least Squares\". The mean of the cost function $\\text{R}$ over all samples is called Mean Squared Error, or MSE. Our design matrix is built by appending each data row with a bias constant of 1 - an alternative would be to first center our data to get rid of the intercept entirely. To now minimize our cost function we differentiate $\\text{R}$ with respect to $\\beta$, giving us the following unique minimum:\n\n$$\\hat{\\beta} = (X^T X)^{-1} X^T Y$$\n\n... which results in the estimated least-squares coefficients given the training data, also called the normal equation. We can classify by simply multiplying our input data with the found coefficient matrix: $\\hat{Y} = X \\hat{\\beta}$. Let's observe our fitted regression line onto the data:\n\nWe can observe visually that our estimator explains both the training and testing data reasonably well: the line positioned itself along the mean of the data. This is in fact the proposition we make in least-squares - we assume the target to be Gaussian distributed; which in the case of modeling this natural phenomenon, penguins, seems to fit quite well.\n\nBecause at the moment we are very curious, we would also like to explore using a more flexible model. Note that our normal equation we defined above tries to find whatever parameters make the system of linear equations produce the best predictions on our target variable. This means, that hypothetically, we could add any linear combination of explanatory variables we like: such create estimators of a higher-order polynomial form. This is called polynomial regression. To illustrate, a design matrix for one explanatory variable $X_1$ would look as follows:\n\n$$X= \\left[\\begin{array}{ccccc}1 & x_{1} & x_{1}^{2} & \\ldots & x_{1}^{d} \\\\ 1 & x_{2} & x_{2}^{2} & \\ldots & x_{2}^{d} \\\\ 1 & x_{3} & x_{3}^{2} & \\ldots & x_{3}^{d} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n} & x_{n}^{2} & \\ldots & x_{n}^{d}\\end{array}\\right]$$\n\nWhich results in $d$-th degree polynomial regression. The case $d=1$ is just normal linear regression. For example sake, let us sample only $n=10$ samples from our training dataset, and try to fit those with a polynomial regressors of increasing degrees. Let us observe what happens to the training and testing loss accordingly:\n\nIt can be observed that although for some degrees the losses remain almost the same, we suffer from overfitting after the degree passes $d=30$. We can also visually show how the polynomials of varying degrees fit our data:\n\nWe can indeed observe that the polynomials of higher degree definitely do not better explain our data. Also, the polynomials tend to get rather erratic beyond the last data points of the training data - which is important to consider whenever predicting outside the training data value ranges. Generally, polynomials of exceedingly high degree can overfit too easily and should only be considered in very special cases.\n\nUp till now our experiments have been relatively simple - we used only one explanatory and one response variable. Let us now explore an example in which we use all available explanatory variables to predict body mass, to see whether we can achieve an even better fit. Because we are now at risk of suffering from multicolinearity; the situation where multiple explanatory variables are highly linearly related to each other, we will use an extension of linear regression which can deal with such a situation. The technique is called Ridge Regression.\n\n\nRidge Regression\n\nIn Ridge Regression, we aim to tamper the least squares tendency to get as 'flexible' as possible to fit the data best it can. This might, however, cause parameters to get very large. We therefore like to add a penalty on the regression parameters $\\beta$; we penalise the loss function with a square of the parameter vector $\\beta$ scaled by new hyperparameter $\\lambda$. This is called a shrinkage method, or also: regularization. This causes the squared loss function to become:\n\n$$\\text{R}(\\beta) = (Y - X \\beta)^T (Y - X \\beta)+\\lambda \\beta^T \\beta$$\n\nThis is called regularization with an $L^2$ norm; which generalization is called Tikhonov regularization, which allows for the case where not every parameter scalar is regularized equally. If we were to use an $L^1$ norm instead, we would speak of LASSO regression. If we were to now derive the solutions of $\\beta$ given this new cost function by differentiation w.r.t. $\\beta$:\n\n$$\\hat{\\beta}^{\\text {ridge }}=\\left(\\mathbf{X}^{T} \\mathbf{X}+\\lambda \\mathbf{I}\\right)^{-1} \\mathbf{X}^{T} \\mathbf{Y}$$\n\nIn which $\\lambda$ will be a scaling constant that controls the amount of regularization that is applied. Note $\\mathbf{I}$ is the $p\\times p$ identity matrix - in which $p$ are the amount of data dimensions used. An important intuition to be known about Ridge Regression, is that directions in the column space of $X$ with small variance will be shrinked the most; this behavior can be easily shown be deconstructing the least-squares fitted vector using a Singular Value Decomposition. That said, let us see whether we can benefit from this new technique in our experiment.\n\nIn the next experiment, we will now use all available quantitative variables to try and predict the Penguin body mass. The Penguin- bill length, bill depth and flipper length will be used as independent variables. Note, however, they might be somewhat correlated: see this pairplot on the Penguin data for details. This poses an interesting challenge for our regression. Let us combine this with varying dataset sample sizes and varying settings of $\\lambda$ to see the effects on our loss.\n\nRidge Regression using all quantitative variables in the Penguin dataset to predict body mass. Varying subset sizes of the dataset $n$ as well as different regularization strengths $\\lambda$ are shown.\n\nIt can be observed, that using including all quantitative variables did improve the loss on predicting the Penguin body mass using Ridge Regression. In fact, the penalty imposed probably pulled the hyperplane angle down such that the error in fact increased. Ridge Regression is a very powerful technique, nonetheless, and most importantly introduced us to the concept of regularization. In the next chapters on Logistic Regression in Neural Networks, we assume all our models to use $L^2$ regularization.\n\nNow, the data we fit up until now had only a small dimensionality - this is perhaps a drastic oversimplification in comparison to the real world. How does the analytic way of solving linear regression using the normal equation fare with higher-dimensional data?\n\n\nHigh-dimensional data\n\nIn the real world, datasets might be of very high dimensionality: think of images, speech, or a biomedical dataset storing DNA sequences. These datasets cause different computational strain on the equations to be solved to fit a linear regression model: so let us simulate such a high-dimensional situation.\n\nIn our simulation the amount of dimensions will configured to outmatch the amount of dataset samples ($p \\gg n$), which extra dimensions we will create by simply adding some noise columns to the design matrix $X$. The noise will be drawn from a Gaussian distribution $\\epsilon \\sim \\mathcal{N}(0, 1)$. We can now run an experiment by fitting our linear regression model to the higher-dimensional noised dataset, benchmarking the fitting times of the algorithm.\n\nWe can observe that the normal equation takes a lot longer to compute for higher-dimensional data. In fact, numerically computing the matrix inverse is very computationally expensive, i.e. computing $(X^TX)^{-1}$. Luckily, there are computationally cheaper techniques to do a regression in higher-dimensional spaces. One such technique is an iterative procedure, called Gradient Descent.\n\n\nGradient Descent\n\nInstead of trying to analytically solve the system of linear equations at once, we can choose an iterative procedure instead, such as Gradient Descent. It works by computing the gradient of the cost function with respect to the model weights - such that we can then move in the opposite direction of the gradient in parameter space. Given some loss function $R(\\beta)$ and $R_i(\\beta)$, which computes the empirical loss for entire dataset and for the $i$-th observation, respectively, we can define one gradient descent step as:\n\n$$\\begin{aligned} \\beta^{(r + 1)} &= \\beta^{(r)} - \\gamma \\nabla_{\\beta^{(r)}} R(\\beta^{(r)}) \\\\ &= \\beta^{(r)} - \\gamma \\sum_{i=1}^N \\frac{\\partial R_i(\\beta^{(r)})}{\\partial \\beta^{(r)}}\\\\ \\end{aligned}$$\n\nIn which $\\gamma$ is the learning rate and $r$ indicates some iteration - given some initial parameters $\\beta^0$ and $N$ training samples. Using this equation, we are able to reduce the loss in every iteration, until we converge. Convergence occurs when every element of the gradient is zero - or very close to it. Although gradient descent is used in this vanilla form, two modifications are common: (1) subsampling and ¬†using a (2) learning rate schedule.\n\n 1. Although in a scenario in which our loss function landscape is convex the vanilla variant does converge toward the global optimum relatively easily, this might not be the case for non-convex error landscapes. We are at risk of getting stuck in local extremes. In this case, it is desirable to introduce some randomness ‚Äî allowing us to jump out local extrema. We can introduce randomness by instead of computing the gradient over the entire sample set, we can do so for a random sample of the dataset called a minibatch (Goodfellow et al., 2014). A side effect is a lighter computational burden per iteration; sometimes causing faster convergence. Because the introduced randomness makes the procedure stochastic instead of deterministic, we call this algorithm Stochastic Gradient Descent, or simply SGD.\n 2. Accommodating SGD is often a learning rate schedule: making the learning rate parameter $\\gamma$ dependent on the iteration number $r$ such that $\\gamma = \\gamma^{(r)}$. In this way, we made the learning rate adaptive over time, allowing us to create a custom learning rate scheme. Many schemes (Dogo et al., 2018) exist - which can be used to avoid spending a long time on flat areas in the error landscape called plateaus, or to avoid 'overshooting' the optimal solution. Even, a technique analogous with momentum (Qian, 1999) in physics might be used: a particle traveling through space is 'accelerated' by the loss gradient, causing the gradient to change faster if it keeps going in the same direction.\n\nSo, let's now redefine our gradient descent formula to accommodate for these modifications:\n\n$$\\beta^{(r+1)}=\\beta^{(r)}-\\gamma^{(r)} \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial R_i(\\beta^{(r)})}{\\partial \\beta^{(r)}} $$\n\n... where we, before each iteration, randomly shuffle our training dataset such that we draw $m$ random samples each step. The variable $m$ denotes the batch size - which can be anywhere between 1 and the amount of dataset samples minus one $N - 1$. The smaller the batch size, the more stochastic the procedure will get.\n\nUsing gradient descent for our linear regression is straight-forward. We differentiate the cost function with respect to the weights; the least squares derivative is then as follows:\n\n$$\\begin{aligned} \\frac{\\partial R_i(\\beta^{(r)})}{\\partial \\beta^{(r)}} &= \\frac{\\partial}{\\partial \\beta^{(r)}} (y_i - x_i \\beta^{(r)})^2\\\\ &= 2 (y_i - x_i \\beta^{(r)})\\\\ \\end{aligned}$$\n\nWe then run the algorithm in a loop, to iteratively get closer to the optimum parameter values.\n\nNow, using this newly introduced iterative optimization procedure, let's see whether we can solve linear regression faster. First, we will compare SGD and the analytic method for our Penguin dataset with standard Gaussian noise dimensions added such that $p=2000$.\n\nIndeed - our iterative procedure is faster for such a high-dimensional dataset. Because the analytic method always finds the optimum value, it is most plausible that SGD does not achieve the same performance - as can be seen in the MSE loss in the figure. Only in a couple of runs does SGD achieve near-optimum performance - in the other cases the algorithm was either stopped by its maximum iterations limit or it got stuck in some local extrema and has not gotten out yet. If we wanted to get better results, we could have used a more lenient maximum amount of iterations or a stricter convergence condition. This is a clear trade-off between computational workload and the optimality of the solution. We can run some more experiments for various levels of augmented dimensions:\n\nIn which we can empirically show that for our experiment, the analytic computation time grows about exponentially whilst SGD causes only a mild increase in computational time. SGD does suffer a higher loss due to its approximative nature - but this might just be worth the trade-off.\n\nNow that we have gotten familiar with Gradient Descent, we can explore a realm of techniques that rely on being solved iteratively. Instead of doing regression, we will now try to classify penguins by their species type ‚Äî a method for doing so is Logistic Regression.\n\n\nLogistic Regression (Code)\n\nIn general, linear regression is no good for classification. There is no notion incorporated into the objective function to desire a hyperplane that best separates two classes. Even if we would encode qualitative target variables in a quantitative way, i.e. in zeros or ones, a normal equation fit would result in predicted values outside the target range.\n\nTherefore, we require a different scheme. In Logistic Regression, we first want to make sure all estimations remain in $[0,1]$. This can be done using the Sigmoid function:\n\n$$S(z)=\\frac{e^z}{e^z+1}=\\frac{1}{1+e^{-z}}$$\n\nAlso called the Logistic function. So, the goal is to predict some class $G \\in \\{1,\\dots,K\\}$ given inputs $X$. We assume an intercept constant of 1 to be embedded in $X$. Now let us take a closer look at the case where $K=2$, i.e. the binary or binomial case.\n\nIf we were to encode our class targets $Y$ as either ones or zeros, i.e. $Y \\in \\{0,1\\}$, we can predict values using $X \\beta$ and pull them through a sigmoid $S(X\\beta)$ to obtain the probabilities whether samples belongs to the class encoded as 1. This can be written as:\n\n$$\\begin{aligned} \\Pr(G=2|X;\\beta)&=S(X\\beta)\\\\ &=\\frac{1}{1+\\exp(-X\\beta)}\\\\ &=p(X;\\beta) \\end{aligned}$$\n\nBecause we consider only two classes, we can compute one probability and infer the other one, like so:\n\n$$\\begin{aligned} \\Pr(G=1|X;\\beta)&=1-p(X;\\beta) \\end{aligned}$$\n\nFor which it can be easily seen that both probabilities form a probability vector, i.e. their values sum to 1. Note we can consider the targets as a sequence of Bernoulli trials $y_i,\\dots,y_N$ - each outcome a binary - assuming all observations are independent of one another. This allows us to write:\n\n$$\\begin{aligned} \\Pr (y| X;\\beta)&=p(X;\\beta)^y(1-p(X;\\beta))^{(1-y)}\\\\ \\end{aligned}$$\n\nSo, how to approximate $\\beta$? Like in linear regression, we can optimize a loss function to obtain an estimator $\\hat{\\beta}$. We can express the loss function as a likelihood using Maximum Likelihood Estimation. First, we express our objective into a conditional likelihood function.\n\n$$\\begin{aligned} L(\\beta)&=\\Pr (Y| X;\\beta)\\\\ &=\\prod_{i=1}^N \\Pr (y_i|X=x_i;\\beta)\\\\ &=\\prod_{i=1}^N p(x_i;\\beta)^{y_i}(1-p(x_i;\\beta))^{(1-y_i)} \\end{aligned}$$\n\nThe likelihood becomes easier to maximize in practice if we rewrite the product to a sum using a logarithm; such scaling does not change the resulting parameters. We obtain the log-likelihood (Bischop, 2006):\n\n$$\\begin{aligned} \\ell(\\beta)&=\\log L(\\beta)\\\\ &=\\sum_{i=1}^{N}\\left\\{y_{i} \\log p\\left(x_{i} ; \\beta\\right)+\\left(1-y_{i}\\right) \\log \\left(1-p\\left(x_{i} ; \\beta\\right)\\right)\\right\\}\\\\ &=\\sum_{i=1}^{N}\\left\\{y_{i} \\beta^{T} x_{i}-\\log \\left(1+e^{\\beta^{T} x_{i}}\\right)\\right\\} \\end{aligned}$$\n\nAlso called the logistic loss; which multi-dimensional counterpart is the cross-entropy loss. We can maximize this likelihood function by computing its gradient:\n\n$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta}=\\sum_{i=1}^{N} x_{i}\\left(y_{i}-p\\left(x_{i} ; \\beta\\right)\\right)$$\n\n...resulting in $p+1$ equations nonlinear in $\\beta$. The equation is transcendental: meaning no closed-form solution exists and hence we cannot simply solve for zero. It is possible, however, to use numerical approximations: Newton-Raphson method based strategies can be used, such as Newton Conjugate-Gradient, or quasi-Newton procedures might be used such as L-BFGS (Zhu et al., 1997). Different strategies have varying benefits based on the problem type, e.g. the amount of samples $n$ or dimensions $p$. Since the gradient can be approximated just fine, we can also simply use Gradient Descent, i.e. SGD.\n\nIn the case where more response variables are to be predicted, i.e. $K>2$, a multinomial variant of Logistic Regression can be used. For easier implementation, some software implementations just perform multiple binomial logistic regressions in order to conduct a multinomial one; which is called a One-versus-All strategy. The resulting probabilities are then normalized to still output a probability vector (Pedregosa et al., 2001).\n\nThat theory out of the way, let's fit a Logistic Regression model to our penguin data! We will try to classify whether a penguin is a Chinstrap yes or no, in other words: we will perform a binomial logistic regression. We will perform 30K iterations, each iteration an epoch over the training data:\n\nWe can observe that the model converged to a stable state already after about 10K epochs - we could have implemented an early stopping rule; for example by checking whether validation scores stop improving or when our loss is no longer changing much. We can also visualize our model fit over time: by predicting over a grid of values at every time step during training. This yields the following animation:\n\nClearly, our decision boundary is not optimal yet - whilst the data is somewhat Gaussian distributed our model linearly separates the data. We can do better ‚Äî we need some way to introduce more non-linearity into our model. A model that does just so is a Neural Network.\n\n\nNeural Network (Code)\n\nAt last, we arrive at the Neural Network. Using the previously learned concepts, we are really not that far off from assembling a Neural Network. Really, a single-layer Neural Network essentially just a linear model, like before. The difference is, that we conduct some extra projections in order to make the data better linearly separable. In a Neural Network, we aim to find the parameters facilitating such projections automatically. We call each such projection a Hidden Layer. After having conducted a suitable projection, ¬†we can pull the projected data through a logistic function to estimate a probability - similarly to logistic regression. One such architecture is like so:\n\nSo, given one input vector $x_i$, we can compute its estimated value by feeding its values through the network from left to right, in each layer multiplying with its parameter vector. We call this type of network feed-forward. Networks that do not feed forward include recurrent or recursive networks, though we will only concern ourselves with feed-forward networks for now.\n\nAn essential component of any such network is an activation function; a non-linear differentiable function mapping $\\mathbb{R} \\rightarrow \\mathbb{R}$, aimed to overcome model linearity constraints. We apply the activation function to every hidden node; we compute the total input, add a bias, and then activate. This process is somewhat analogous to what happens in neurons in the brain - hence the name Neural Network. Among many possible activation functions (Nwankpa et al., 2018), a popular choice is the Rectified Linear Unit, or ReLU: $\\sigma(z)=\\max\\{0, z\\}$. It looks as follows:\n\nAlso because ReLU is just a max operation, it is fast to compute (e.g. compared to a sigmoid). Using our activation function, we can define a forward-pass through our network, as follows:\n\n$$\\begin{aligned} h^{(1)}&=\\sigma(X W^{(1)} + b^{(1)})\\\\ h^{(2)}&=\\sigma(h^{(1)} W^{(2)} + b^{(2)})\\\\ h^{(3)}&=\\sigma(h^{(2)} W^{(3)} + b^{(3)})\\\\ \\hat{Y}&=S(h^{(3)}W^{(4)}+b^{(4)}) \\end{aligned}$$\n\nIn which $h$ resembles the intermediate projections indexed by its hidden layer; and the parameters $\\beta$ mapping every two layers together are accessible through $W$. A bias vector is accessible through $b$, such to add a bias term to every node in the layer. Finally, we apply a Sigmoid to the results of the last layer to receive probability estimates; in the case of multi-class outputs its multi-dimensional counterpart is used, the Softmax, which normalizes the logistic function such to produce a probability vector. Do note that the activation function could differ per layer; and in practice, this might happen. In our case, we will just use one activation function for all hidden layers in our network.\n\nWe are also going to have to define a cost function, such to be able to optimize the parameters based on its gradient. We can do so using the minimizing the negative log-likelihood using Maximum Likelihood, given some loss function such as:\n\n$$ R(\\theta)=-\\mathbb{E}_{\\mathbf{x}, \\mathbf{y}\\sim\\hat{p}_{\\text{data }}}\\log p_{\\operatorname{model}}(\\boldsymbol{y}\\mid\\boldsymbol{x}) $$\n\nIn which we combined weights $W$ and biases $b$ into a single parameter term $\\theta$. Our cost function says to quantify the chance of encountering a target $y$ given an input vector $x$. Suitable loss functions to be used are log-loss/cross-entropy, or simply squared error:\n\n$$ R(\\theta)=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}, \\mathbf{y}\\sim\\hat{p}_{\\text{data }}}\\|\\boldsymbol{y}-f(\\boldsymbol{x} ; \\boldsymbol{\\theta})\\|^{2}+ \\text{const} $$\n\nAssuming $p_{\\text{model}}(y|x)$ to be Gaussian distributed. Of course, in any implementation we can only approach the expected value by averaging over a discrete set of observations; thus allowing us to compute the loss of our network.\n\nNow that we are able to do a forward pass by (1) making predictions given a set of parameters $\\theta$ and (2) computing its loss using a cost function $R(\\theta)$, we will have to figure out how to actually train our network. Because our computation involves quite some operations by now, computing the gradient of the cost function is not trivial - to approximate the full gradient one would have to compute partial derivatives with respect to every weight separately. Luckily, we can exploit the calculus chain rule to break up the problem into smaller pieces: allowing us to much more efficiently re-use previously computed answers. The algorithm using this trick is called back-propagation.\n\nIn back-propagation, we re-visit the network in reverse order; i.e. starting at the output layer and working our way back to the input layer. We then use the calculus derivative chain rule (Goodfellow et al., 2014):\n\n$$\\begin{aligned} \\frac{\\partial z}{\\partial x_{i}}&=\\sum_{j} \\frac{\\partial z}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{i}}\\\\ &\\text{in vector notation:}\\\\ \\nabla_{\\boldsymbol{x}} z&=\\left(\\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}\\right)^{\\top} \\nabla_{\\boldsymbol{y}} z \\end{aligned}$$\n\n...to compute the gradient in modular fashion. Note we need to consider the network in its entirety when computing the partial derivatives; the output activation, the loss function, node activations and the biases. To systematically apply back-prop to a network often these functions are abstracted as being an operation - which can then be assembled in a computational graph. Given a suitable such graph, many generic back-prop implementations can be used.\n\nOnce we have now computed the derivative of the cost function $R(\\theta)$, our situation became similar to when we iteratively solved linear- or logistic regression: we can now use just Gradient Descent to move in the error landscape.\n\nNow that we know how to train a Neural Network, let's apply it! We aim to get better accuracy for our Penguin classification problem than using our Logistic Regression model.\n\nIndeed, our more flexible Neural Network model better fits the data. The NN achieves 94.9% testing accuracy, in comparison to 89.7% testing accuracy for the Logistic Regression model. Let's see how our model is fitted over time:\n\nIn which it can be observed that the model converged after some 750 iterations. Intuitively, the decision region looks to have been approximated fairly well - it might just have been slightly 'stretched' out.\n\n\nEnding note\n\nNow that we have been able to fit a more 'complicated' data distribution, we conclude our journey from simple statistical models such a linear regression up to Neural Networks. Having a diverse set of statistical and iterative techniques in your tool belt is essential for any Machine Learning practitioner: even though immensely powerful models are available and widespread today, sometimes a simpler model will do just fine.\n\nIn tandem with how the bias/variance dilemma is fundamental to understanding how to construct good distribution learning models, one should always take into account not to overreach on model complexity given a learning task (Occam's Razor; Rasmussen et al., 2001): use an as simple as possible model, wherever possible.\n\n\nCitations\n\n * Gorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081.\n * Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media.\n * Dogo, E. M., Afolabi, O. J., Nwulu, N. I., Twala, B., & Aigbavboa, C. O. (2018, December). A comparative analysis of gradient descent-based optimization algorithms on convolutional neural networks. In 2018 International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS) (pp. 92-99). IEEE.\n * Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\n * Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). Deep learning (Vol. 1, No. 2). Cambridge: MIT press.\n * Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural networks, 12(1), 145-151.\n * Zhu, C., Byrd, R. H., Lu, P., & Nocedal, J. (1997). Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4), 550-560.\n * Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 12, 2825-2830.\n * Nwankpa, C., Ijomah, W., Gachagan, A., & Marshall, S. (2018). Activation functions: Comparison of trends in practice and research for deep learning. arXiv preprint arXiv:1811.03378.\n * Rasmussen, C. E., & Ghahramani, Z. (2001). Occam's razor. Advances in neural information processing systems, 294-300.\n\n\n\n\nCode\n\nThe code is freely available on Github, see:\n\n\n\n","feature_image":"__GHOST_URL__/content/images/2021/11/linear-regression-to-neural-networks.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","created_at":"2021-11-28 21:29:51","created_by":"1","updated_at":"2024-11-22 13:37:00","updated_by":"1","published_at":"2021-04-17 22:00:00","published_by":"1","custom_excerpt":"How are linear regression, logistic regression and neural networks related? What is overfitting and how do we fight it? In this post, we find answers to these questions in an interactive way by working with a real-world dataset on penguins.","codeinjection_head":"<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css\" integrity=\"sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs\" crossorigin=\"anonymous\">\n<script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js\" integrity=\"sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx\" crossorigin=\"anonymous\"></script>\n<script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js\" integrity=\"sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR\" crossorigin=\"anonymous\"\n    onload=\"renderMathInElement(document.body, {delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\(', right: '\\\\)', display: false}, {left: '\\\\begin{equation}', right: '\\\\end{equation}', display: true}, {left: '\\\\begin{align}', right: '\\\\end{align}', display: true}, {left: '\\\\begin{alignat}', right: '\\\\end{alignat}', display: true}, {left: '\\\\begin{gather}', right: '\\\\end{gather}', display: true}, {left: '\\\\begin{CD}', right: '\\\\end{CD}', display: true}, {left: '\\\\[', right: '\\\\]', display: true}]});\"></script>","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"These days there exists much hype around sophisticated machine learning methods such as Neural Networks ‚Äî they are massively powerful models that allow us to fit very flexible models. However, we do not always require the full complexity of a Neural Network: sometimes, a simpler model will do the job just fine. In this project, we take a journey starting from the most fundamental statistical machinery to model data distributions, linear regression, to then explain the benefits of constructing more complex models, such as logistic regression or a Neural Network. In this way, this text aims to build a bridge from the statistical, analytical world to the more approximative world of Machine Learning. We will not shy away from the math, whilst still working with tangible examples at all times: we will work with real-world datasets and we will get to apply our models as we go on. Let's start!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"html\":\"<h2>Linear Regression <small style=\\\"color:#ccc;\\\">(<a  style=\\\"color:#ccc;\\\" href=\\\"https://dunnkers.com/linear-regression-to-neural-networks/linear-regression.html\\\">Code</a>)</small></h2>\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"First, we will explore linear regression, for it is an easy to understand model upon which we can build more sophisticated concepts. We will use a \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"dataset\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/allisonhorst/penguins\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" on Antarctican penguins (Gorman et al., 2014) to conduct a regression between the penguin \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"flipper length\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" as independent variable $X$ and the penguin \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"body mass\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" as the dependent variable $Y$. We can analytically solve Linear Regression by minimizing the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Residual Sum-of-Squares\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" cost function (Hastie et al., 2009):\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\text{R}(\\\\beta) = (Y - X \\\\beta)^T (Y - X \\\\beta)$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which $X$ is our \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"design matrix.\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" Regression using this loss function is also referred to as \\\"Ordinary Least Squares\\\". The mean of the cost function $\\\\text{R}$ over all samples is called Mean Squared Error, or MSE. Our design matrix is built by appending each data row with a bias constant of 1 - an alternative would be to first center our data to get rid of the intercept entirely. To now minimize our cost function we differentiate $\\\\text{R}$ with respect to $\\\\beta$, giving us the following unique minimum:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\hat{\\\\beta} = (X^T X)^{-1} X^T Y$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"... which results in the estimated least-squares coefficients given the training data, also called the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"normal equation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". We can classify by simply multiplying our input data with the found coefficient matrix: $\\\\hat{Y} = X \\\\hat{\\\\beta}$. Let's observe our fitted regression line onto the data:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/linear-regression-flipper-vs-bodymass.svg\",\"alt\":\"Linear Regression fit on Penguin data using the normal equation. Using a validation data split of ¬º testing data and ¬æ training data.\",\"title\":\"\",\"width\":527,\"height\":388,\"caption\":\"Linear Regression fit on Penguin data using the normal equation. Using a validation data split of ¬º testing data and ¬æ training data.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can observe visually that our estimator explains both the training and testing data reasonably well: the line positioned itself along the mean of the data. This is in fact the proposition we make in least-squares - we assume the target to be Gaussian distributed; which in the case of modeling this natural phenomenon, penguins, seems to fit quite well.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Because at the moment we are very curious, we would also like to explore using a more flexible model. Note that our normal equation we defined above tries to find whatever parameters make the system of linear equations produce the best predictions on our target variable. This means, that hypothetically, we could add any linear combination of explanatory variables we like: such create estimators of a higher-order polynomial form. This is called \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"polynomial regression\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". To illustrate, a design matrix for one explanatory variable $X_1$ would look as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$X= \\\\left[\\\\begin{array}{ccccc}1 & x_{1} & x_{1}^{2} & \\\\ldots & x_{1}^{d} \\\\\\\\ 1 & x_{2} & x_{2}^{2} & \\\\ldots & x_{2}^{d} \\\\\\\\ 1 & x_{3} & x_{3}^{2} & \\\\ldots & x_{3}^{d} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ 1 & x_{n} & x_{n}^{2} & \\\\ldots & x_{n}^{d}\\\\end{array}\\\\right]$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Which results in $d$-th degree polynomial regression. The case $d=1$ is just normal linear regression. For example sake, let us sample only $n=10$ samples from our training dataset, and try to fit those with a polynomial regressors of increasing degrees. Let us observe what happens to the training and testing loss accordingly:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/polynomial-degrees.svg\",\"alt\":\"Polynomial fits of various degrees on just $n=10$ training dataset samples. Testing dataset remained unchanged.\",\"title\":\"\",\"width\":576,\"height\":384,\"caption\":\"Polynomial fits of various degrees on just $n=10$ training dataset samples. Testing dataset remained unchanged.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"It can be observed that although for some degrees the losses remain almost the same, we suffer from overfitting after the degree passes $d=30$. We can also visually show how the polynomials of varying degrees fit our data:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/polynomial-fit.gif\",\"alt\":\"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b7abbcf6-2bfe-492e-aa73-c4644924ec24/polynomial-fit.gif\",\"title\":\"\",\"width\":432,\"height\":288},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can indeed observe that the polynomials of higher degree definitely do not better explain our data. Also, the polynomials tend to get rather erratic beyond the last data points of the training data - which is important to consider whenever predicting outside the training data value ranges. Generally, polynomials of exceedingly high degree can overfit too easily and should only be considered in very special cases.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Up till now our experiments have been relatively simple - we used only one explanatory and one response variable. Let us now explore an example in which we use all available explanatory variables to predict body mass, to see whether we can achieve an even better fit. Because we are now at risk of suffering from \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"multicolinearity\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"; the situation where multiple explanatory variables are highly linearly related to each other, we will use an extension of linear regression which can deal with such a situation. The technique is called \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ridge Regression\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ridge Regression\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In Ridge Regression, we aim to tamper the least squares tendency to get as 'flexible' as possible to fit the data best it can. This might, however, cause parameters to get very large. We therefore like to add a penalty on the regression parameters $\\\\beta$; we penalise the loss function with a square of the parameter vector $\\\\beta$ scaled by new hyperparameter $\\\\lambda$. This is called a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"shrinkage method\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", or also: \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"regularization.\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" This causes the squared loss function to become:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\text{R}(\\\\beta) = (Y - X \\\\beta)^T (Y - X \\\\beta)+\\\\lambda \\\\beta^T \\\\beta$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This is called regularization with an $L^2$ norm; which generalization is called Tikhonov regularization, which allows for the case where not every parameter scalar is regularized equally. If we were to use an $L^1$ norm instead, we would speak of LASSO regression. If we were to now derive the solutions of $\\\\beta$ given this new cost function by differentiation w.r.t. $\\\\beta$:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\hat{\\\\beta}^{\\\\text {ridge }}=\\\\left(\\\\mathbf{X}^{T} \\\\mathbf{X}+\\\\lambda \\\\mathbf{I}\\\\right)^{-1} \\\\mathbf{X}^{T} \\\\mathbf{Y}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which $\\\\lambda$ will be a scaling constant that controls the amount of regularization that is applied. Note $\\\\mathbf{I}$ is the $p\\\\times p$ identity matrix - in which $p$ are the amount of data dimensions used. An important intuition to be known about Ridge Regression, is that directions in the column space of $X$ with small variance will be shrinked the most; this behavior can be easily shown be deconstructing the least-squares fitted vector using a Singular Value Decomposition. That said, let us see whether we can benefit from this new technique in our experiment.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In the next experiment, we will now use \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"all\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" available quantitative variables to try and predict the Penguin body mass. The Penguin- bill length, bill depth and flipper length will be used as independent variables. Note, however, they might be somewhat correlated: see \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"this pairplot\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dunnkers.com/linear-regression-to-neural-networks/images/penguin-pairplot.svg\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" on the Penguin data for details. This poses an interesting challenge for our regression. Let us combine this with varying dataset sample sizes and varying settings of $\\\\lambda$ to see the effects on our loss.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/ridge-vs-loss.svg\",\"alt\":\"Ridge Regression using all quantitative variables in the Penguin dataset to predict body mass. Varying subset sizes of the dataset $n$ as well as different regularization strengths $\\\\lambda$ are shown.\",\"title\":\"\",\"width\":768,\"height\":384},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ridge Regression using all quantitative variables in the Penguin dataset to predict body mass. Varying subset sizes of the dataset $n$ as well as different regularization strengths $\\\\lambda$ are shown.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"It can be observed, that using including all quantitative variables did improve the loss on predicting the Penguin body mass using Ridge Regression. In fact, the penalty imposed probably pulled the hyperplane angle down such that the error in fact increased. Ridge Regression is a very powerful technique, nonetheless, and most importantly introduced us to the concept of regularization. In the next chapters on Logistic Regression in Neural Networks, we assume all our models to use $L^2$ regularization.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now, the data we fit up until now had only a small dimensionality - this is perhaps a drastic oversimplification in comparison to the real world. How does the analytic way of solving linear regression using the normal equation fare with \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"higher-dimensional data\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"High-dimensional data\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In the real world, datasets might be of very high dimensionality: think of images, speech, or a biomedical dataset storing DNA sequences. These datasets cause different computational strain on the equations to be solved to fit a linear regression model: so let us \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"simulate\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" such a high-dimensional situation.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In our simulation the amount of dimensions will configured to outmatch the amount of dataset samples ($p \\\\gg n$), which extra dimensions we will create by simply adding some noise columns to the design matrix $X$. The noise will be drawn from a Gaussian distribution $\\\\epsilon \\\\sim \\\\mathcal{N}(0, 1)$. We can now run an experiment by fitting our linear regression model to the higher-dimensional noised dataset, benchmarking the fitting times of the algorithm.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Analytic_lower-vs-higher-dimensional.svg\",\"alt\":\"Linear Regression fitting times for lower- and higher- dimensional Penguin data.\",\"title\":\"\",\"width\":225,\"height\":142,\"caption\":\"Linear Regression fitting times for lower- and higher- dimensional Penguin data.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can observe that the normal equation takes \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"a lot\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" longer to compute for higher-dimensional data. In fact, numerically computing the matrix inverse is very computationally expensive, i.e. computing $(X^TX)^{-1}$. Luckily, there are computationally cheaper techniques to do a regression in higher-dimensional spaces. One such technique is an iterative procedure, called \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gradient Descent\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gradient Descent\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Instead of trying to analytically solve the system of linear equations at once, we can choose an iterative procedure instead, such as Gradient Descent. It works by computing the gradient of the cost function with respect to the model weights - such that we can then move in the opposite direction of the gradient in parameter space. Given some loss function $R(\\\\beta)$ and $R_i(\\\\beta)$, which computes the empirical loss for entire dataset and for the $i$-th observation, respectively, we can define one gradient descent step as:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\beta^{(r + 1)} &= \\\\beta^{(r)} - \\\\gamma \\\\nabla_{\\\\beta^{(r)}} R(\\\\beta^{(r)}) \\\\\\\\ &= \\\\beta^{(r)} - \\\\gamma \\\\sum_{i=1}^N \\\\frac{\\\\partial R_i(\\\\beta^{(r)})}{\\\\partial \\\\beta^{(r)}}\\\\\\\\ \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which $\\\\gamma$ is the learning rate and $r$ indicates some iteration - given some initial parameters $\\\\beta^0$ and $N$ training samples. Using this equation, we are able to reduce the loss in every iteration, until we converge. Convergence occurs when every element of the gradient is zero - or very close to it. Although gradient descent is used in this vanilla form, two modifications are common: (1) \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"subsampling\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and  using a (2) \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"learning rate schedule\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Although in a scenario in which our loss function landscape is convex the vanilla variant does converge toward the global optimum relatively easily, this might not be the case for non-convex error landscapes. We are at risk of getting stuck in local extremes. In this case, it is desirable to introduce some randomness ‚Äî allowing us to jump out local extrema. We can introduce randomness by instead of computing the gradient over the entire sample set, we can do so for a random sample of the dataset called a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"minibatch\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (Goodfellow et al., 2014). A side effect is a lighter computational burden per iteration; sometimes causing faster convergence. Because the introduced randomness makes the procedure stochastic instead of deterministic, we call this algorithm \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Stochastic\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" Gradient Descent, or simply \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"SGD\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Accommodating SGD is often a learning rate schedule: making the learning rate parameter $\\\\gamma$ dependent on the iteration number $r$ such that $\\\\gamma = \\\\gamma^{(r)}$. In this way, we made the learning rate adaptive over time, allowing us to create a custom learning rate scheme. Many schemes (Dogo et al., 2018) exist - which can be used to avoid spending a long time on flat areas in the error landscape called plateaus, or to avoid 'overshooting' the optimal solution. Even, a technique analogous with \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"momentum\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (Qian, 1999) in physics might be used: a particle traveling through space is 'accelerated' by the loss gradient, causing the gradient to change faster if it keeps going in the same direction.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So, let's now redefine our gradient descent formula to accommodate for these modifications:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\beta^{(r+1)}=\\\\beta^{(r)}-\\\\gamma^{(r)} \\\\frac{1}{m} \\\\sum_{i=1}^m \\\\frac{\\\\partial R_i(\\\\beta^{(r)})}{\\\\partial \\\\beta^{(r)}} $$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"... where we, before each iteration, randomly shuffle our training dataset such that we draw $m$ random samples each step. The variable $m$ denotes the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"batch size\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" - which can be anywhere between 1 and the amount of dataset samples minus one $N - 1$. The smaller the batch size, the more stochastic the procedure will get.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Using gradient descent for our linear regression is straight-forward. We differentiate the cost function with respect to the weights; the least squares derivative is then as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\frac{\\\\partial R_i(\\\\beta^{(r)})}{\\\\partial \\\\beta^{(r)}} &= \\\\frac{\\\\partial}{\\\\partial \\\\beta^{(r)}} (y_i - x_i \\\\beta^{(r)})^2\\\\\\\\ &= 2 (y_i - x_i \\\\beta^{(r)})\\\\\\\\ \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We then run the algorithm in a loop, to iteratively get closer to the optimum parameter values.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now, using this newly introduced iterative optimization procedure, let's see whether we can solve linear regression faster. First, we will compare SGD and the analytic method for our Penguin dataset with standard Gaussian noise dimensions added such that $p=2000$.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/SVGvsAnalytic_p2000.svg\",\"alt\":\"Fitting time and MSE loss differences of Linear Regression solved using SGD and analytically using the normal equation. 10 experiments are shown; each one is a dot. SGD uses $\\\\gamma^0=0.001$ with an inverse scaling schedule of $\\\\gamma^{r+1} = \\\\frac{\\\\gamma^0}{t^{0.25}}$ and 20 thousand iterations maximum.\",\"title\":\"\",\"width\":948,\"height\":460,\"caption\":\"Fitting time and MSE loss differences of Linear Regression solved using SGD and analytically using the normal equation. 10 experiments are shown; each one is a dot. SGD uses $\\\\gamma^0=0.001$ with an inverse scaling schedule of $\\\\gamma^{r+1} = \\\\frac{\\\\gamma^0}{t^{0.25}}$ and 20 thousand iterations maximum.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Indeed - our iterative procedure is faster for such a high-dimensional dataset. Because the analytic method always finds the optimum value, it is most plausible that SGD does not achieve the same performance - as can be seen in the MSE loss in the figure. Only in a couple of runs does SGD achieve near-optimum performance - in the other cases the algorithm was either stopped by its maximum iterations limit or it got stuck in some local extrema and has not gotten out yet. If we wanted to get better results, we could have used a more lenient maximum amount of iterations or a stricter convergence condition. This is a clear trade-off between computational workload and the optimality of the solution. We can run some more experiments for various levels of augmented dimensions:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/SVGvsAnalytic_many_p.svg\",\"alt\":\"Fitting time and MSE loss for several degrees of dataset dimensionality. For each dimensionality, the average and its 95% confidence intervals over 10 experiments are shown. Loss plot is the average of the training and testing set.\",\"title\":\"\",\"width\":957,\"height\":442,\"caption\":\"Fitting time and MSE loss for several degrees of dataset dimensionality. For each dimensionality, the average and its 95% confidence intervals over 10 experiments are shown. Loss plot is the average of the training and testing set.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which we can empirically show that for our experiment, the analytic computation time grows about exponentially whilst SGD causes only a mild increase in computational time. SGD does suffer a higher loss due to its approximative nature - but this might just be worth the trade-off.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now that we have gotten familiar with Gradient Descent, we can explore a realm of techniques that rely on being solved iteratively. Instead of doing regression, we will now try to \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"classify\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" penguins by their species type ‚Äî a method for doing so is \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Logistic Regression\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"html\":\"<h2>Logistic Regression <small style=\\\"color:#ccc;\\\">(<a  style=\\\"color:#ccc;\\\" href=\\\"https://dunnkers.com/linear-regression-to-neural-networks/logistic-regression.html\\\">Code</a>)</small></h2>\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In general, linear regression is no good for classification. There is no notion incorporated into the objective function to desire a hyperplane that best separates two classes. Even if we would encode qualitative target variables in a quantitative way, i.e. in zeros or ones, a normal equation fit would result in predicted values outside the target range.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Therefore, we require a different scheme. In Logistic Regression, we first want to make sure all estimations remain in $[0,1]$. This can be done using the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Sigmoid function\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$S(z)=\\\\frac{e^z}{e^z+1}=\\\\frac{1}{1+e^{-z}}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/Logistic-curve.svg\",\"alt\":\"Sigmoid function $S(z)$. Given any number $z \\\\in \\\\mathbb{R}$ the function always returns a number in $[0, 1]$. Image: source.\",\"title\":\"\",\"width\":600,\"height\":400,\"caption\":\"Sigmoid function $S(z)$. Given any number $z \\\\in \\\\mathbb{R}$ the function always returns a number in $[0, 1]$. Image: <a href=\\\"https://en.wikipedia.org/wiki/Sigmoid_function#/media/File:Logistic-curve.svg\\\">source</a>.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Also called the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Logistic function.\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" So, the goal is to predict some class $G \\\\in \\\\{1,\\\\dots,K\\\\}$ given inputs $X$. We assume an intercept constant of 1 to be embedded in $X$. Now let us take a closer look at the case where $K=2$, i.e. the binary or \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"binomial\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" case.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If we were to encode our class targets $Y$ as either ones or zeros, i.e. $Y \\\\in \\\\{0,1\\\\}$, we can predict values using $X \\\\beta$ and pull them through a sigmoid $S(X\\\\beta)$ to obtain the probabilities whether samples belongs to the class encoded as 1. This can be written as:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\Pr(G=2|X;\\\\beta)&=S(X\\\\beta)\\\\\\\\ &=\\\\frac{1}{1+\\\\exp(-X\\\\beta)}\\\\\\\\ &=p(X;\\\\beta) \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Because we consider only two classes, we can compute one probability and infer the other one, like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\Pr(G=1|X;\\\\beta)&=1-p(X;\\\\beta) \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"For which it can be easily seen that both probabilities form a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"probability vector\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", i.e. their values sum to 1. Note we can consider the targets as a sequence of \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Bernoulli trials\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" $y_i,\\\\dots,y_N$ - each outcome a binary - assuming all observations are independent of one another. This allows us to write:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\Pr (y| X;\\\\beta)&=p(X;\\\\beta)^y(1-p(X;\\\\beta))^{(1-y)}\\\\\\\\ \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So, how to approximate $\\\\beta$? Like in linear regression, we can optimize a loss function to obtain an estimator $\\\\hat{\\\\beta}$. We can express the loss function as a likelihood using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Maximum Likelihood Estimation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". First, we express our objective into a conditional \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"likelihood\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" function.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} L(\\\\beta)&=\\\\Pr (Y| X;\\\\beta)\\\\\\\\ &=\\\\prod_{i=1}^N \\\\Pr (y_i|X=x_i;\\\\beta)\\\\\\\\ &=\\\\prod_{i=1}^N p(x_i;\\\\beta)^{y_i}(1-p(x_i;\\\\beta))^{(1-y_i)} \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The likelihood becomes easier to maximize in practice if we rewrite the product to a sum using a logarithm; such scaling does not change the resulting parameters. We obtain the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"log-likelihood\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (Bischop, 2006):\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\ell(\\\\beta)&=\\\\log L(\\\\beta)\\\\\\\\ &=\\\\sum_{i=1}^{N}\\\\left\\\\{y_{i} \\\\log p\\\\left(x_{i} ; \\\\beta\\\\right)+\\\\left(1-y_{i}\\\\right) \\\\log \\\\left(1-p\\\\left(x_{i} ; \\\\beta\\\\right)\\\\right)\\\\right\\\\}\\\\\\\\ &=\\\\sum_{i=1}^{N}\\\\left\\\\{y_{i} \\\\beta^{T} x_{i}-\\\\log \\\\left(1+e^{\\\\beta^{T} x_{i}}\\\\right)\\\\right\\\\} \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Also called the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":3,\"mode\":\"normal\",\"style\":\"\",\"text\":\"logistic loss\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"; which multi-dimensional counterpart is the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"cross-entropy\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" loss. We can maximize this likelihood function by computing its gradient:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\frac{\\\\partial \\\\ell(\\\\beta)}{\\\\partial \\\\beta}=\\\\sum_{i=1}^{N} x_{i}\\\\left(y_{i}-p\\\\left(x_{i} ; \\\\beta\\\\right)\\\\right)$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"...resulting in $p+1$ equations nonlinear in $\\\\beta$. The equation is \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"transcendental\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": meaning no closed-form solution exists and hence we cannot simply solve for zero. It is possible, however, to use numerical approximations: Newton-Raphson method based strategies can be used, such as Newton Conjugate-Gradient, or quasi-Newton procedures might be used such as L-BFGS (Zhu et al., 1997). Different strategies have varying benefits based on the problem type, e.g. the amount of samples $n$ or dimensions $p$. Since the gradient can be approximated just fine, we can also simply use Gradient Descent, i.e. SGD.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In the case where more response variables are to be predicted, i.e. $K>2$, a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"multinomial\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" variant of Logistic Regression can be used. For easier implementation, some software implementations just perform multiple binomial logistic regressions in order to conduct a multinomial one; which is called a One-versus-All strategy. The resulting probabilities are then normalized to still output a probability vector (Pedregosa et al., 2001).\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That theory out of the way, let's fit a Logistic Regression model to our penguin data! We will try to classify whether a penguin is a Chinstrap yes or no, in other words: we will perform a binomial logistic regression. We will perform 30K iterations, each iteration an epoch over the training data:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/logistic-metrics.svg\",\"alt\":\"Logistic Regression model fit on a binary penguin classification task. The model converged at 88.2% training-, 89.7% testing accuracy and a loss of 0.304 on the training set.\",\"title\":\"\",\"width\":960,\"height\":336,\"caption\":\"Logistic Regression model fit on a binary penguin classification task. The model converged at 88.2% training-, 89.7% testing accuracy and a loss of 0.304 on the training set.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can observe that the model converged to a stable state already after about 10K epochs - we could have implemented an early stopping rule; for example by checking whether validation scores stop improving or when our loss is no longer changing much. We can also visualize our model fit over time: by predicting over a grid of values at every time step during training. This yields the following animation:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/logistic-fit.gif\",\"alt\":\"Logistic Regression model fit using SGD with constant learning rate of $\\\\gamma=0.001$ and $L^2$ regularization using $\\\\alpha=0.0005$ .\",\"title\":\"\",\"width\":432,\"height\":288,\"caption\":\"Logistic Regression model fit using SGD with constant learning rate of $\\\\gamma=0.001$ and $L^2$ regularization using $\\\\alpha=0.0005$.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Clearly, our decision boundary is not optimal yet - whilst the data is somewhat Gaussian distributed our model linearly separates the data. We can do better ‚Äî we need some way to introduce more non-linearity into our model. A model that does just so is a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Neural Network\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"html\":\"<h2>Neural Network <small style=\\\"color:#ccc;\\\">(<a  style=\\\"color:#ccc;\\\" href=\\\"https://dunnkers.com/linear-regression-to-neural-networks/neural-network.html\\\">Code</a>)</small></h2>\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"At last, we arrive at the Neural Network. Using the previously learned concepts, we are really not that far off from assembling a Neural Network. Really, a single-layer Neural Network essentially just a linear model, like before. The difference is, that we conduct some extra projections in order to make the data better linearly separable. In a Neural Network, we aim to find the parameters facilitating such projections automatically. We call each such projection a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hidden Layer\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". After having conducted a suitable projection,  we can pull the projected data through a logistic function to estimate a probability - similarly to logistic regression. One such architecture is like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/nn.svg\",\"alt\":\"Neural Network architecture for 2-dimensional inputs and a 1-dimensional output with $l=3$ hidden layers each containing 5 neurons (image generated using NN-SVG).\",\"title\":\"\",\"caption\":\"Neural Network architecture for 2-dimensional inputs and a 1-dimensional output with $l=3$ hidden layers each containing 5 neurons (image generated using <a href=\\\"http://alexlenail.me/NN-SVG/\\\">NN-SVG</a>).\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So, given one input vector $x_i$, we can compute its estimated value by feeding its values through the network from left to right, in each layer multiplying with its parameter vector. We call this type of network \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"feed-forward\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Networks that do not feed forward include \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"recurrent\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" or \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"recursive\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" networks, though we will only concern ourselves with feed-forward networks for now.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"An essential component of any such network is an \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":3,\"mode\":\"normal\",\"style\":\"\",\"text\":\"activation function\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\";\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"non-linear\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" differentiable function mapping $\\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$, aimed to overcome model linearity constraints. We apply the activation function to every hidden node; we compute the total input, add a bias, and then activate. This process is somewhat analogous to what happens in neurons in the brain - hence the name Neural Network. Among many possible activation functions (Nwankpa et al., 2018), a popular choice is the Rectified Linear Unit, or \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"ReLU\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": $\\\\sigma(z)=\\\\max\\\\{0, z\\\\}$. It looks as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/relu.svg\",\"alt\":\"ReLU activation function $\\\\sigma(z)=\\\\max \\\\{0,z\\\\}$. The function is easily seen to be piecewise-linear.\",\"title\":\"\",\"width\":480,\"height\":384,\"caption\":\"ReLU activation function $\\\\sigma(z)=\\\\max \\\\{0,z\\\\}$. The function is easily seen to be piecewise-linear.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Also because ReLU is just a max operation, it is fast to compute (e.g. compared to a sigmoid). Using our activation function, we can define a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"forward-pass\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" through our network, as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} h^{(1)}&=\\\\sigma(X W^{(1)} + b^{(1)})\\\\\\\\ h^{(2)}&=\\\\sigma(h^{(1)} W^{(2)} + b^{(2)})\\\\\\\\ h^{(3)}&=\\\\sigma(h^{(2)} W^{(3)} + b^{(3)})\\\\\\\\ \\\\hat{Y}&=S(h^{(3)}W^{(4)}+b^{(4)}) \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which $h$ resembles the intermediate projections indexed by its hidden layer; and the parameters $\\\\beta$ mapping every two layers together are accessible through $W$. A bias vector is accessible through $b$, such to add a bias term to every node in the layer. Finally, we apply a Sigmoid to the results of the last layer to receive probability estimates; in the case of multi-class outputs its multi-dimensional counterpart is used, the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Softmax\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", which normalizes the logistic function such to produce a probability vector. Do note that the activation function \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"could\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" differ per layer; and in practice, this might happen. In our case, we will just use one activation function for all hidden layers in our network.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We are also going to have to define a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"cost function\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", such to be able to optimize the parameters based on its gradient. We can do so using the minimizing the negative log-likelihood using Maximum Likelihood, given some loss function such as:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$ R(\\\\theta)=-\\\\mathbb{E}_{\\\\mathbf{x}, \\\\mathbf{y}\\\\sim\\\\hat{p}_{\\\\text{data }}}\\\\log p_{\\\\operatorname{model}}(\\\\boldsymbol{y}\\\\mid\\\\boldsymbol{x}) $$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which we combined weights $W$ and biases $b$ into a single parameter term $\\\\theta$. Our cost function says to quantify the chance of encountering a target $y$ given an input vector $x$. Suitable loss functions to be used are log-loss/cross-entropy, or simply squared error:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$ R(\\\\theta)=\\\\frac{1}{2}\\\\mathbb{E}_{\\\\mathbf{x}, \\\\mathbf{y}\\\\sim\\\\hat{p}_{\\\\text{data }}}\\\\|\\\\boldsymbol{y}-f(\\\\boldsymbol{x} ; \\\\boldsymbol{\\\\theta})\\\\|^{2}+ \\\\text{const} $$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Assuming $p_{\\\\text{model}}(y|x)$ to be Gaussian distributed. Of course, in any implementation we can only approach the expected value by averaging over a discrete set of observations; thus allowing us to compute the loss of our network.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now that we are able to do a forward pass by (1) making predictions given a set of parameters $\\\\theta$ and (2) computing its loss using a cost function $R(\\\\theta)$, we will have to figure out how to actually \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"train\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" our network. Because our computation involves quite some operations by now, computing the gradient of the cost function is not trivial - to approximate the full gradient one would have to compute partial derivatives with respect to every weight separately. Luckily, we can exploit the calculus chain rule to break up the problem into smaller pieces: allowing us to much more efficiently re-use previously computed answers. The algorithm using this trick is called \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"back-propagation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In back-propagation, we re-visit the network in reverse order; i.e. starting at the output layer and working our way back to the input layer. We then use the calculus derivative chain rule (Goodfellow et al., 2014):\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"$$\\\\begin{aligned} \\\\frac{\\\\partial z}{\\\\partial x_{i}}&=\\\\sum_{j} \\\\frac{\\\\partial z}{\\\\partial y_{j}} \\\\frac{\\\\partial y_{j}}{\\\\partial x_{i}}\\\\\\\\ &\\\\text{in vector notation:}\\\\\\\\ \\\\nabla_{\\\\boldsymbol{x}} z&=\\\\left(\\\\frac{\\\\partial \\\\boldsymbol{y}}{\\\\partial \\\\boldsymbol{x}}\\\\right)^{\\\\top} \\\\nabla_{\\\\boldsymbol{y}} z \\\\end{aligned}$$\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"...to compute the gradient in modular fashion. Note we need to consider the network in its entirety when computing the partial derivatives; the output activation, the loss function, node activations and the biases. To systematically apply back-prop to a network often these functions are abstracted as being an \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"operation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" - which can then be assembled in a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"computational graph\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Given a suitable such graph, many generic back-prop implementations can be used.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Once we have now computed the derivative of the cost function $R(\\\\theta)$, our situation became similar to when we iteratively solved linear- or logistic regression: we can now use just Gradient Descent to move in the error landscape.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now that we know how to train a Neural Network, let's apply it! We aim to get better accuracy for our Penguin classification problem than using our Logistic Regression model.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/neural-metrics.svg\",\"alt\":\"Neural Network fit on a binary penguin classification task. The model converged at 96.5% training-, 94.9% testing accuracy and a loss of 0.108 on the training set.\",\"title\":\"\",\"width\":960,\"height\":336,\"caption\":\"Neural Network fit on a binary penguin classification task. The model converged at 96.5% training-, 94.9% testing accuracy and a loss of 0.108 on the training set.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Indeed, our more flexible Neural Network model better fits the data. The NN achieves 94.9% testing accuracy, in comparison to 89.7% testing accuracy for the Logistic Regression model. Let's see how our model is fitted over time:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/12/neural-fit.gif\",\"alt\":\"Neural Network fit performing a binary classification task on penguin species. Has 3 hidden layers of 5 nodes each; uses $L^2$ regularization with $\\\\alpha=0.0005$ and a constant learning rate of $\\\\gamma=0.001$.\",\"title\":\"\",\"width\":432,\"height\":288,\"caption\":\"Neural Network fit performing a binary classification task on penguin species. Has 3 hidden layers of 5 nodes each; uses $L^2$ regularization with $\\\\alpha=0.0005$ and a constant learning rate of $\\\\gamma=0.001$.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In which it can be observed that the model converged after some 750 iterations. Intuitively, the decision region looks to have been approximated fairly well - it might just have been slightly 'stretched' out.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ending note\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now that we have been able to fit a more 'complicated' data distribution, we conclude our journey from simple statistical models such a linear regression up to Neural Networks. Having a diverse set of statistical and iterative techniques in your tool belt is essential for any Machine Learning practitioner: even though immensely powerful models are available and widespread today, sometimes a simpler model will do just fine.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In tandem with how the bias/variance dilemma is fundamental to understanding how to construct good distribution learning models, one should always take into account not to overreach on model complexity given a learning task (Occam's Razor; Rasmussen et al., 2001): use an as simple as possible model, wherever possible.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Citations\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://doi.org/10.1371/journal.pone.0090081\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://web.stanford.edu/~hastie/ElemStatLearn/\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dogo, E. M., Afolabi, O. J., Nwulu, N. I., Twala, B., & Aigbavboa, C. O. (2018, December). A comparative analysis of gradient descent-based optimization algorithms on convolutional neural networks. In 2018 International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS) (pp. 92-99). IEEE.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://ieeexplore.ieee.org/abstract/document/8769211\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). Deep learning (Vol. 1, No. 2). Cambridge: MIT press.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.deeplearningbook.org/\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":5,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural networks, 12(1), 145-151.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.sciencedirect.com/science/article/pii/S0893608098001166?casa_token=1Cj40vh2xXcAAAAA:Km2rWQK3qSQfFRp5u8RFongBdcCNOAGpBpa3g0nQO3lq7lUSG9ocYx2ExZfaz55dOWsAl102MDc\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":6,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Zhu, C., Byrd, R. H., Lu, P., & Nocedal, J. (1997). Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4), 550-560.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dl.acm.org/doi/abs/10.1145/279232.279236?casa_token=vPvVfjPO5LYAAAAA:HRqyyBJ8KBVy09S8331ZV2pKZOfJrK820r6kuf9kxvpXi5y5DVQxGZzKN4eHeHYBaZ-DGqubi-oUaw\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":7,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 12, 2825-2830.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf?source=post_page---------------------------\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":8,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Nwankpa, C., Ijomah, W., Gachagan, A., & Marshall, S. (2018). Activation functions: Comparison of trends in practice and research for deep learning. arXiv preprint arXiv:1811.03378.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://arxiv.org/abs/1811.03378\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":9,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Rasmussen, C. E., & Ghahramani, Z. (2001). Occam's razor. Advances in neural information processing systems, 294-300.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://books.google.nl/books?hl=en&lr=&id=Mgs2FwtgNxwC&oi=fnd&pg=PA294&dq=occams+razor&ots=EMXQ4ohtev&sig=KRoX-dtpPwJNdPLujn4Qz7O3sI0&redir_esc=y#v=onepage&q&f=false\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":10,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Code\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The code is freely available on Github, see:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2021/11/github32-2.png\",\"width\":32,\"height\":32,\"caption\":\"<a href=\\\"https://github.com/dunnkers/linear-regression-to-neural-networks\\\">linear-regression-to-neural-networks</a>\",\"href\":\"https://github.com/dunnkers/linear-regression-to-neural-networks\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"62a48f659cd77522dc21c27f","uuid":"329c65a7-83dd-4ea4-a363-00bfaa675eea","title":"My programming life when I was young","slug":"dunklunar","mobiledoc":null,"html":"<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://code.google.com/archive/p/dunkscripts/downloads\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Code Archive - Long-term storage for Google Code Project Hosting.</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://code.google.com/archive/img/project-hosting.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Long-term storage for Google Code Project Hosting.</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_116x41dp.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><a href=\"https://www.youtube.com/watch?v=khaP5cAkvcs\">https://www.youtube.com/watch?v=khaP5cAkvcs</a></p><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/2014-04-03-Eight-Media-projecten-overzicht.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"754\" height=\"1403\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/2014-04-03-Eight-Media-projecten-overzicht.png 600w, __GHOST_URL__/content/images/2021/12/2014-04-03-Eight-Media-projecten-overzicht.png 754w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://dunnkers.com/DunkPathMaker/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">DunkPathMaker</div><div class=\"kg-bookmark-description\">DunkPathMaker : A useful tool for RuneScape bot scripters</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://dunnkers.com/DunkPathMaker/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://dunnkers.com/DunkPathMaker/images/osbot-logo.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>","comment_id":"61ab38cc06ec530609ffafe8","plaintext":"Google Code Archive - Long-term storage for Google Code Project Hosting.Long-term storage for Google Code Project Hosting.\n\nhttps://www.youtube.com/watch?v=khaP5cAkvcs\n\n\n\n\n\nDunkPathMakerDunkPathMaker : A useful tool for RuneScape bot scripters","feature_image":"https://images.unsplash.com/photo-1730758405638-ab8659278e96?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8YWxsfDJ8fHx8fHx8fDE3MzIyODI5NDF8&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-12-04 09:45:48","created_by":"1","updated_at":"2024-11-22 13:42:29","updated_by":"1","published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":"custom-full-feature-image","canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"type\":\"bookmark\",\"version\":1,\"url\":\"https://code.google.com/archive/p/dunkscripts/downloads\",\"metadata\":{\"icon\":\"https://code.google.com/archive/img/project-hosting.ico\",\"title\":\"Google Code Archive - Long-term storage for Google Code Project Hosting.\",\"description\":\"\",\"author\":\"\",\"publisher\":\"Long-term storage for Google Code Project Hosting.\",\"thumbnail\":\"https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_116x41dp.png\"},\"caption\":\"\"},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://www.youtube.com/watch?v=khaP5cAkvcs\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=khaP5cAkvcs\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"__GHOST_URL__/content/images/2021/12/2014-04-03-Eight-Media-projecten-overzicht.png\",\"width\":754,\"height\":1403,\"title\":\"\",\"alt\":\"\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"bookmark\",\"version\":1,\"url\":\"https://dunnkers.com/DunkPathMaker/\",\"metadata\":{\"icon\":\"https://dunnkers.com/DunkPathMaker/favicon.ico\",\"title\":\"DunkPathMaker\",\"description\":\"DunkPathMaker : A useful tool for RuneScape bot scripters\",\"author\":\"\",\"publisher\":\"\",\"thumbnail\":\"https://dunnkers.com/DunkPathMaker/images/osbot-logo.png\"},\"caption\":\"\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"634aca22d0d59f00011b7ba1","uuid":"04094669-d46c-44cb-b3a4-45b5b1fa9cd4","title":"pyspark-bucketmap","slug":"pyspark-bucketmap","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"\\nHave you ever heard of pyspark's [`Bucketizer`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html)? It can be really useful! Although you perhaps won't need it for some simple transformation, it can be really useful for certain usecases. \\n\\nIn this blogpost, we will:\\n\\n1. Explore the `Bucketizer` class\\n2. Combine it with `create_map`\\n3. Use a module so we don't have to write the logic ourselves üóùü•≥\\n\\nLet's get started!\\n\\n## The problem\\n\\nFirst, let's boot up a local spark session:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.getOrCreate()\\nspark\\n```\\n\\nSay we have this dataset containing some persons:\\n\\n```python\\nfrom pyspark.sql import Row\\n\\npeople = spark.createDataFrame(\\n    [\\n        Row(age=12, name=\\\"Damian\\\"),\\n        Row(age=15, name=\\\"Jake\\\"),\\n        Row(age=18, name=\\\"Dominic\\\"),\\n        Row(age=20, name=\\\"John\\\"),\\n        Row(age=27, name=\\\"Jerry\\\"),\\n        Row(age=101, name=\\\"Jerry's Grandpa\\\"),\\n    ]\\n)\\npeople\\n```\\n\\nOkay, that's great. Now, what we would like to do, is map each person's age to an age category.\\n\\n|age range|life phase|\\n|-|-|\\n|0 to 12|Child|\\n|12 to 18|Teenager|\\n|18 to 25|Young adulthood|\\n|25 to 70|Adult|\\n|70 and beyond|Elderly|\\n\\nHow best to go about this?\\n\\n## Using `Bucketizer` + `create_map`\\nWe can use pyspark's `Bucketizer` for this. It works like so:\\n\\n```python\\nfrom pyspark.ml.feature import Bucketizer\\nfrom pyspark.sql import DataFrame\\n\\nbucketizer = Bucketizer(\\n    inputCol=\\\"age\\\",\\n    outputCol=\\\"life phase\\\",\\n    splits=[\\n        -float(\\\"inf\\\"), 0, 12, 18, 25, 70, float(\\\"inf\\\")\\n    ]\\n)\\nbucketed: DataFrame = bucketizer.transform(people)\\nbucketed.show()\\n```\\n\\n\\n|age|           name|life phase|\\n|-|-|-|\\n| 12|         Damian|       2.0|\\n| 15|           Jake|       2.0|\\n| 18|        Dominic|       3.0|\\n| 20|           John|       3.0|\\n| 27|          Jerry|       4.0|\\n|101|Jerry's Grandpa|       5.0|\\n\\n\\nCool! We just put our ages in buckets, represented by numbers. Let's now map each bucket to a life phase.\\n\\n\\n```python\\nfrom pyspark.sql.functions import lit, create_map\\nfrom typing import Dict\\nfrom pyspark.sql.column import Column\\n\\nrange_mapper = create_map(\\n    [lit(0.0), lit(\\\"Not yet born\\\")]\\n    + [lit(1.0), lit(\\\"Child\\\")]\\n    + [lit(2.0), lit(\\\"Teenager\\\")]\\n    + [lit(3.0), lit(\\\"Young adulthood\\\")]\\n    + [lit(4.0), lit(\\\"Adult\\\")]\\n    + [lit(5.0), lit(\\\"Elderly\\\")]\\n)\\npeople_phase_column: Column = bucketed[\\\"life phase\\\"]\\npeople_with_phase: DataFrame = bucketed.withColumn(\\n    \\\"life phase\\\", range_mapper[people_phase_column]\\n)\\npeople_with_phase.show()\\n\\n```\\n\\n|age|           name|     life phase|\\n|-|-|-|\\n| 12|         Damian|       Teenager|\\n| 15|           Jake|       Teenager|\\n| 18|        Dominic|Young adulthood|\\n| 20|           John|Young adulthood|\\n| 27|          Jerry|          Adult|\\n|101|Jerry's Grandpa|        Elderly|\\n\\n\\n\\nüéâ Success!\\n\\nUsing a combination of `Bucketizer` and `create_map`, we managed to map people's age to their life phases.\\n\\n## `pyspark-bucketmap`\\n\\nüéÅ As a bonus, I put all of the above in a neat little module, which you can install simply using `pip`.\\n\\n\\n```python\\n%pip install pyspark-bucketmap\\n```\\n\\nDefine the splits and mappings like before. Each dictionary key is a mapping to the n-th bucket (for example, bucket 1 refers to the range `0` to `12`).\\n\\n\\n```python\\nfrom typing import List\\n\\nsplits: List[float] = [-float(\\\"inf\\\"), 0, 12, 18, 25, 70, float(\\\"inf\\\")]\\nmapping: Dict[int, Column] = {\\n    0: lit(\\\"Not yet born\\\"),\\n    1: lit(\\\"Child\\\"),\\n    2: lit(\\\"Teenager\\\"),\\n    3: lit(\\\"Young adulthood\\\"),\\n    4: lit(\\\"Adult\\\"),\\n    5: lit(\\\"Elderly\\\"),\\n}\\n```\\n\\nThen, simply import `pyspark_bucketmap.BucketMap` and call `transform()`.\\n\\n\\n```python\\nfrom pyspark_bucketmap import BucketMap\\nfrom typing import List, Dict\\n\\nbucket_mapper = BucketMap(\\n    splits=splits, mapping=mapping, inputCol=\\\"age\\\", outputCol=\\\"phase\\\"\\n)\\nphases_actual: DataFrame = bucket_mapper.transform(people).select(\\\"name\\\", \\\"phase\\\")\\nphases_actual.show()\\n```\\n\\n|           name|          phase|\\n|-|-|\\n|         Damian|       Teenager|\\n|           Jake|       Teenager|\\n|        Dominic|Young adulthood|\\n|           John|Young adulthood|\\n|          Jerry|          Adult|\\n|Jerry's Grandpa|        Elderly|\\n\\n\\nCheers üôèüèª\\n\\nYou can find the module here:\\n[https://github.com/dunnkers/pyspark-bucketmap](https://github.com/dunnkers/pyspark-bucketmap)\\n\\n\\n---\\n\\nWritten by [Jeroen Overschie](https://jeroenoverschie.nl/), working at [GoDataDriven](https://godatadriven.com/).\\n\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<!--kg-card-begin: markdown--><p>Have you ever heard of pyspark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html\"><code>Bucketizer</code></a>? It can be really useful! Although you perhaps won't need it for some simple transformation, it can be really useful for certain usecases.</p>\n<p>In this blogpost, we will:</p>\n<ol>\n<li>Explore the <code>Bucketizer</code> class</li>\n<li>Combine it with <code>create_map</code></li>\n<li>Use a module so we don't have to write the logic ourselves üóùü•≥</li>\n</ol>\n<p>Let's get started!</p>\n<h2 id=\"the-problem\">The problem</h2>\n<p>First, let's boot up a local spark session:</p>\n<pre><code class=\"language-python\">from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nspark\n</code></pre>\n<p>Say we have this dataset containing some persons:</p>\n<pre><code class=\"language-python\">from pyspark.sql import Row\n\npeople = spark.createDataFrame(\n    [\n        Row(age=12, name=&quot;Damian&quot;),\n        Row(age=15, name=&quot;Jake&quot;),\n        Row(age=18, name=&quot;Dominic&quot;),\n        Row(age=20, name=&quot;John&quot;),\n        Row(age=27, name=&quot;Jerry&quot;),\n        Row(age=101, name=&quot;Jerry's Grandpa&quot;),\n    ]\n)\npeople\n</code></pre>\n<p>Okay, that's great. Now, what we would like to do, is map each person's age to an age category.</p>\n<table>\n<thead>\n<tr>\n<th>age range</th>\n<th>life phase</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0 to 12</td>\n<td>Child</td>\n</tr>\n<tr>\n<td>12 to 18</td>\n<td>Teenager</td>\n</tr>\n<tr>\n<td>18 to 25</td>\n<td>Young adulthood</td>\n</tr>\n<tr>\n<td>25 to 70</td>\n<td>Adult</td>\n</tr>\n<tr>\n<td>70 and beyond</td>\n<td>Elderly</td>\n</tr>\n</tbody>\n</table>\n<p>How best to go about this?</p>\n<h2 id=\"using-bucketizer-createmap\">Using <code>Bucketizer</code> + <code>create_map</code></h2>\n<p>We can use pyspark's <code>Bucketizer</code> for this. It works like so:</p>\n<pre><code class=\"language-python\">from pyspark.ml.feature import Bucketizer\nfrom pyspark.sql import DataFrame\n\nbucketizer = Bucketizer(\n    inputCol=&quot;age&quot;,\n    outputCol=&quot;life phase&quot;,\n    splits=[\n        -float(&quot;inf&quot;), 0, 12, 18, 25, 70, float(&quot;inf&quot;)\n    ]\n)\nbucketed: DataFrame = bucketizer.transform(people)\nbucketed.show()\n</code></pre>\n<table>\n<thead>\n<tr>\n<th>age</th>\n<th>name</th>\n<th>life phase</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>12</td>\n<td>Damian</td>\n<td>2.0</td>\n</tr>\n<tr>\n<td>15</td>\n<td>Jake</td>\n<td>2.0</td>\n</tr>\n<tr>\n<td>18</td>\n<td>Dominic</td>\n<td>3.0</td>\n</tr>\n<tr>\n<td>20</td>\n<td>John</td>\n<td>3.0</td>\n</tr>\n<tr>\n<td>27</td>\n<td>Jerry</td>\n<td>4.0</td>\n</tr>\n<tr>\n<td>101</td>\n<td>Jerry's Grandpa</td>\n<td>5.0</td>\n</tr>\n</tbody>\n</table>\n<p>Cool! We just put our ages in buckets, represented by numbers. Let's now map each bucket to a life phase.</p>\n<pre><code class=\"language-python\">from pyspark.sql.functions import lit, create_map\nfrom typing import Dict\nfrom pyspark.sql.column import Column\n\nrange_mapper = create_map(\n    [lit(0.0), lit(&quot;Not yet born&quot;)]\n    + [lit(1.0), lit(&quot;Child&quot;)]\n    + [lit(2.0), lit(&quot;Teenager&quot;)]\n    + [lit(3.0), lit(&quot;Young adulthood&quot;)]\n    + [lit(4.0), lit(&quot;Adult&quot;)]\n    + [lit(5.0), lit(&quot;Elderly&quot;)]\n)\npeople_phase_column: Column = bucketed[&quot;life phase&quot;]\npeople_with_phase: DataFrame = bucketed.withColumn(\n    &quot;life phase&quot;, range_mapper[people_phase_column]\n)\npeople_with_phase.show()\n\n</code></pre>\n<table>\n<thead>\n<tr>\n<th>age</th>\n<th>name</th>\n<th>life phase</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>12</td>\n<td>Damian</td>\n<td>Teenager</td>\n</tr>\n<tr>\n<td>15</td>\n<td>Jake</td>\n<td>Teenager</td>\n</tr>\n<tr>\n<td>18</td>\n<td>Dominic</td>\n<td>Young adulthood</td>\n</tr>\n<tr>\n<td>20</td>\n<td>John</td>\n<td>Young adulthood</td>\n</tr>\n<tr>\n<td>27</td>\n<td>Jerry</td>\n<td>Adult</td>\n</tr>\n<tr>\n<td>101</td>\n<td>Jerry's Grandpa</td>\n<td>Elderly</td>\n</tr>\n</tbody>\n</table>\n<p>üéâ Success!</p>\n<p>Using a combination of <code>Bucketizer</code> and <code>create_map</code>, we managed to map people's age to their life phases.</p>\n<h2 id=\"pyspark-bucketmap\"><code>pyspark-bucketmap</code></h2>\n<p>üéÅ As a bonus, I put all of the above in a neat little module, which you can install simply using <code>pip</code>.</p>\n<pre><code class=\"language-python\">%pip install pyspark-bucketmap\n</code></pre>\n<p>Define the splits and mappings like before. Each dictionary key is a mapping to the n-th bucket (for example, bucket 1 refers to the range <code>0</code> to <code>12</code>).</p>\n<pre><code class=\"language-python\">from typing import List\n\nsplits: List[float] = [-float(&quot;inf&quot;), 0, 12, 18, 25, 70, float(&quot;inf&quot;)]\nmapping: Dict[int, Column] = {\n    0: lit(&quot;Not yet born&quot;),\n    1: lit(&quot;Child&quot;),\n    2: lit(&quot;Teenager&quot;),\n    3: lit(&quot;Young adulthood&quot;),\n    4: lit(&quot;Adult&quot;),\n    5: lit(&quot;Elderly&quot;),\n}\n</code></pre>\n<p>Then, simply import <code>pyspark_bucketmap.BucketMap</code> and call <code>transform()</code>.</p>\n<pre><code class=\"language-python\">from pyspark_bucketmap import BucketMap\nfrom typing import List, Dict\n\nbucket_mapper = BucketMap(\n    splits=splits, mapping=mapping, inputCol=&quot;age&quot;, outputCol=&quot;phase&quot;\n)\nphases_actual: DataFrame = bucket_mapper.transform(people).select(&quot;name&quot;, &quot;phase&quot;)\nphases_actual.show()\n</code></pre>\n<table>\n<thead>\n<tr>\n<th>name</th>\n<th>phase</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Damian</td>\n<td>Teenager</td>\n</tr>\n<tr>\n<td>Jake</td>\n<td>Teenager</td>\n</tr>\n<tr>\n<td>Dominic</td>\n<td>Young adulthood</td>\n</tr>\n<tr>\n<td>John</td>\n<td>Young adulthood</td>\n</tr>\n<tr>\n<td>Jerry</td>\n<td>Adult</td>\n</tr>\n<tr>\n<td>Jerry's Grandpa</td>\n<td>Elderly</td>\n</tr>\n</tbody>\n</table>\n<p>Cheers üôèüèª</p>\n<p>You can find the module here:<br>\n<a href=\"https://github.com/dunnkers/pyspark-bucketmap\">https://github.com/dunnkers/pyspark-bucketmap</a></p>\n<hr>\n<p>Written by <a href=\"https://jeroenoverschie.nl/\">Jeroen Overschie</a>, working at <a href=\"https://godatadriven.com/\">GoDataDriven</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"634aca22d0d59f00011b7ba1","plaintext":"Have you ever heard of pyspark's Bucketizer? It can be really useful! Although you perhaps won't need it for some simple transformation, it can be really useful for certain usecases.\n\n\nIn this blogpost, we will:\n\n\n 1. Explore the Bucketizer class\n 2. Combine it with create_map\n 3. Use a module so we don't have to write the logic ourselves üóùü•≥\n\n\nLet's get started!\n\n\n\nThe problem\n\n\nFirst, let's boot up a local spark session:\n\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nspark\n\n\n\nSay we have this dataset containing some persons:\n\n\nfrom pyspark.sql import Row\n\npeople = spark.createDataFrame(\n    [\n        Row(age=12, name=\"Damian\"),\n        Row(age=15, name=\"Jake\"),\n        Row(age=18, name=\"Dominic\"),\n        Row(age=20, name=\"John\"),\n        Row(age=27, name=\"Jerry\"),\n        Row(age=101, name=\"Jerry's Grandpa\"),\n    ]\n)\npeople\n\n\n\nOkay, that's great. Now, what we would like to do, is map each person's age to an age category.\n\n\n\n\n\nage range\nlife phase\n\n\n\n\n0 to 12\nChild\n\n\n12 to 18\nTeenager\n\n\n18 to 25\nYoung adulthood\n\n\n25 to 70\nAdult\n\n\n70 and beyond\nElderly\n\n\n\n\n\nHow best to go about this?\n\n\n\nUsing Bucketizer + create_map\n\n\nWe can use pyspark's Bucketizer for this. It works like so:\n\n\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.sql import DataFrame\n\nbucketizer = Bucketizer(\n    inputCol=\"age\",\n    outputCol=\"life phase\",\n    splits=[\n        -float(\"inf\"), 0, 12, 18, 25, 70, float(\"inf\")\n    ]\n)\nbucketed: DataFrame = bucketizer.transform(people)\nbucketed.show()\n\n\n\n\n\n\nage\nname\nlife phase\n\n\n\n\n12\nDamian\n2.0\n\n\n15\nJake\n2.0\n\n\n18\nDominic\n3.0\n\n\n20\nJohn\n3.0\n\n\n27\nJerry\n4.0\n\n\n101\nJerry's Grandpa\n5.0\n\n\n\n\n\nCool! We just put our ages in buckets, represented by numbers. Let's now map each bucket to a life phase.\n\n\nfrom pyspark.sql.functions import lit, create_map\nfrom typing import Dict\nfrom pyspark.sql.column import Column\n\nrange_mapper = create_map(\n    [lit(0.0), lit(\"Not yet born\")]\n    + [lit(1.0), lit(\"Child\")]\n    + [lit(2.0), lit(\"Teenager\")]\n    + [lit(3.0), lit(\"Young adulthood\")]\n    + [lit(4.0), lit(\"Adult\")]\n    + [lit(5.0), lit(\"Elderly\")]\n)\npeople_phase_column: Column = bucketed[\"life phase\"]\npeople_with_phase: DataFrame = bucketed.withColumn(\n    \"life phase\", range_mapper[people_phase_column]\n)\npeople_with_phase.show()\n\n\n\n\n\n\n\nage\nname\nlife phase\n\n\n\n\n12\nDamian\nTeenager\n\n\n15\nJake\nTeenager\n\n\n18\nDominic\nYoung adulthood\n\n\n20\nJohn\nYoung adulthood\n\n\n27\nJerry\nAdult\n\n\n101\nJerry's Grandpa\nElderly\n\n\n\n\n\nüéâ Success!\n\n\nUsing a combination of Bucketizer and create_map, we managed to map people's age to their life phases.\n\n\n\npyspark-bucketmap\n\n\nüéÅ As a bonus, I put all of the above in a neat little module, which you can install simply using pip.\n\n\n%pip install pyspark-bucketmap\n\n\n\nDefine the splits and mappings like before. Each dictionary key is a mapping to the n-th bucket (for example, bucket 1 refers to the range 0 to 12).\n\n\nfrom typing import List\n\nsplits: List[float] = [-float(\"inf\"), 0, 12, 18, 25, 70, float(\"inf\")]\nmapping: Dict[int, Column] = {\n    0: lit(\"Not yet born\"),\n    1: lit(\"Child\"),\n    2: lit(\"Teenager\"),\n    3: lit(\"Young adulthood\"),\n    4: lit(\"Adult\"),\n    5: lit(\"Elderly\"),\n}\n\n\n\nThen, simply import pyspark_bucketmap.BucketMap and call transform().\n\n\nfrom pyspark_bucketmap import BucketMap\nfrom typing import List, Dict\n\nbucket_mapper = BucketMap(\n    splits=splits, mapping=mapping, inputCol=\"age\", outputCol=\"phase\"\n)\nphases_actual: DataFrame = bucket_mapper.transform(people).select(\"name\", \"phase\")\nphases_actual.show()\n\n\n\n\n\n\nname\nphase\n\n\n\n\nDamian\nTeenager\n\n\nJake\nTeenager\n\n\nDominic\nYoung adulthood\n\n\nJohn\nYoung adulthood\n\n\nJerry\nAdult\n\n\nJerry's Grandpa\nElderly\n\n\n\n\n\nCheers üôèüèª\n\n\nYou can find the module here:\n\nhttps://github.com/dunnkers/pyspark-bucketmap\n\n\n\nWritten by Jeroen Overschie, working at GoDataDriven.\n","feature_image":"https://images.unsplash.com/photo-1472141521881-95d0e87e2e39?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fGNhdGVnb3JpZXN8ZW58MHx8fHwxNjg3NzIyOTIzfDA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-10-15 14:56:34","created_by":"1","updated_at":"2023-06-25 19:56:35","updated_by":"1","published_at":"2022-10-22 10:01:09","published_by":"1","custom_excerpt":"pyspark-bucketmap is a tiny module for pyspark which allows you to bucketize DataFrame rows and map their values easily.","codeinjection_head":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css\" />","codeinjection_foot":"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/9000.0.1/components/prism-python.min.js\" integrity=\"sha512-3qtI9+9JXi658yli19POddU1RouYtkTEhTHo6X5ilOvMiDfNvo6GIS6k2Ukrsx8MyaKSXeVrnIWeyH8G5EOyIQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>","custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"636fdea4f90fec00015b93de","uuid":"be1fe6d8-a3a0-4987-a96d-a6da4f2c72a5","title":"Climbing the Mateo","slug":"climbing-the-mateo","mobiledoc":null,"html":"<p>In front of me I see a steep snow slope, where our guide, like us, takes a breath. With crampons under our shoes we have more grip and we slowly clamber up the slope. And all at the same pace, because we are all tied together with a rope, for safety. The goal is to climb the 'Mateo', a mountain of just over 5 km in the Andes mountains. Half an hour later we are at the top and we are rewarded with a beautiful view of the surrounding mountains.</p><p>We went to Huaraz without any equipment, but that was no problem according to the guide - we could borrow everything. So I got a set of gloves with holes and worn out shoes, and we go out. Fortunately, the Mateo is the easiest mountain in the region, and we can afford the poor equipment. The most important thing is to get used to the altitude again first, so we do that by hiking to a few lakes first.</p><p>On the way back from one of the lakes we are looking for a van that will transport us back to the city. But when we arrive at the previously empty village, it is packed with people. That was true, it was a holiday in Peru. But we had not counted on the fact that the traffic around this village would be flat. As we move through the crowd it doesn't take long before we are identified as 'Gringo' and are handed a beer by some exuberant Peruvians. After having a nice toast, we continue, without a mouth cap, because the Peruvians don't do that at a village party.</p><p>After walking in Huaraz we head for Mancora for some warm weather and beach.</p>","comment_id":"636fdea4f90fec00015b93de","plaintext":"In front of me I see a steep snow slope, where our guide, like us, takes a breath. With crampons under our shoes we have more grip and we slowly clamber up the slope. And all at the same pace, because we are all tied together with a rope, for safety. The goal is to climb the 'Mateo', a mountain of just over 5 km in the Andes mountains. Half an hour later we are at the top and we are rewarded with a beautiful view of the surrounding mountains.\n\nWe went to Huaraz without any equipment, but that was no problem according to the guide - we could borrow everything. So I got a set of gloves with holes and worn out shoes, and we go out. Fortunately, the Mateo is the easiest mountain in the region, and we can afford the poor equipment. The most important thing is to get used to the altitude again first, so we do that by hiking to a few lakes first.\n\nOn the way back from one of the lakes we are looking for a van that will transport us back to the city. But when we arrive at the previously empty village, it is packed with people. That was true, it was a holiday in Peru. But we had not counted on the fact that the traffic around this village would be flat. As we move through the crowd it doesn't take long before we are identified as 'Gringo' and are handed a beer by some exuberant Peruvians. After having a nice toast, we continue, without a mouth cap, because the Peruvians don't do that at a village party.\n\nAfter walking in Huaraz we head for Mancora for some warm weather and beach.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-11-12 17:57:56","created_by":"1","updated_at":"2024-11-22 13:42:14","updated_by":"1","published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In front of me I see a steep snow slope, where our guide, like us, takes a breath. With crampons under our shoes we have more grip and we slowly clamber up the slope. And all at the same pace, because we are all tied together with a rope, for safety. The goal is to climb the 'Mateo', a mountain of just over 5 km in the Andes mountains. Half an hour later we are at the top and we are rewarded with a beautiful view of the surrounding mountains.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We went to Huaraz without any equipment, but that was no problem according to the guide - we could borrow everything. So I got a set of gloves with holes and worn out shoes, and we go out. Fortunately, the Mateo is the easiest mountain in the region, and we can afford the poor equipment. The most important thing is to get used to the altitude again first, so we do that by hiking to a few lakes first.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"On the way back from one of the lakes we are looking for a van that will transport us back to the city. But when we arrive at the previously empty village, it is packed with people. That was true, it was a holiday in Peru. But we had not counted on the fact that the traffic around this village would be flat. As we move through the crowd it doesn't take long before we are identified as 'Gringo' and are handed a beer by some exuberant Peruvians. After having a nice toast, we continue, without a mouth cap, because the Peruvians don't do that at a village party.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"After walking in Huaraz we head for Mancora for some warm weather and beach.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"6394bbd42bf4040001d221b6","uuid":"7df60032-cb80-40c3-b60d-862361c009a4","title":"Publications","slug":"publications","mobiledoc":null,"html":"<p>Hi! I sometimes publish stuff: talks, blogposts or research. Here's a list ü§ó.</p><h1 id=\"talks\">Talks</h1><p>üì¢ These are talks I held at tech conferences:</p><ul><li>How to create a Devcontainer for your Python project üê≥<br>@ <a href=\"https://eindhoven2022.pydata.org/cfp/talk/YPULZZ/\">GoDataFest 2022, 26th of October, Amsterdam</a><br>@ <a href=\"https://eindhoven2022.pydata.org/cfp/talk/YPULZZ/\">PyData Eindhoven 2022, 12th of December, Eindhoven</a><br>\t[üé¨ <a href=\"https://www.youtube.com/watch?v=SLsaCdRAV0U\">YouTube</a>]</li><li>Dataset enrichment using LLM‚Äôs ‚ú®<br>@ <a href=\"https://summit.datamass.io/#agenda\">DataMass 2023, 5th of October, Gda≈Ñsk - Poland</a><br>@ <a href=\"https://godatafest.com/broadcasts/data-set-enrichment-using-llms/\">GoDataFest 2023, 24th of October, Amsterdam</a><br>@ <a href=\"https://eindhoven2023.pydata.org/pydata/talk/JMAZDV/\">PyData Eindhoven 2023, 30th of November, Eindhoven</a><br>\t[üé¨ <a href=\"https://www.youtube.com/watch?v=hJeMtABYkEQ\">YouTube</a>]</li><li>Are you ready for MLOps? ü´µ<br>@ <a href=\"https://2024.pycon.sk/en/talks.html\">PyCon SK 2024, 16th of March, Bratislava, Slovakia</a><br>\t[üé¨ <a href=\"https://www.youtube.com/watch?v=PWskUnAsxH4\">YouTube</a>]<br>@ <a href=\"https://london2024.pydata.org/cfp/talk/3AU7BP/\">PyData London 2024, 16th of June, London</a><br>        [üé¨ <a href=\"https://www.youtube.com/watch?v=9E-97c5j-Co\" rel=\"noreferrer\">YouTube</a>]<br>@ <a href=\"https://www.meetup.com/eindhoven-data-community/events/301197610/?eventOrigin=group_events_list\">Eindhoven Data Community meetup, 29th of August 2024, Eindhoven</a></li><li>From Prototype to Production:‚Äã A Practical Guide to‚Äã LLMOps Platforms for‚Äã RAG Applications on Azure‚Äã<br>@ <a href=\"https://www.aicommunityday.nl\">AI Community Day 2024, 14th of May, Utrecht</a></li><li>How Klaverblad Insurance is enhancing its claim handling using GenAI<br>@ <a href=\"https://www.data-expo.nl/nl/bezoekers/programma/how-klaverblad-insurance-is-enhancing-its-claim-handling-using-genai\">Data Expo 2024, 11th of September, Utrecht</a></li><li>The Levels of RAG ü¶ú<br>@ <a href=\"https://eindhoven2024.pydata.org/cfp/talk/G8VWGY/\">PyData Eindhoven 2024, 11th of July, Eindhoven</a><br>    [üé¨ <a href=\"https://www.youtube.com/watch?v=SpGZr2gzDQM\">YouTube</a>]</li></ul><h1 id=\"blogposts\">Blogposts</h1><ul><li><a href=\"https://xebia.com/blog/dropblox-coding-challenge-at-pycon-de-pydata-berlin-2022/\">DropBlox: Coding Challenge at PyCon DE &amp; PyData Berlin 2022</a><br>27th of July 2022, xebia.com</li><li><a href=\"https://godatadriven.com/blog/how-to-create-a-devcontainer-for-your-python-project-%F0%9F%90%B3/\">How to create a Devcontainer for your Python project üê≥</a><br>21st of November 2022, xebia.com</li><li><a href=\"https://xebia.com/blog/scaling-up-your-azure-devops-ci-cd-setup/\">Scaling up: bringing your Azure Devops CI/CD setup to the next level üöÄ</a><br>8th of December 2023, xebia.com</li><li><a href=\"https://xebia.com/blog/dataset-enrichment-using-llms/\">Dataset enrichment using LLM's ‚ú®</a><br>28th of December 2023, xebia.com</li><li><a href=\"https://xebia.com/blog/rag-on-gcp/\">RAG on GCP: production-ready GenAI on Google Cloud Platform</a><br>26th of March 2024, xebia.com</li><li><a href=\"https://xebia.com/blog/levels-of-rag/\">The Levels of RAG</a><br>26th of July 2024, xebia.com</li></ul><h1 id=\"research\">Research</h1><p>üìö Academic articles that are accessible online.</p><ul><li>fseval: A Benchmarking Framework for Feature Selection and Feature Ranking Algorithms<br>‚Äì <a href=\"https://orcid.org/0000-0003-3304-3800\">Jeroen G. S. Overschie</a>, <a href=\"https://joss.theoj.org/papers/by/Ahmad%20Alsahaf\">Ahmad Alsahaf</a> and <a href=\"https://joss.theoj.org/papers/by/George%20Azzopardi\">George Azzopardi</a><br>Published in the <strong>Journal of Open Source Software</strong> on 23rd of November, 2022<br>[<a href=\"https://github.com/dunnkers/fseval\">code</a>] [<a href=\"https://www.theoj.org/joss-papers/joss.04611/10.21105.joss.04611.pdf\">pdf</a>] [<a href=\"https://joss.theoj.org/papers/10.21105/joss.04611\">journal</a>]</li><li>A novel evaluation methodology for supervised Feature Ranking algorithms<br>‚Äì Jeroen G. S. Overschie<br>Master's thesis, published 9th of July 2022<br>[<a href=\"https://github.com/dunnkers/msc-thesis\">code</a>] [<a href=\"https://arxiv.org/abs/2207.04258\">pdf</a>] [<a href=\"https://arxiv.org/abs/2207.04258\">arxiv</a>]</li><li>A fusion between a deep-learning and a filter-based approach for road marker detection<br>‚Äì Jeroen G. S. Overschie<br>Bachelor's thesis, published August 2019<br>[<a href=\"https://github.com/dunnkers/bsc-thesis\">code</a>] [<a href=\"https://github.com/dunnkers/bsc-thesis/blob/master/report.pdf\">pdf</a>]</li></ul><p></p><h2 id=\"other\">Other</h2><ul><li>Machine Learning Engineer - Working at GoDataDriven<br>    [üé¨ <a href=\"https://www.youtube.com/watch?v=hoK0EmnulS8\">YouTube</a>]</li></ul><p></p>","comment_id":"6394bbd42bf4040001d221b6","plaintext":"Hi! I sometimes publish stuff: talks, blogposts or research. Here's a list ü§ó.\n\n\nTalks\n\nüì¢ These are talks I held at tech conferences:\n\n * How to create a Devcontainer for your Python project üê≥\n   @ GoDataFest 2022, 26th of October, Amsterdam\n   @ PyData Eindhoven 2022, 12th of December, Eindhoven\n   [üé¨ YouTube]\n * Dataset enrichment using LLM‚Äôs ‚ú®\n   @ DataMass 2023, 5th of October, Gda≈Ñsk - Poland\n   @ GoDataFest 2023, 24th of October, Amsterdam\n   @ PyData Eindhoven 2023, 30th of November, Eindhoven\n   [üé¨ YouTube]\n * Are you ready for MLOps? ü´µ\n   @ PyCon SK 2024, 16th of March, Bratislava, Slovakia\n   [üé¨ YouTube]\n   @ PyData London 2024, 16th of June, London\n   [üé¨ YouTube]\n   @ Eindhoven Data Community meetup, 29th of August 2024, Eindhoven\n * From Prototype to Production: A Practical Guide to LLMOps Platforms for RAG Applications on Azure\n   @ AI Community Day 2024, 14th of May, Utrecht\n * How Klaverblad Insurance is enhancing its claim handling using GenAI\n   @ Data Expo 2024, 11th of September, Utrecht\n * The Levels of RAG ü¶ú\n   @ PyData Eindhoven 2024, 11th of July, Eindhoven\n   [üé¨ YouTube]\n\n\nBlogposts\n\n * DropBlox: Coding Challenge at PyCon DE & PyData Berlin 2022\n   27th of July 2022, xebia.com\n * How to create a Devcontainer for your Python project üê≥\n   21st of November 2022, xebia.com\n * Scaling up: bringing your Azure Devops CI/CD setup to the next level üöÄ\n   8th of December 2023, xebia.com\n * Dataset enrichment using LLM's ‚ú®\n   28th of December 2023, xebia.com\n * RAG on GCP: production-ready GenAI on Google Cloud Platform\n   26th of March 2024, xebia.com\n * The Levels of RAG\n   26th of July 2024, xebia.com\n\n\nResearch\n\nüìö Academic articles that are accessible online.\n\n * fseval: A Benchmarking Framework for Feature Selection and Feature Ranking Algorithms\n   ‚Äì Jeroen G. S. Overschie, Ahmad Alsahaf and George Azzopardi\n   Published in the Journal of Open Source Software on 23rd of November, 2022\n   [code] [pdf] [journal]\n * A novel evaluation methodology for supervised Feature Ranking algorithms\n   ‚Äì Jeroen G. S. Overschie\n   Master's thesis, published 9th of July 2022\n   [code] [pdf] [arxiv]\n * A fusion between a deep-learning and a filter-based approach for road marker detection\n   ‚Äì Jeroen G. S. Overschie\n   Bachelor's thesis, published August 2019\n   [code] [pdf]\n\n\n\n\nOther\n\n * Machine Learning Engineer - Working at GoDataDriven\n   [üé¨ YouTube]\n\n","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-12-10 17:03:16","created_by":"1","updated_at":"2024-11-07 14:38:55","updated_by":"1","published_at":"2022-12-10 17:17:18","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hi! I sometimes publish stuff: talks, blogposts or research. Here's a list ü§ó.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Talks\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"üì¢ These are talks I held at tech conferences:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How to create a Devcontainer for your Python project üê≥\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GoDataFest 2022, 26th of October, Amsterdam\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://eindhoven2022.pydata.org/cfp/talk/YPULZZ/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyData Eindhoven 2022, 12th of December, Eindhoven\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://eindhoven2022.pydata.org/cfp/talk/YPULZZ/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"\\t[üé¨ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"YouTube\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=SLsaCdRAV0U\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dataset enrichment using LLM‚Äôs ‚ú®\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"DataMass 2023, 5th of October, Gda≈Ñsk - Poland\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://summit.datamass.io/#agenda\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GoDataFest 2023, 24th of October, Amsterdam\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://godatafest.com/broadcasts/data-set-enrichment-using-llms/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyData Eindhoven 2023, 30th of November, Eindhoven\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://eindhoven2023.pydata.org/pydata/talk/JMAZDV/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"\\t[üé¨ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"YouTube\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=hJeMtABYkEQ\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Are you ready for MLOps? ü´µ\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyCon SK 2024, 16th of March, Bratislava, Slovakia\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://2024.pycon.sk/en/talks.html\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"\\t[üé¨ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"YouTube\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=PWskUnAsxH4\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyData London 2024, 16th of June, London\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://london2024.pydata.org/cfp/talk/3AU7BP/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"        [üé¨ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"YouTube\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noreferrer\",\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=9E-97c5j-Co\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Eindhoven Data Community meetup, 29th of August 2024, Eindhoven\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.meetup.com/eindhoven-data-community/events/301197610/?eventOrigin=group_events_list\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"From Prototype to Production:‚Äã A Practical Guide to‚Äã LLMOps Platforms for‚Äã RAG Applications on Azure‚Äã\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"AI Community Day 2024, 14th of May, Utrecht\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.aicommunityday.nl\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":4},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How Klaverblad Insurance is enhancing its claim handling using GenAI\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Data Expo 2024, 11th of September, Utrecht\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.data-expo.nl/nl/bezoekers/programma/how-klaverblad-insurance-is-enhancing-its-claim-handling-using-genai\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":5},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The Levels of RAG ü¶ú\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"@ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyData Eindhoven 2024, 11th of July, Eindhoven\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://eindhoven2024.pydata.org/cfp/talk/G8VWGY/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"    [üé¨ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"YouTube\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=SpGZr2gzDQM\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":6}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Blogposts\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"DropBlox: Coding Challenge at PyCon DE & PyData Berlin 2022\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/blog/dropblox-coding-challenge-at-pycon-de-pydata-berlin-2022/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"27th of July 2022, xebia.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How to create a Devcontainer for your Python project üê≥\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://godatadriven.com/blog/how-to-create-a-devcontainer-for-your-python-project-%F0%9F%90%B3/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"21st of November 2022, xebia.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Scaling up: bringing your Azure Devops CI/CD setup to the next level üöÄ\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/blog/scaling-up-your-azure-devops-ci-cd-setup/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"8th of December 2023, xebia.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dataset enrichment using LLM's ‚ú®\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/blog/dataset-enrichment-using-llms/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"28th of December 2023, xebia.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":4},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG on GCP: production-ready GenAI on Google Cloud Platform\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/blog/rag-on-gcp/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"26th of March 2024, xebia.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":5},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The Levels of RAG\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/blog/levels-of-rag/\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"26th of July 2024, xebia.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":6}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Research\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"üìö Academic articles that are accessible online.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"fseval: A Benchmarking Framework for Feature Selection and Feature Ranking Algorithms\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äì \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Jeroen G. S. Overschie\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://orcid.org/0000-0003-3304-3800\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ahmad Alsahaf\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://joss.theoj.org/papers/by/Ahmad%20Alsahaf\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"George Azzopardi\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://joss.theoj.org/papers/by/George%20Azzopardi\"},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Published in the \",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Journal of Open Source Software\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" on 23rd of November, 2022\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"code\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/dunnkers/fseval\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"] [\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pdf\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.theoj.org/joss-papers/joss.04611/10.21105.joss.04611.pdf\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"] [\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"journal\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://joss.theoj.org/papers/10.21105/joss.04611\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A novel evaluation methodology for supervised Feature Ranking algorithms\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äì Jeroen G. S. Overschie\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Master's thesis, published 9th of July 2022\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"code\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/dunnkers/msc-thesis\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"] [\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pdf\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://arxiv.org/abs/2207.04258\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"] [\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"arxiv\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://arxiv.org/abs/2207.04258\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A fusion between a deep-learning and a filter-based approach for road marker detection\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äì Jeroen G. S. Overschie\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Bachelor's thesis, published August 2019\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"code\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/dunnkers/bsc-thesis\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"] [\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pdf\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/dunnkers/bsc-thesis/blob/master/report.pdf\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Other\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Machine Learning Engineer - Working at GoDataDriven\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"    [üé¨ \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"YouTube\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.youtube.com/watch?v=hoK0EmnulS8\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"6394c3a02bf4040001d22235","uuid":"1448c9dd-a2fe-4a4a-be4e-2a7d6b2c25ce","title":"How to create a Devcontainer for your Python project üê≥ (on Xebia.com ‚ßâ)","slug":"how-to-create-a-devcontainer-for-your-python-project","mobiledoc":null,"html":"<p>Imagine the following scenario üí≠.</p><p>Your company uses Apache Spark to process data, and your team has pyspark set up in a Python project. The codebase is built on a specific Python version, using a certain Java installation, and an accompanying pyspark version that works with the former. To onboard a new member, you will need to pass a list of instructions the developer needs to follow carefully to get their setup working. But not everyone might run this on the same laptop environment: different hardware, and different operating systems. This is getting challenging.<br>But the setup is a one-off, right? Just go through the setup once and you‚Äôll be good. Not entirely. Your code environment will change over time: your team will probably install-, update- or remove packages during the project‚Äôs development. This means that if a developer creates a new feature and changes their own environment to do so; he or she also needs to make sure that the other team members change theirs and that the production environment is updated accordingly. This makes it easy to get misaligned environments: between developers, and between development &amp; production.</p><p>We can do better than this! Instead of giving other developers a setup document, let‚Äôs make sure we also create formal instructions so we can <em>automatically</em> set up the development environment. <em>Devcontainers</em> let us do exactly this.</p><p><strong>Devcontainers</strong> let you connect your IDE to a running Docker container. In this way, we get the benefits of reproducibility and isolation, whilst getting a native development experience.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/devcontainer-overview-4.png.webp\" class=\"kg-image\" alt=\"Devcontainer usage overview\" loading=\"lazy\"><figcaption>With Devcontainers you can interact with your IDE like you're used to whilst under the hood running everything inside a Docker container.</figcaption></figure><p></p><p>Devcontainers can help us:</p><ul><li>Get a reproducible development environment</li><li>‚ö°Ô∏è Instantly onboard new team members onto your project</li><li>‚Äç ‚Äç ‚Äç Better align the environments between team members</li><li>‚è± Keeping your dev environment up-to-date &amp; reproducible saves your team time going into production later</li></ul><p>Let‚Äôs explore how we can set up a Devcontainer for your Python project!</p><h2 id=\"creating-your-first-devcontainer\"><strong>Creating your first Devcontainer</strong></h2><p>Note that this tutorial is focused on <strong>VSCode</strong>. Other IDE‚Äôs like PyCharm support running in Docker containers but support is less comprehensive than on VSCode.</p><h3 id=\"recap\"><strong>Recap</strong></h3><p>To recap, we are trying to create a dev environment that installs: 1) Java, 2) Python and 3) pyspark. And we want to do so <em>automatically</em>, that is, inside a Docker image.</p><h3 id=\"project-structure\"><strong>Project structure</strong></h3><p>Let‚Äôs say we have a really simple project that looks like this:</p><pre><code class=\"language-bash\">$ tree .\n.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ requirements-dev.txt\n‚îú‚îÄ‚îÄ sales_analysis.py\n‚îî‚îÄ‚îÄ test_sales_analysis.py</code></pre><p>That is, we have a Python module with an accompanying test, a <code>requirements.txt</code> file describing our production dependencies (pyspark), and a <code>requirements-dev.txt</code> describing dependencies that should be installed in development only (pytest, black, mypy). Now let‚Äôs see how we can extend this setup to include a Devcontainer.</p><h3 id=\"the-devcontainer-folder\"><strong>The <code>.devcontainer</code> folder</strong></h3><p>Your Devcontainer spec will live inside the <code>.devcontainer</code> folder. There will be two main files:</p><ul><li><code>devcontainer.json</code></li><li><code>Dockerfile</code></li></ul><p>Create a new file called <code>devcontainer.json</code>:</p><pre><code class=\"language-json\">{\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\",\n        \"context\": \"..\"\n    }\n}</code></pre><p>This means: as a base for our Devcontainer, use the <code>Dockerfile</code> located in the current directory, and build it with a <em>current working directory</em> (cwd) of <code>..</code>.</p><p>So what does this <code>Dockerfile</code> look like?</p><pre><code class=\"language-docker\">FROM python:3.10\n\n# Install Java\nRUN apt update &amp;&amp; \n    apt install -y sudo &amp;&amp; \n    sudo apt install default-jdk -y\n\n## Pip dependencies\n# Upgrade pip\nRUN pip install --upgrade pip\n# Install production dependencies\nCOPY requirements.txt /tmp/requirements.txt\nRUN pip install -r /tmp/requirements.txt &amp;&amp; \n    rm /tmp/requirements.txt\n# Install development dependencies\nCOPY requirements-dev.txt /tmp/requirements-dev.txt\nRUN pip install -r /tmp/requirements-dev.txt &amp;&amp; \n    rm /tmp/requirements-dev.txt</code></pre><p>We are building our image on top of <code>python:3.10</code>, which is a Debian-based image. This is one of the Linux distributions that a Devcontainer can be built on. The main requirement is that <strong>Node.js</strong> should be able to run: VSCode automatically installs VSCode Server on the machine. For an extensive list of supported distributions, see <a href=\"https://code.visualstudio.com/docs/remote/linux\">‚ÄúRemote Development with Linux‚Äù</a>.</p><p>On top of <code>python:3.10</code>, we install Java and the required pip packages.</p><h3 id=\"opening-the-devcontainer\"><strong>Opening the Devcontainer</strong></h3><p>The <code>.devcontainer</code> folder is in place, so it‚Äôs now time to open our Devcontainer.</p><p>First, make sure you have the <a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers\">Dev Containers extension</a> installed in VSCode (previously called ‚ÄúRemote ‚Äì Containers‚Äù. That done, if you open your repo again, the extension should already detect your Devcontainer:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/folder-contains-a-dev-container-config-file.png.webp\" class=\"kg-image\" alt=\"folder contains a dev container config file\" loading=\"lazy\"></figure><p>Alternatively, you can open up the command palette (CMD + Shift + P) and select ‚Äú<em>Dev Containers: Reopen in Container</em>‚Äù:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/reopen-in-devcontainer-notification.png.webp\" class=\"kg-image\" alt=\"Dev Containers: Reopen in Container\" loading=\"lazy\"></figure><p>Your VSCode is now connected to the Docker container :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/opening-the-devcontainer.gif.webp\" class=\"kg-image\" alt=\"VSCode is now connected to the Docker container\" loading=\"lazy\"></figure><h3 id=\"what-is-happening-under-the-hood\"><strong>What is happening under the hood</strong></h3><p>Besides starting the Docker image and attaching the terminal to it, VSCode is doing a couple more things:</p><ol><li><a href=\"https://code.visualstudio.com/docs/remote/vscode-server\"><strong>VSCode Server</strong></a> is being installed on your Devcontainer. VSCode Server is installed as a service in the container itself so your VSCode installation can communicate with the container. For example, install and run extensions.</li><li><strong>Config is copied</strong> over. Config like <code>~/.gitconfig</code> and <code>~/.ssh/known_hosts</code> are copied over to their respective locations in the container.<br>This then allows you to use your Git repo like you do normally, without re-authenticating.</li><li><strong>Filesystem mounts</strong>. VSCode automatically takes care of mounting: 1) The folder you are running the Devcontainer from and 2) your VSCode workspace folder.</li></ol><h3 id=\"opening-your-repo-directly-in-a-devcontainer\"><strong>Opening your repo directly in a Devcontainer</strong></h3><p>Since all instructions on how to configure your dev environment are now defined in a Dockerfile, users can open up your Devcontainer with just one button:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/11/v1.svg\" class=\"kg-image\" alt=\"Open in Remote - Containers\" loading=\"lazy\"></figure><p>Ain‚Äôt that cool? You can add a button to your repo like so:</p><pre><code>[\n    ![Open in Remote - Containers](\n        https://xebia.com/wp-content/uploads/2023/11/v1.svg    )\n](\n    https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/godatadriven/python-devcontainer-template\n)</code></pre><p>Just modify the GitHub URL ‚úì.</p><p>That said, we can see having built a Devcontainer can make our README massively more readable. What kind of README would you rather like?</p><!--kg-card-begin: html--><table style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse; color: rgb(53, 68, 90); font-family: Proxima, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><thead style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><th style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\">Manual installation</th><th style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\">Using a Devcontainer</th></tr></thead><tbody style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\"><img decoding=\"async\" src=\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-manual.png.webp\" alt=\"\" data-src-img=\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-manual.png\" data-src-webp=\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-manual.png.webp\" data-eio=\"j\" class=\"ewww_webp_loaded\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"></td><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\"><img decoding=\"async\" src=\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-devcontainer.png.webp\" alt=\"\" data-src-img=\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-devcontainer.png\" data-src-webp=\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-devcontainer.png.webp\" data-eio=\"j\" class=\"ewww_webp_loaded\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"></td></tr></tbody></table><!--kg-card-end: html--><h2 id=\"extending-the-devcontainer\"><strong>Extending the Devcontainer</strong></h2><p>We have built a working Devcontainer, which is great! But a couple of things are still missing. We still want to:</p><ul><li>Install a non-root user for extra safety and good-practice</li><li>Pass in custom VSCode settings and install extensions by default</li><li>Be able to access Spark UI (port 4040)</li><li>Run Continuous Integration (CI) in the Devcontainer</li></ul><p>Let‚Äôs see how.</p><h3 id=\"installing-a-non-root-user\"><strong>Installing a non-root user</strong></h3><p>If you <code>pip install</code> a new package, you will see the following message:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/running-pip-as-root.png.webp\" class=\"kg-image\" alt=\"The warning message: ‚Äú*WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: [https://pip.pypa.io/warnings/venv](https://pip.pypa.io/warnings/venv)*\" loading=\"lazy\"></figure><p>Indeed, it is not recommended to develop as a <em>root</em> user. It is considered a good practice to create a different user with fewer rights to run in production. So let‚Äôs go ahead and create a user for this scenario.</p><pre><code class=\"language-bash\"># Add non-root user\nARG USERNAME=nonroot\nRUN groupadd --gid 1000 $USERNAME &amp;&amp; \n    useradd --uid 1000 --gid 1000 -m $USERNAME\n## Make sure to reflect new user in PATH\nENV PATH=\"/home/${USERNAME}/.local/bin:${PATH}\"\nUSER $USERNAME</code></pre><p>Add the following property to <code>devcontainer.json</code>:</p><pre><code class=\"language-json\">    \"remoteUser\": \"nonroot\"</code></pre><p>That‚Äôs great! When we now start the container we should connect as the user <code>nonroot</code>.</p><h3 id=\"passing-custom-vscode-settings\"><strong>Passing custom VSCode settings</strong></h3><p>Our Devcontainer is still a bit bland, without extensions and settings. Besides any custom extensions a user might want to install, we can install some for them by default already. We can define such settings in <code>customizations.vscode</code>:</p><pre><code class=\"language-json\">     \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-python.python\"\n            ],\n            \"settings\": {\n                \"python.testing.pytestArgs\": [\n                    \".\"\n                ],\n                \"python.testing.unittestEnabled\": false,\n                \"python.testing.pytestEnabled\": true,\n                \"python.formatting.provider\": \"black\",\n                \"python.linting.mypyEnabled\": true,\n                \"python.linting.enabled\": true\n            }\n        }\n    }</code></pre><p>The defined extensions are always installed in the Devcontainer. However, the defined settings provide just a <strong>default</strong> for the user to use, and can still be overridden by other setting scopes like User Settings, Remote Settings, or Workspace Settings.</p><h3 id=\"accessing-spark-ui\"><strong>Accessing Spark UI</strong></h3><p>Since we are using pyspark, we want to be able to access <strong>Spark UI</strong>. When we start a Spark session, VSCode will ask whether you want to forward the specific port. Since we already know this is Spark UI, we can do so automatically:</p><pre><code class=\"language-json\">    \"portsAttributes\": {\n        \"4040\": {\n            \"label\": \"SparkUI\",\n            \"onAutoForward\": \"notify\"\n        }\n    },\n\n    \"forwardPorts\": [\n        4040\n    ]</code></pre><p>When we now run our code, we get a notification we can open Spark UI in the browser:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/application-running-on-port-4040.png.webp\" class=\"kg-image\" alt=\"open Spark UI in the browser\" loading=\"lazy\"></figure><p>Resulting in the Spark UI as we know it:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/spark-ui-visible-in-localhost-4040.png.webp\" class=\"kg-image\" alt=\"spark UI in the browser\" loading=\"lazy\"></figure><p>‚ú®</p><h3 id=\"running-our-ci-in-the-devcontainer\"><strong>Running our CI in the Devcontainer</strong></h3><p>Wouldn‚Äôt it be convenient if we could re-use our Devcontainer to run our Continuous Integration (CI) pipeline as well? Indeed, we can do this with Devcontainers. Similarly to how the Devcontainer image is built locally using <code>docker build</code>, the same can be done <em>within</em> a CI/CD pipeline. There are two basic options:</p><ol><li>Build the Docker image <em>within</em> the CI/CD pipeline</li><li>Prebuilding the image</li></ol><p>To pre-build the image, the build step will need to run either periodically or whenever the Docker definition has changed. Since this adds quite some complexity let‚Äôs dive into building the Devcontainer as part of the CI/CD pipeline first (for pre-building the image, see the ‚ÄòAwesome resources‚Äô section). We will do so using <strong>GitHub Actions</strong>.</p><h4 id=\"using-devcontainersci\"><strong>Using <code>devcontainers/ci</code></strong></h4><p>Luckily, a GitHub Action was already set up for us to do exactly this:</p><p><a href=\"https://github.com/devcontainers/ci\">https://github.com/devcontainers/ci</a></p><p>To now build, push and run a command in the Devcontainer is as easy as:</p><pre><code class=\"language-yaml\">name: Python app\n\non:\n  pull_request:\n  push:\n    branches:\n      - \"**\"\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout (GitHub)\n        uses: actions/checkout@v3\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v2\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build and run dev container task\n        uses: devcontainers/ci@v0.2\n        with:\n          imageName: ghcr.io/${{ github.repository }}/devcontainer\n          runCmd: pytest .</code></pre><p>That‚Äôs great! Whenever this workflow runs on your main branch, the image will be pushed to the configured registry; in this case GitHub Container Registry (GHCR). See below a trace of the executed GitHub Action:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/running-ci-in-the-devcontainer-github-actions.png.webp\" class=\"kg-image\" alt=\"running-ci-in-the-devcontainer-github-actions\" loading=\"lazy\"></figure><p>Awesome!</p><h2 id=\"the-final-devcontainer-definition\"><strong>The final Devcontainer definition</strong></h2><p>We built the following Devcontainer definitions. First, <code>devcontainer.json</code>:</p><pre><code class=\"language-json\">{\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\",\n        \"context\": \"..\"\n    },\n\n    \"remoteUser\": \"nonroot\",\n\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-python.python\"\n            ],\n            \"settings\": {\n                \"python.testing.pytestArgs\": [\n                    \".\"\n                ],\n                \"python.testing.unittestEnabled\": false,\n                \"python.testing.pytestEnabled\": true,\n                \"python.formatting.provider\": \"black\",\n                \"python.linting.mypyEnabled\": true,\n                \"python.linting.enabled\": true\n            }\n        }\n    },\n\n    \"portsAttributes\": {\n        \"4040\": {\n            \"label\": \"SparkUI\",\n            \"onAutoForward\": \"notify\"\n        }\n    },\n\n    \"forwardPorts\": [\n        4040\n    ]\n}</code></pre><p>And our <code>Dockerfile</code>:</p><pre><code class=\"language-docker\">FROM python:3.10\n\n# Install Java\nRUN apt update &amp;&amp; \n    apt install -y sudo &amp;&amp; \n    sudo apt install default-jdk -y\n\n# Add non-root user\nARG USERNAME=nonroot\nRUN groupadd --gid 1000 $USERNAME &amp;&amp; \n    useradd --uid 1000 --gid 1000 -m $USERNAME\n## Make sure to reflect new user in PATH\nENV PATH=\"/home/${USERNAME}/.local/bin:${PATH}\"\nUSER $USERNAME\n\n## Pip dependencies\n# Upgrade pip\nRUN pip install --upgrade pip\n# Install production dependencies\nCOPY --chown=nonroot:1000 requirements.txt /tmp/requirements.txt\nRUN pip install -r /tmp/requirements.txt &amp;&amp; \n    rm /tmp/requirements.txt\n# Install development dependencies\nCOPY --chown=nonroot:1000 requirements-dev.txt /tmp/requirements-dev.txt\nRUN pip install -r /tmp/requirements-dev.txt &amp;&amp; \n    rm /tmp/requirements-dev.txt</code></pre><blockquote>The full Devcontainer implementation and all the above steps can be found in the various branches of the <a href=\"https://github.com/godatadriven/python-devcontainer-template\">godatadriven/python-devcontainer-template</a> repo.</blockquote><h2 id=\"docker-images-architecture-three-environments\"><strong>Docker images architecture: Three environments</strong></h2><p>With the CI now set up, we can reuse the same Docker image for two purposes. For local development and running our quality checks. And, once we deploy this application to production, we could configure the Devcontainer to use our production image as a base, and install extra dependencies on top. If we want to optimize the CI image to be as lightweight as possible, we could also strip off any extra dependencies that we do not require in the CI environment; things as extra CLI tooling, a better shell such as ZSH, and so forth.</p><p>This sets us up for having 3 different images for our entire lifecycle. One for Development, one for CI, and finally one for production. This can be visualized like so:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/three-environments.png.webp\" class=\"kg-image\" alt=\"three-environments-docker-images-devcontainer\" loading=\"lazy\"></figure><p>So, we can see, when using a Devcontainer you can re-use your production image and build on top of it. Install extra tooling, make sure it can talk to VSCode, and you‚Äôre done .</p><h2 id=\"going-further\"><strong>Going further</strong></h2><p>There are lots of other resources to explore; Devcontainers are well-documented and there are many posts about it. If you‚Äôre up for more, let‚Äôs see what else you can do.</p><h3 id=\"devcontainer-features\"><strong>Devcontainer features</strong></h3><p>Devcontainer <a href=\"https://containers.dev/features\">features</a> allow you to easily extend your Docker definition with common additions. Some useful features are:</p><ul><li><a href=\"https://github.com/devcontainers/features/tree/main/src/common-utils\">Common Debian Utilities</a> (Installs ZSH using <em>Oh My ZSH</em>, a non-root user, and useful CLI tools like <code>curl</code>)</li><li><a href=\"https://github.com/devcontainers/features/tree/main/src/aws-cli\">AWS CLI</a></li><li><a href=\"https://github.com/devcontainers/features/tree/main/src/azure-cli\">Azure CLI</a></li><li><a href=\"https://github.com/devcontainers/features/tree/main/src/git\">Git</a></li><li><a href=\"https://github.com/devcontainers/features/tree/main/src/node\">Node.js</a></li><li><a href=\"https://github.com/devcontainers/features/tree/main/src/python\">Python</a></li><li><a href=\"https://github.com/devcontainers/features/tree/main/src/java\">Java</a></li></ul><h3 id=\"devcontainer-templates\"><strong>Devcontainer templates</strong></h3><p>On the official Devcontainer specification website there are <strong>loads</strong> of templates available. Good chance (part of) your setup is in there. A nice way to get a head-start in building your Devcontainer or to get started quickly.</p><p>See: <a href=\"https://containers.dev/templates\">https://containers.dev/templates</a></p><h3 id=\"mounting-directories\"><strong>Mounting directories</strong></h3><p>Re-authenticating your CLI tools is annoying. So one trick is to mount your AWS/Azure/GCP credentials from your local computer into your Devcontainer. This way, authentications done in either environment are shared with the other. You can easily do this by adding this to <code>devcontainer.json</code>:</p><pre><code class=\"language-json\">  \"mounts\": [\n    \"source=/Users/&lt;your_username&gt;/.aws,target=/home/nonroot/.aws,type=bind,consistency=cached\"\n  ]</code></pre><p>^ the above example mounts your AWS credentials, but the process should be similar for other cloud providers (GCP / Azure).</p><h3 id=\"awesome-resources\"><strong>Awesome resources</strong></h3><ul><li><a href=\"https://github.com/devcontainers/ci\">devcontainers/ci</a>. Run your CI in your Devcontainers. Built on the <a href=\"https://github.com/devcontainers/cli\">Devcontainer CLI</a>.</li><li><a href=\"https://containers.dev/\">https://containers.dev/</a>. The official Devcontainer specification.</li><li><a href=\"https://github.com/devcontainers/images\">devcontainers/images</a>. A collection of ready-to-use Devcontainer images.</li><li><a href=\"https://code.visualstudio.com/remote/advancedcontainers/add-nonroot-user\">Add a non-root user to a container</a>. More explanations &amp; instructions for adding a non-root user to your <code>Dockerfile</code> and <code>devcontainer.json</code>.</li><li><a href=\"https://code.visualstudio.com/docs/remote/containers#_prebuilding-dev-container-images\">Pre-building dev container images</a></li><li><a href=\"https://github.com/manekinekko/awesome-devcontainers\">awesome-devcontainers</a>. A repo pointing to yet even more awesome resources.</li></ul><h2 id=\"concluding\"><strong>Concluding</strong></h2><p>Devcontainers allow you to connect your IDE to a running Docker container, allowing for a native development experience but with the benefits of reproducibility and isolation. This makes easier to onboard new joiners and align development environments between team members. Devcontainers are very well supported for <strong>VSCode</strong> but are now being standardized in an <a href=\"https://containers.dev/\">open specification</a>. Even though it will probably still take a while to see wide adoption, the specification is a good candidate for the standardization of Devcontainers.</p><h2 id=\"about\"><strong>About</strong></h2><p>This blogpost is written by <a href=\"https://www.github.com/dunnkers\">Jeroen Overschie</a>, working at <a href=\"https://xebia.com/\">Xebia</a>.</p>","comment_id":"6394c3a02bf4040001d22235","plaintext":"Imagine the following scenario üí≠.\n\nYour company uses Apache Spark to process data, and your team has pyspark set up in a Python project. The codebase is built on a specific Python version, using a certain Java installation, and an accompanying pyspark version that works with the former. To onboard a new member, you will need to pass a list of instructions the developer needs to follow carefully to get their setup working. But not everyone might run this on the same laptop environment: different hardware, and different operating systems. This is getting challenging.\nBut the setup is a one-off, right? Just go through the setup once and you‚Äôll be good. Not entirely. Your code environment will change over time: your team will probably install-, update- or remove packages during the project‚Äôs development. This means that if a developer creates a new feature and changes their own environment to do so; he or she also needs to make sure that the other team members change theirs and that the production environment is updated accordingly. This makes it easy to get misaligned environments: between developers, and between development & production.\n\nWe can do better than this! Instead of giving other developers a setup document, let‚Äôs make sure we also create formal instructions so we can automatically set up the development environment. Devcontainers let us do exactly this.\n\nDevcontainers let you connect your IDE to a running Docker container. In this way, we get the benefits of reproducibility and isolation, whilst getting a native development experience.\n\n\n\nDevcontainers can help us:\n\n * Get a reproducible development environment\n * ‚ö°Ô∏è Instantly onboard new team members onto your project\n * ‚Äç ‚Äç ‚Äç Better align the environments between team members\n * ‚è± Keeping your dev environment up-to-date & reproducible saves your team time going into production later\n\nLet‚Äôs explore how we can set up a Devcontainer for your Python project!\n\n\nCreating your first Devcontainer\n\nNote that this tutorial is focused on VSCode. Other IDE‚Äôs like PyCharm support running in Docker containers but support is less comprehensive than on VSCode.\n\n\nRecap\n\nTo recap, we are trying to create a dev environment that installs: 1) Java, 2) Python and 3) pyspark. And we want to do so automatically, that is, inside a Docker image.\n\n\nProject structure\n\nLet‚Äôs say we have a really simple project that looks like this:\n\n$ tree .\n.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ requirements-dev.txt\n‚îú‚îÄ‚îÄ sales_analysis.py\n‚îî‚îÄ‚îÄ test_sales_analysis.py\n\nThat is, we have a Python module with an accompanying test, a requirements.txt file describing our production dependencies (pyspark), and a requirements-dev.txt describing dependencies that should be installed in development only (pytest, black, mypy). Now let‚Äôs see how we can extend this setup to include a Devcontainer.\n\n\nThe .devcontainer folder\n\nYour Devcontainer spec will live inside the .devcontainer folder. There will be two main files:\n\n * devcontainer.json\n * Dockerfile\n\nCreate a new file called devcontainer.json:\n\n{\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\",\n        \"context\": \"..\"\n    }\n}\n\nThis means: as a base for our Devcontainer, use the Dockerfile located in the current directory, and build it with a current working directory (cwd) of ...\n\nSo what does this Dockerfile look like?\n\nFROM python:3.10\n\n# Install Java\nRUN apt update && \n    apt install -y sudo && \n    sudo apt install default-jdk -y\n\n## Pip dependencies\n# Upgrade pip\nRUN pip install --upgrade pip\n# Install production dependencies\nCOPY requirements.txt /tmp/requirements.txt\nRUN pip install -r /tmp/requirements.txt && \n    rm /tmp/requirements.txt\n# Install development dependencies\nCOPY requirements-dev.txt /tmp/requirements-dev.txt\nRUN pip install -r /tmp/requirements-dev.txt && \n    rm /tmp/requirements-dev.txt\n\nWe are building our image on top of python:3.10, which is a Debian-based image. This is one of the Linux distributions that a Devcontainer can be built on. The main requirement is that Node.js should be able to run: VSCode automatically installs VSCode Server on the machine. For an extensive list of supported distributions, see ‚ÄúRemote Development with Linux‚Äù.\n\nOn top of python:3.10, we install Java and the required pip packages.\n\n\nOpening the Devcontainer\n\nThe .devcontainer folder is in place, so it‚Äôs now time to open our Devcontainer.\n\nFirst, make sure you have the Dev Containers extension installed in VSCode (previously called ‚ÄúRemote ‚Äì Containers‚Äù. That done, if you open your repo again, the extension should already detect your Devcontainer:\n\nAlternatively, you can open up the command palette (CMD + Shift + P) and select ‚ÄúDev Containers: Reopen in Container‚Äù:\n\nYour VSCode is now connected to the Docker container :\n\n\nWhat is happening under the hood\n\nBesides starting the Docker image and attaching the terminal to it, VSCode is doing a couple more things:\n\n 1. VSCode Server is being installed on your Devcontainer. VSCode Server is installed as a service in the container itself so your VSCode installation can communicate with the container. For example, install and run extensions.\n 2. Config is copied over. Config like ~/.gitconfig and ~/.ssh/known_hosts are copied over to their respective locations in the container.\n    This then allows you to use your Git repo like you do normally, without re-authenticating.\n 3. Filesystem mounts. VSCode automatically takes care of mounting: 1) The folder you are running the Devcontainer from and 2) your VSCode workspace folder.\n\n\nOpening your repo directly in a Devcontainer\n\nSince all instructions on how to configure your dev environment are now defined in a Dockerfile, users can open up your Devcontainer with just one button:\n\nAin‚Äôt that cool? You can add a button to your repo like so:\n\n[\n    ![Open in Remote - Containers](\n        https://xebia.com/wp-content/uploads/2023/11/v1.svg    )\n](\n    https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/godatadriven/python-devcontainer-template\n)\n\nJust modify the GitHub URL ‚úì.\n\nThat said, we can see having built a Devcontainer can make our README massively more readable. What kind of README would you rather like?\n\nManual installationUsing a Devcontainer\n\n\nExtending the Devcontainer\n\nWe have built a working Devcontainer, which is great! But a couple of things are still missing. We still want to:\n\n * Install a non-root user for extra safety and good-practice\n * Pass in custom VSCode settings and install extensions by default\n * Be able to access Spark UI (port 4040)\n * Run Continuous Integration (CI) in the Devcontainer\n\nLet‚Äôs see how.\n\n\nInstalling a non-root user\n\nIf you pip install a new package, you will see the following message:\n\nIndeed, it is not recommended to develop as a root user. It is considered a good practice to create a different user with fewer rights to run in production. So let‚Äôs go ahead and create a user for this scenario.\n\n# Add non-root user\nARG USERNAME=nonroot\nRUN groupadd --gid 1000 $USERNAME && \n    useradd --uid 1000 --gid 1000 -m $USERNAME\n## Make sure to reflect new user in PATH\nENV PATH=\"/home/${USERNAME}/.local/bin:${PATH}\"\nUSER $USERNAME\n\nAdd the following property to devcontainer.json:\n\n    \"remoteUser\": \"nonroot\"\n\nThat‚Äôs great! When we now start the container we should connect as the user nonroot.\n\n\nPassing custom VSCode settings\n\nOur Devcontainer is still a bit bland, without extensions and settings. Besides any custom extensions a user might want to install, we can install some for them by default already. We can define such settings in customizations.vscode:\n\n     \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-python.python\"\n            ],\n            \"settings\": {\n                \"python.testing.pytestArgs\": [\n                    \".\"\n                ],\n                \"python.testing.unittestEnabled\": false,\n                \"python.testing.pytestEnabled\": true,\n                \"python.formatting.provider\": \"black\",\n                \"python.linting.mypyEnabled\": true,\n                \"python.linting.enabled\": true\n            }\n        }\n    }\n\nThe defined extensions are always installed in the Devcontainer. However, the defined settings provide just a default for the user to use, and can still be overridden by other setting scopes like User Settings, Remote Settings, or Workspace Settings.\n\n\nAccessing Spark UI\n\nSince we are using pyspark, we want to be able to access Spark UI. When we start a Spark session, VSCode will ask whether you want to forward the specific port. Since we already know this is Spark UI, we can do so automatically:\n\n    \"portsAttributes\": {\n        \"4040\": {\n            \"label\": \"SparkUI\",\n            \"onAutoForward\": \"notify\"\n        }\n    },\n\n    \"forwardPorts\": [\n        4040\n    ]\n\nWhen we now run our code, we get a notification we can open Spark UI in the browser:\n\nResulting in the Spark UI as we know it:\n\n‚ú®\n\n\nRunning our CI in the Devcontainer\n\nWouldn‚Äôt it be convenient if we could re-use our Devcontainer to run our Continuous Integration (CI) pipeline as well? Indeed, we can do this with Devcontainers. Similarly to how the Devcontainer image is built locally using docker build, the same can be done within a CI/CD pipeline. There are two basic options:\n\n 1. Build the Docker image within the CI/CD pipeline\n 2. Prebuilding the image\n\nTo pre-build the image, the build step will need to run either periodically or whenever the Docker definition has changed. Since this adds quite some complexity let‚Äôs dive into building the Devcontainer as part of the CI/CD pipeline first (for pre-building the image, see the ‚ÄòAwesome resources‚Äô section). We will do so using GitHub Actions.\n\nUsing devcontainers/ci\n\nLuckily, a GitHub Action was already set up for us to do exactly this:\n\nhttps://github.com/devcontainers/ci\n\nTo now build, push and run a command in the Devcontainer is as easy as:\n\nname: Python app\n\non:\n  pull_request:\n  push:\n    branches:\n      - \"**\"\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout (GitHub)\n        uses: actions/checkout@v3\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v2\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build and run dev container task\n        uses: devcontainers/ci@v0.2\n        with:\n          imageName: ghcr.io/${{ github.repository }}/devcontainer\n          runCmd: pytest .\n\nThat‚Äôs great! Whenever this workflow runs on your main branch, the image will be pushed to the configured registry; in this case GitHub Container Registry (GHCR). See below a trace of the executed GitHub Action:\n\nAwesome!\n\n\nThe final Devcontainer definition\n\nWe built the following Devcontainer definitions. First, devcontainer.json:\n\n{\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\",\n        \"context\": \"..\"\n    },\n\n    \"remoteUser\": \"nonroot\",\n\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-python.python\"\n            ],\n            \"settings\": {\n                \"python.testing.pytestArgs\": [\n                    \".\"\n                ],\n                \"python.testing.unittestEnabled\": false,\n                \"python.testing.pytestEnabled\": true,\n                \"python.formatting.provider\": \"black\",\n                \"python.linting.mypyEnabled\": true,\n                \"python.linting.enabled\": true\n            }\n        }\n    },\n\n    \"portsAttributes\": {\n        \"4040\": {\n            \"label\": \"SparkUI\",\n            \"onAutoForward\": \"notify\"\n        }\n    },\n\n    \"forwardPorts\": [\n        4040\n    ]\n}\n\nAnd our Dockerfile:\n\nFROM python:3.10\n\n# Install Java\nRUN apt update && \n    apt install -y sudo && \n    sudo apt install default-jdk -y\n\n# Add non-root user\nARG USERNAME=nonroot\nRUN groupadd --gid 1000 $USERNAME && \n    useradd --uid 1000 --gid 1000 -m $USERNAME\n## Make sure to reflect new user in PATH\nENV PATH=\"/home/${USERNAME}/.local/bin:${PATH}\"\nUSER $USERNAME\n\n## Pip dependencies\n# Upgrade pip\nRUN pip install --upgrade pip\n# Install production dependencies\nCOPY --chown=nonroot:1000 requirements.txt /tmp/requirements.txt\nRUN pip install -r /tmp/requirements.txt && \n    rm /tmp/requirements.txt\n# Install development dependencies\nCOPY --chown=nonroot:1000 requirements-dev.txt /tmp/requirements-dev.txt\nRUN pip install -r /tmp/requirements-dev.txt && \n    rm /tmp/requirements-dev.txt\n\nThe full Devcontainer implementation and all the above steps can be found in the various branches of the godatadriven/python-devcontainer-template repo.\n\n\nDocker images architecture: Three environments\n\nWith the CI now set up, we can reuse the same Docker image for two purposes. For local development and running our quality checks. And, once we deploy this application to production, we could configure the Devcontainer to use our production image as a base, and install extra dependencies on top. If we want to optimize the CI image to be as lightweight as possible, we could also strip off any extra dependencies that we do not require in the CI environment; things as extra CLI tooling, a better shell such as ZSH, and so forth.\n\nThis sets us up for having 3 different images for our entire lifecycle. One for Development, one for CI, and finally one for production. This can be visualized like so:\n\nSo, we can see, when using a Devcontainer you can re-use your production image and build on top of it. Install extra tooling, make sure it can talk to VSCode, and you‚Äôre done .\n\n\nGoing further\n\nThere are lots of other resources to explore; Devcontainers are well-documented and there are many posts about it. If you‚Äôre up for more, let‚Äôs see what else you can do.\n\n\nDevcontainer features\n\nDevcontainer features allow you to easily extend your Docker definition with common additions. Some useful features are:\n\n * Common Debian Utilities (Installs ZSH using Oh My ZSH, a non-root user, and useful CLI tools like curl)\n * AWS CLI\n * Azure CLI\n * Git\n * Node.js\n * Python\n * Java\n\n\nDevcontainer templates\n\nOn the official Devcontainer specification website there are loads of templates available. Good chance (part of) your setup is in there. A nice way to get a head-start in building your Devcontainer or to get started quickly.\n\nSee: https://containers.dev/templates\n\n\nMounting directories\n\nRe-authenticating your CLI tools is annoying. So one trick is to mount your AWS/Azure/GCP credentials from your local computer into your Devcontainer. This way, authentications done in either environment are shared with the other. You can easily do this by adding this to devcontainer.json:\n\n  \"mounts\": [\n    \"source=/Users/<your_username>/.aws,target=/home/nonroot/.aws,type=bind,consistency=cached\"\n  ]\n\n^ the above example mounts your AWS credentials, but the process should be similar for other cloud providers (GCP / Azure).\n\n\nAwesome resources\n\n * devcontainers/ci. Run your CI in your Devcontainers. Built on the Devcontainer CLI.\n * https://containers.dev/. The official Devcontainer specification.\n * devcontainers/images. A collection of ready-to-use Devcontainer images.\n * Add a non-root user to a container. More explanations & instructions for adding a non-root user to your Dockerfile and devcontainer.json.\n * Pre-building dev container images\n * awesome-devcontainers. A repo pointing to yet even more awesome resources.\n\n\nConcluding\n\nDevcontainers allow you to connect your IDE to a running Docker container, allowing for a native development experience but with the benefits of reproducibility and isolation. This makes easier to onboard new joiners and align development environments between team members. Devcontainers are very well supported for VSCode but are now being standardized in an open specification. Even though it will probably still take a while to see wide adoption, the specification is a good candidate for the standardization of Devcontainers.\n\n\nAbout\n\nThis blogpost is written by Jeroen Overschie, working at Xebia.","feature_image":"__GHOST_URL__/content/images/2022/12/without-ship-taller-img.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-12-10 17:36:32","created_by":"1","updated_at":"2025-04-09 13:50:21","updated_by":"1","published_at":"2022-11-21 17:36:00","published_by":"1","custom_excerpt":"Instead of giving other developers a setup document, let‚Äôs make sure we also create formal instructions so we can automatically set up the development environment. Devcontainers let us do exactly this.","codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; URL=https://xebia.com/blog/how-to-create-a-devcontainer-for-your-python-project/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Imagine the following scenario üí≠.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your company uses Apache Spark to process data, and your team has pyspark set up in a Python project. The codebase is built on a specific Python version, using a certain Java installation, and an accompanying pyspark version that works with the former. To onboard a new member, you will need to pass a list of instructions the developer needs to follow carefully to get their setup working. But not everyone might run this on the same laptop environment: different hardware, and different operating systems. This is getting challenging.\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"But the setup is a one-off, right? Just go through the setup once and you‚Äôll be good. Not entirely. Your code environment will change over time: your team will probably install-, update- or remove packages during the project‚Äôs development. This means that if a developer creates a new feature and changes their own environment to do so; he or she also needs to make sure that the other team members change theirs and that the production environment is updated accordingly. This makes it easy to get misaligned environments: between developers, and between development & production.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can do better than this! Instead of giving other developers a setup document, let‚Äôs make sure we also create formal instructions so we can \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"automatically\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" set up the development environment. \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainers\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" let us do exactly this.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainers\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" let you connect your IDE to a running Docker container. In this way, we get the benefits of reproducibility and isolation, whilst getting a native development experience.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/devcontainer-overview-4.png.webp\",\"alt\":\"Devcontainer usage overview\",\"caption\":\"With Devcontainers you can interact with your IDE like you're used to whilst under the hood running everything inside a Docker container.\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainers can help us:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Get a reproducible development environment\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚ö°Ô∏è Instantly onboard new team members onto your project\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äç ‚Äç ‚Äç Better align the environments between team members\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚è± Keeping your dev environment up-to-date & reproducible saves your team time going into production later\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs explore how we can set up a Devcontainer for your Python project!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Creating your first Devcontainer\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Note that this tutorial is focused on \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"VSCode\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Other IDE‚Äôs like PyCharm support running in Docker containers but support is less comprehensive than on VSCode.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Recap\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"To recap, we are trying to create a dev environment that installs: 1) Java, 2) Python and 3) pyspark. And we want to do so \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"automatically\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", that is, inside a Docker image.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Project structure\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs say we have a really simple project that looks like this:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"$ tree .\\n.\\n‚îú‚îÄ‚îÄ README.md\\n‚îú‚îÄ‚îÄ requirements.txt\\n‚îú‚îÄ‚îÄ requirements-dev.txt\\n‚îú‚îÄ‚îÄ sales_analysis.py\\n‚îî‚îÄ‚îÄ test_sales_analysis.py\",\"language\":\"bash\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is, we have a Python module with an accompanying test, a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"requirements.txt\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" file describing our production dependencies (pyspark), and a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"requirements-dev.txt\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" describing dependencies that should be installed in development only (pytest, black, mypy). Now let‚Äôs see how we can extend this setup to include a Devcontainer.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":17,\"mode\":\"normal\",\"style\":\"\",\"text\":\".devcontainer\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\" folder\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your Devcontainer spec will live inside the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\".devcontainer\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" folder. There will be two main files:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainer.json\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dockerfile\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Create a new file called \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainer.json\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"{\\n    \\\"build\\\": {\\n        \\\"dockerfile\\\": \\\"Dockerfile\\\",\\n        \\\"context\\\": \\\"..\\\"\\n    }\\n}\",\"language\":\"json\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This means: as a base for our Devcontainer, use the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dockerfile\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" located in the current directory, and build it with a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"current working directory\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (cwd) of \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"..\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So what does this \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dockerfile\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" look like?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"FROM python:3.10\\n\\n# Install Java\\nRUN apt update && \\n    apt install -y sudo && \\n    sudo apt install default-jdk -y\\n\\n## Pip dependencies\\n# Upgrade pip\\nRUN pip install --upgrade pip\\n# Install production dependencies\\nCOPY requirements.txt /tmp/requirements.txt\\nRUN pip install -r /tmp/requirements.txt && \\n    rm /tmp/requirements.txt\\n# Install development dependencies\\nCOPY requirements-dev.txt /tmp/requirements-dev.txt\\nRUN pip install -r /tmp/requirements-dev.txt && \\n    rm /tmp/requirements-dev.txt\",\"language\":\"docker\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We are building our image on top of \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"python:3.10\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", which is a Debian-based image. This is one of the Linux distributions that a Devcontainer can be built on. The main requirement is that \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Node.js\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" should be able to run: VSCode automatically installs VSCode Server on the machine. For an extensive list of supported distributions, see \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚ÄúRemote Development with Linux‚Äù\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://code.visualstudio.com/docs/remote/linux\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"On top of \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"python:3.10\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", we install Java and the required pip packages.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Opening the Devcontainer\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\".devcontainer\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" folder is in place, so it‚Äôs now time to open our Devcontainer.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"First, make sure you have the \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dev Containers extension\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" installed in VSCode (previously called ‚ÄúRemote ‚Äì Containers‚Äù. That done, if you open your repo again, the extension should already detect your Devcontainer:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/folder-contains-a-dev-container-config-file.png.webp\",\"alt\":\"folder contains a dev container config file\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Alternatively, you can open up the command palette (CMD + Shift + P) and select ‚Äú\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dev Containers: Reopen in Container\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äù:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/reopen-in-devcontainer-notification.png.webp\",\"alt\":\"Dev Containers: Reopen in Container\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your VSCode is now connected to the Docker container :\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/opening-the-devcontainer.gif.webp\",\"alt\":\"VSCode is now connected to the Docker container\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What is happening under the hood\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Besides starting the Docker image and attaching the terminal to it, VSCode is doing a couple more things:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"VSCode Server\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://code.visualstudio.com/docs/remote/vscode-server\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" is being installed on your Devcontainer. VSCode Server is installed as a service in the container itself so your VSCode installation can communicate with the container. For example, install and run extensions.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Config is copied\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" over. Config like \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"~/.gitconfig\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"~/.ssh/known_hosts\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" are copied over to their respective locations in the container.\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This then allows you to use your Git repo like you do normally, without re-authenticating.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Filesystem mounts\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". VSCode automatically takes care of mounting: 1) The folder you are running the Devcontainer from and 2) your VSCode workspace folder.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Opening your repo directly in a Devcontainer\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Since all instructions on how to configure your dev environment are now defined in a Dockerfile, users can open up your Devcontainer with just one button:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/11/v1.svg\",\"alt\":\"Open in Remote - Containers\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ain‚Äôt that cool? You can add a button to your repo like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"[\\n    ![Open in Remote - Containers](\\n        https://xebia.com/wp-content/uploads/2023/11/v1.svg    )\\n](\\n    https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/godatadriven/python-devcontainer-template\\n)\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Just modify the GitHub URL ‚úì.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That said, we can see having built a Devcontainer can make our README massively more readable. What kind of README would you rather like?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"html\":\"<table style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse; color: rgb(53, 68, 90); font-family: Proxima, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\\\"><thead style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><th style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\\\">Manual installation</th><th style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\\\">Using a Devcontainer</th></tr></thead><tbody style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\\\"><img decoding=\\\"async\\\" src=\\\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-manual.png.webp\\\" alt=\\\"\\\" data-src-img=\\\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-manual.png\\\" data-src-webp=\\\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-manual.png.webp\\\" data-eio=\\\"j\\\" class=\\\"ewww_webp_loaded\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"></td><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center;\\\"><img decoding=\\\"async\\\" src=\\\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-devcontainer.png.webp\\\" alt=\\\"\\\" data-src-img=\\\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-devcontainer.png\\\" data-src-webp=\\\"https://xebia.com/wp-content/uploads/2023/02/installation-instructions-devcontainer.png.webp\\\" data-eio=\\\"j\\\" class=\\\"ewww_webp_loaded\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"></td></tr></tbody></table>\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Extending the Devcontainer\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We have built a working Devcontainer, which is great! But a couple of things are still missing. We still want to:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Install a non-root user for extra safety and good-practice\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Pass in custom VSCode settings and install extensions by default\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Be able to access Spark UI (port 4040)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Run Continuous Integration (CI) in the Devcontainer\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs see how.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Installing a non-root user\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pip install\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" a new package, you will see the following message:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/running-pip-as-root.png.webp\",\"alt\":\"The warning message: ‚Äú*WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: [https://pip.pypa.io/warnings/venv](https://pip.pypa.io/warnings/venv)*\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Indeed, it is not recommended to develop as a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"root\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" user. It is considered a good practice to create a different user with fewer rights to run in production. So let‚Äôs go ahead and create a user for this scenario.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"# Add non-root user\\nARG USERNAME=nonroot\\nRUN groupadd --gid 1000 $USERNAME && \\n    useradd --uid 1000 --gid 1000 -m $USERNAME\\n## Make sure to reflect new user in PATH\\nENV PATH=\\\"/home/${USERNAME}/.local/bin:${PATH}\\\"\\nUSER $USERNAME\",\"language\":\"bash\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Add the following property to \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainer.json\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"    \\\"remoteUser\\\": \\\"nonroot\\\"\",\"language\":\"json\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That‚Äôs great! When we now start the container we should connect as the user \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"nonroot\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Passing custom VSCode settings\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Our Devcontainer is still a bit bland, without extensions and settings. Besides any custom extensions a user might want to install, we can install some for them by default already. We can define such settings in \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"customizations.vscode\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"     \\\"customizations\\\": {\\n        \\\"vscode\\\": {\\n            \\\"extensions\\\": [\\n                \\\"ms-python.python\\\"\\n            ],\\n            \\\"settings\\\": {\\n                \\\"python.testing.pytestArgs\\\": [\\n                    \\\".\\\"\\n                ],\\n                \\\"python.testing.unittestEnabled\\\": false,\\n                \\\"python.testing.pytestEnabled\\\": true,\\n                \\\"python.formatting.provider\\\": \\\"black\\\",\\n                \\\"python.linting.mypyEnabled\\\": true,\\n                \\\"python.linting.enabled\\\": true\\n            }\\n        }\\n    }\",\"language\":\"json\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The defined extensions are always installed in the Devcontainer. However, the defined settings provide just a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"default\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" for the user to use, and can still be overridden by other setting scopes like User Settings, Remote Settings, or Workspace Settings.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Accessing Spark UI\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Since we are using pyspark, we want to be able to access \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Spark UI\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". When we start a Spark session, VSCode will ask whether you want to forward the specific port. Since we already know this is Spark UI, we can do so automatically:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"    \\\"portsAttributes\\\": {\\n        \\\"4040\\\": {\\n            \\\"label\\\": \\\"SparkUI\\\",\\n            \\\"onAutoForward\\\": \\\"notify\\\"\\n        }\\n    },\\n\\n    \\\"forwardPorts\\\": [\\n        4040\\n    ]\",\"language\":\"json\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"When we now run our code, we get a notification we can open Spark UI in the browser:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/application-running-on-port-4040.png.webp\",\"alt\":\"open Spark UI in the browser\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Resulting in the Spark UI as we know it:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/spark-ui-visible-in-localhost-4040.png.webp\",\"alt\":\"spark UI in the browser\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚ú®\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Running our CI in the Devcontainer\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Wouldn‚Äôt it be convenient if we could re-use our Devcontainer to run our Continuous Integration (CI) pipeline as well? Indeed, we can do this with Devcontainers. Similarly to how the Devcontainer image is built locally using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"docker build\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", the same can be done \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"within\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" a CI/CD pipeline. There are two basic options:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Build the Docker image \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"within\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" the CI/CD pipeline\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Prebuilding the image\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"To pre-build the image, the build step will need to run either periodically or whenever the Docker definition has changed. Since this adds quite some complexity let‚Äôs dive into building the Devcontainer as part of the CI/CD pipeline first (for pre-building the image, see the ‚ÄòAwesome resources‚Äô section). We will do so using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GitHub Actions\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":17,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainers/ci\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h4\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Luckily, a GitHub Action was already set up for us to do exactly this:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://github.com/devcontainers/ci\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/ci\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"To now build, push and run a command in the Devcontainer is as easy as:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"name: Python app\\n\\non:\\n  pull_request:\\n  push:\\n    branches:\\n      - \\\"**\\\"\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n\\n    steps:\\n      - name: Checkout (GitHub)\\n        uses: actions/checkout@v3\\n\\n      - name: Login to GitHub Container Registry\\n        uses: docker/login-action@v2\\n        with:\\n          registry: ghcr.io\\n          username: ${{ github.repository_owner }}\\n          password: ${{ secrets.GITHUB_TOKEN }}\\n\\n      - name: Build and run dev container task\\n        uses: devcontainers/ci@v0.2\\n        with:\\n          imageName: ghcr.io/${{ github.repository }}/devcontainer\\n          runCmd: pytest .\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That‚Äôs great! Whenever this workflow runs on your main branch, the image will be pushed to the configured registry; in this case GitHub Container Registry (GHCR). See below a trace of the executed GitHub Action:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/running-ci-in-the-devcontainer-github-actions.png.webp\",\"alt\":\"running-ci-in-the-devcontainer-github-actions\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Awesome!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The final Devcontainer definition\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We built the following Devcontainer definitions. First, \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainer.json\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"{\\n    \\\"build\\\": {\\n        \\\"dockerfile\\\": \\\"Dockerfile\\\",\\n        \\\"context\\\": \\\"..\\\"\\n    },\\n\\n    \\\"remoteUser\\\": \\\"nonroot\\\",\\n\\n    \\\"customizations\\\": {\\n        \\\"vscode\\\": {\\n            \\\"extensions\\\": [\\n                \\\"ms-python.python\\\"\\n            ],\\n            \\\"settings\\\": {\\n                \\\"python.testing.pytestArgs\\\": [\\n                    \\\".\\\"\\n                ],\\n                \\\"python.testing.unittestEnabled\\\": false,\\n                \\\"python.testing.pytestEnabled\\\": true,\\n                \\\"python.formatting.provider\\\": \\\"black\\\",\\n                \\\"python.linting.mypyEnabled\\\": true,\\n                \\\"python.linting.enabled\\\": true\\n            }\\n        }\\n    },\\n\\n    \\\"portsAttributes\\\": {\\n        \\\"4040\\\": {\\n            \\\"label\\\": \\\"SparkUI\\\",\\n            \\\"onAutoForward\\\": \\\"notify\\\"\\n        }\\n    },\\n\\n    \\\"forwardPorts\\\": [\\n        4040\\n    ]\\n}\",\"language\":\"json\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"And our \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dockerfile\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"FROM python:3.10\\n\\n# Install Java\\nRUN apt update && \\n    apt install -y sudo && \\n    sudo apt install default-jdk -y\\n\\n# Add non-root user\\nARG USERNAME=nonroot\\nRUN groupadd --gid 1000 $USERNAME && \\n    useradd --uid 1000 --gid 1000 -m $USERNAME\\n## Make sure to reflect new user in PATH\\nENV PATH=\\\"/home/${USERNAME}/.local/bin:${PATH}\\\"\\nUSER $USERNAME\\n\\n## Pip dependencies\\n# Upgrade pip\\nRUN pip install --upgrade pip\\n# Install production dependencies\\nCOPY --chown=nonroot:1000 requirements.txt /tmp/requirements.txt\\nRUN pip install -r /tmp/requirements.txt && \\n    rm /tmp/requirements.txt\\n# Install development dependencies\\nCOPY --chown=nonroot:1000 requirements-dev.txt /tmp/requirements-dev.txt\\nRUN pip install -r /tmp/requirements-dev.txt && \\n    rm /tmp/requirements-dev.txt\",\"language\":\"docker\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The full Devcontainer implementation and all the above steps can be found in the various branches of the \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"godatadriven/python-devcontainer-template\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/godatadriven/python-devcontainer-template\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" repo.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"quote\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Docker images architecture: Three environments\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With the CI now set up, we can reuse the same Docker image for two purposes. For local development and running our quality checks. And, once we deploy this application to production, we could configure the Devcontainer to use our production image as a base, and install extra dependencies on top. If we want to optimize the CI image to be as lightweight as possible, we could also strip off any extra dependencies that we do not require in the CI environment; things as extra CLI tooling, a better shell such as ZSH, and so forth.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This sets us up for having 3 different images for our entire lifecycle. One for Development, one for CI, and finally one for production. This can be visualized like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/three-environments.png.webp\",\"alt\":\"three-environments-docker-images-devcontainer\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So, we can see, when using a Devcontainer you can re-use your production image and build on top of it. Install extra tooling, make sure it can talk to VSCode, and you‚Äôre done .\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Going further\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"There are lots of other resources to explore; Devcontainers are well-documented and there are many posts about it. If you‚Äôre up for more, let‚Äôs see what else you can do.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainer features\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainer \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"features\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://containers.dev/features\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" allow you to easily extend your Docker definition with common additions. Some useful features are:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Common Debian Utilities\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/common-utils\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (Installs ZSH using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Oh My ZSH\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", a non-root user, and useful CLI tools like \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"curl\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\")\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"AWS CLI\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/aws-cli\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Azure CLI\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/azure-cli\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Git\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/git\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Node.js\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/node\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":5,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Python\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/python\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":6,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Java\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/features/tree/main/src/java\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":7,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainer templates\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"On the official Devcontainer specification website there are \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"loads\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" of templates available. Good chance (part of) your setup is in there. A nice way to get a head-start in building your Devcontainer or to get started quickly.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"See: \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://containers.dev/templates\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://containers.dev/templates\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Mounting directories\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Re-authenticating your CLI tools is annoying. So one trick is to mount your AWS/Azure/GCP credentials from your local computer into your Devcontainer. This way, authentications done in either environment are shared with the other. You can easily do this by adding this to \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainer.json\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"  \\\"mounts\\\": [\\n    \\\"source=/Users/<your_username>/.aws,target=/home/nonroot/.aws,type=bind,consistency=cached\\\"\\n  ]\",\"language\":\"json\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"^ the above example mounts your AWS credentials, but the process should be similar for other cloud providers (GCP / Azure).\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Awesome resources\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainers/ci\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/ci\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Run your CI in your Devcontainers. Built on the \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainer CLI\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/cli\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://containers.dev/\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://containers.dev/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". The official Devcontainer specification.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainers/images\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/devcontainers/images\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". A collection of ready-to-use Devcontainer images.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Add a non-root user to a container\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://code.visualstudio.com/remote/advancedcontainers/add-nonroot-user\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". More explanations & instructions for adding a non-root user to your \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Dockerfile\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"devcontainer.json\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Pre-building dev container images\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://code.visualstudio.com/docs/remote/containers#_prebuilding-dev-container-images\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":5,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"awesome-devcontainers\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/manekinekko/awesome-devcontainers\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". A repo pointing to yet even more awesome resources.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":6,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Concluding\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Devcontainers allow you to connect your IDE to a running Docker container, allowing for a native development experience but with the benefits of reproducibility and isolation. This makes easier to onboard new joiners and align development environments between team members. Devcontainers are very well supported for \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"VSCode\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" but are now being standardized in an \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"open specification\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://containers.dev/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Even though it will probably still take a while to see wide adoption, the specification is a good candidate for the standardization of Devcontainers.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"About\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This blogpost is written by \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Jeroen Overschie\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.github.com/dunnkers\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", working at \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Xebia\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"type\":\"linebreak\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"65993233cdf9710001822253","uuid":"5be1defe-c404-4d3c-b333-9312673ac5ff","title":"DropBlox: Coding Challenge at PyCon DE & PyData Berlin 2022 (on Xebia.com ‚ßâ)","slug":"dropblox-coding-challenge-at-pycon-de-pydata-berlin-2022","mobiledoc":null,"html":"<p>Conferences are great. You meet new people, you learn new things. But have you ever found yourself back in the hotel after a day at a conference, thinking what to do now? Or were you ever stuck in one session, wishing you had gone for that other one? These moments are the perfect opportunity to open up your laptop and compete with your peers in a coding challenge.</p><p>Attendees of the three-day conference PyCon DE &amp; PyData Berlin 2022 had the possibility to do so, with our coding challenge DropBlox.</p><p>Participants had a bit over one day to submit their solutions. After the deadline, we had received over 100 submissions and rewarded the well-deserved winner a Lego R2D2 in front of a great crowd.</p><p>Read on to learn more about this challenge. We will discuss the following:</p><ul><li>What was the challenge exactly, and what trade-offs were made in the design?</li><li>What was happening behind the screens to make this challenge possible?</li><li>How did we create hype at the conference itself?</li><li>What strategies were adopted by the participants to crack the problem?</li></ul><p>Participants used a public repository that we made available <a href=\"https://github.com/godatadriven/dropblox\">here</a>.</p><h3 id=\"challenge\"><strong>Challenge</strong></h3><p>Participants of the challenge were given the following:</p><ul><li>A 100 x 100 field</li><li>1500 blocks of various colors and shapes, each with a unique identifier (see Fig. 1)</li><li>A rewards table, specifying the points and multipliers per color (see Table 1)</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/type-of-blocks.png.webp\" class=\"kg-image\" alt=\"DropBlox: Type of Blocks\" loading=\"lazy\" width=\"500\" height=\"137\"><figcaption>Figure 1: A random subset of blocks from the challenge and their corresponding IDs.</figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/point-scheme.png.webp\" class=\"kg-image\" alt=\"Score scheme\" loading=\"lazy\" width=\"300\" height=\"349\"><figcaption>Table 1: The rewards table, specifying how each color contributes to the score of a solution of the challenge. We assign points to each tile in the final solution, while the multiplier only applies to rows filled with tiles of the same color.</figcaption></figure><p>The rules are as follows:</p><ul><li>Blocks can be dropped in the field from the top at a specified x-coordinate, without changing their rotation (see Fig. 2)</li><li>Each block can be used at most once</li><li>The score of a solution is computed using the rewards table. For each row, we add the points of each tile to the score. If the row consists of tiles of a single color only, we multiply the points of that row by the corresponding multiplier. The final score is the sum of the scores of the rows. (see Fig. 3)</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/situation-one.png.webp\" class=\"kg-image\" alt=\"DropBlox: Example 1\" loading=\"lazy\" width=\"500\" height=\"237\"><figcaption>Figure 2: An example 6√ó6 field and a blue block dropped at x-coordinate 1.</figcaption></figure><p></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/situation-2.png.webp\" class=\"kg-image\" alt=\"DropBlox: Example 2\" loading=\"lazy\" width=\"500\" height=\"215\"><figcaption>Figure 3: An example of the computation of the score of a solution. The points and multipliers per color are specified in Table 1.</figcaption></figure><p>The solution is a list of block IDs with corresponding x-coordinates. This list specifies which blocks to drop and where, in order to come to the final solution.<br>The goal of the challenge? Getting the most points possible.</p><h3 id=\"the-design\"><strong>The design</strong></h3><p>When designing the challenge, we came up with a few simple requirements to follow:</p><ul><li>The challenge should be easy to get started with</li><li>The progress and final solution should be easy to visualize</li><li>It should be difficult to approach the optimum solution</li></ul><p>Ideas about N-dimensional versions of this challenge came along, but only the ‚Äòsimple‚Äô 2D design ticked all the boxes. It‚Äôs easy to visualize, because it‚Äôs 2D, and (therefore) easy to get started with. Still, a 100 x 100 field with 1500 blocks allows for enough freedom to play this game in more ways than there are atoms in the universe!</p><h3 id=\"behind-the-screens\"><strong>Behind the screens</strong></h3><p>Participants could, and anyone still can, submit their solutions on the <a href=\"https://dropblox.azurewebsites.net/submit\" rel=\"noopener\">submission page</a>, as well as see the leaderboard with submissions of all other participants. To make this possible, several things happened behind the screens, which are worth noting here.</p><p>Most importantly, we worked with a team of excited people with complementing skill sets. Together we created the framework that is visualized in Fig. 4.</p><p>We have a separate private repository, in which we keep all the logic that is hidden to the participant. In there, we have the ground-truth scoring function, and all logic necessary to run our web app. When participants submit their solution or check out the leaderboard, an Azure Function spins up to run the logic of our web app. The Azure Function is connected to an SQL database, where we store or retrieve submissions from. ¬†We store images, such as the visualization of the final solution, in the blob storage. To create the leaderboard, we retrieve the top-scoring submissions of each user and combine them with the corresponding images in the blob storage.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/components.png.webp\" class=\"kg-image\" alt=\"Components of DropBlox\" loading=\"lazy\" width=\"600\" height=\"237\"><figcaption>Figure 4: The different components of the challenge, including those hidden to the participants.</figcaption></figure><h3 id=\"creating-hype\"><strong>Creating hype</strong></h3><p>What‚Äôs the use of a competition if nobody knows about it? Spread the word!</p><p>To attract attention to our coding competition, we did two things. First, we set up an appealing booth at the company stands. We put our prize right in front, and a real-time dashboard showing the highscores beside. Surely, anyone walking past will at least question themselves what that Lego toy is doing at a Python conference.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/booth_gdd.png.webp\" class=\"kg-image\" alt=\"booth_gdd\" loading=\"lazy\" width=\"500\" height=\"331\"><figcaption>Figure 5: Our company booth at PyCon DE &amp; PyData Berlin 2022</figcaption></figure><p>Second, we went out to the conference Lightning Talks and announced our competition there. Really, the audience was great. Gotta love the energy at conferences like these.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/Screenshot-2022-07-27-at-14.56.10.png.webp\" class=\"kg-image\" alt=\"DropBlox Explained\" loading=\"lazy\" width=\"500\" height=\"284\"><figcaption>Figure 6: Promoting the challenge at a Lightning Talk</figcaption></figure><p>With our promotion set up, competitors started trickling in. Let the games begin!</p><h3 id=\"strategies\"><strong>Strategies</strong></h3><p>Strategies varied from near brute-force approaches to the use of convolutional kernels and clever heuristics. In the following, we discuss some interesting and top-scoring submissions of participants.</p><h4 id=\"14\"><strong>#14</strong></h4><p>S. Tsch√∂ke named his first approach ‚ÄúBreakfast cereal‚Äù, as he was inspired by smaller cereal pieces collecting at the bottom and larger ones in the top of a cereal bag. Pieces were dropped from left to right, smaller ones before larger ones, until none could fit anymore. This approach, resulting in around 25k points, was however not successful enough.</p><p>After a less successful, but brave approach using a genetic algorithm, he extended the breakfast cereal approach. This time, instead of the size of a block he used the block‚Äôs density, or the percentage of filled tiles within the height and width of a block; and he sorted the blocks by color. Taking a similar approach as before, but now including the different color orderings, resulted in 46k points. (See Fig. 6)</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/number-14.png\" class=\"kg-image\" alt=\"DropBlox solution of the number 12\" loading=\"lazy\" width=\"450\" height=\"450\"><figcaption>Figure 6: Final solution of the #14 in the challenge, S. Tsch√∂ke, with 45844 points.</figcaption></figure><h4 id=\"2\"><strong>#2</strong></h4><p>We jump up a few places to R. Garnier, who was #1 up until the last moments of the challenge. He went along with a small group of dedicated participants who started exploiting the row multipliers. Unexpectedly, this led to an interesting and exciting development in the competition.</p><p>His strategy consisted of two parts. The first is to manually construct some rows of the same color of the same height. This way, he created 3 full orange rows, one red and one purple. Subsequently, he uses a greedy algorithm, following the steps:</p><ol><li>Assign a score to each block: score = (block values) / (block surface)</li><li>Sort the blocks by score</li><li>For each block, drop the block where it falls down the furthest‚Äã‚Äã</li></ol><p>This strategy resulted in 62k points. (See Fig. 7)</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/number-2.png\" class=\"kg-image\" alt=\"DropBlox solution of the number 2\" loading=\"lazy\" width=\"450\" height=\"450\"><figcaption>Figure 7: Final solution of the #2 in the challenge, R. Garnier, with 62032 points.</figcaption></figure><h3 id=\"1\"><strong>#1</strong></h3><p>With a single last-minute submission, G. Chanturia answered the question to ‚ÄúHow far can we go?‚Äù. He carefully constructed pairs of blocks that fit together, to manually engineer the bottom half of his solution, taking the row multipliers to the next level.</p><p>G. is doing a PhD in Physics and a MSc in Computer Science, and fittingly splits his solution into a ‚Äúphysicist‚Äôs solution‚Äù and a ‚Äúprogrammer‚Äôs solution‚Äù.</p><p>The physicist‚Äôs solution refers to the bottom part of the field. The strategy used here, as summarized by G., was (1) taking a look at the blocks, and (2) placing them in a smart way. Whether you are a theoretical or experimental physicist, data serves as a starting point. G. noticed there is an order in the blocks. First of all, a lot of orange blocks had their top and bottom rows filled. Placing these in a clever way already results in six completely filled rows.<br>Second, there were ‚ÄúW‚Äù and ‚ÄúM‚Äù-shaped blocks that fit perfectly together. He kept going on like this to manually construct the bottom 1/3 of the field, accounting for 57% of the total score.</p><p>The programmer‚Äôs solution refers to the rest of the field. The problem with the first approach is that it is not scalable. Even more, if the blocks would change he would have to start all over. This second approach is more robust, and is similar to R. Garnier‚Äôs approach. The steps are:</p><ol><li>Filter blocks based on their height. Blocks above height = 5 are filtered out, because many of these blocks have too weird shapes to work with.</li><li>Sort the blocks by points (or similarly, by color). Blocks with higher scoring colors are listed first.</li><li>Pick the first n available blocks in the sorted list. The larger n, the better the solution, but the longer it takes to run. The chosen number was around 50.</li><li>Find out which block can fall the furthest down in the field</li><li>Drop that block and remove it from the list</li><li>Repeat from 3</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/number-one.png\" class=\"kg-image\" alt=\"DropBlox solution of the number 1\" loading=\"lazy\" width=\"450\" height=\"450\"><figcaption>Figure 8: Final solution of the #1 in the challenge, G. Chanturia, with 68779 points.</figcaption></figure><p>And most importantly, the #1 place does not go home empty-handed. The #1 contender wins the infamous Lego R2D2! And we can acknowledge that indeed, yes, the R2D2 turned quite some heads at the conference. The award was given to the winner at the last Lightning Talk series of the conference.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2023/02/award-handover.jpg.webp\" class=\"kg-image\" alt=\"award handover\" loading=\"lazy\" width=\"501\" height=\"278\"><figcaption>Figure 9: Our winner receives his coveted prize!</figcaption></figure><h3 id=\"conclusion\"><strong>Conclusion</strong></h3><p>Organising the coding challenge has been lots of fun. We created a framework to host a high score leaderboard, process submissions and display the puzzles online. To sum up the process in a few words:</p><ul><li>Hosting a coding challenge at a conference is fun!</li><li>Gather participants by promoting the challenge</li><li>Hand over the prize to the winner</li></ul><p>It was interesting to see what strategies our participants came up with, and how the high score constantly improved even though this seemed unlikely at some point. We learned that starting off with a simple heuristic and expanding upon that is a good way to get your algorithm to solve a problem quickly. However, to win in our case, a hybrid solution involving a bit of manual engineering was needed to outperform all strategies relying solely on generalizing algorithms.</p><p>Quite likely, we will be re-using the framework we built to host some more code challenges. Will you look out for them?<br>Until that time, check out the repository of the DropBlox challenge <a href=\"https://github.com/godatadriven/dropblox\">here</a>.</p><p>We made the figures in this post using <a href=\"https://excalidraw.com/\">Excalidraw</a>.</p><p>At GoDataDriven, we use data &amp; AI to solve complex problems. But we share our knowledge too! If you liked this blogpost and want to learn more about Data Science, maybe the <a href=\"https://xebia.com/academy/nl/learning-journeys/data-scientist-learning-journeys/\">Data Science Learning Journey</a> is something for you.</p>","comment_id":"65993233cdf9710001822253","plaintext":"Conferences are great. You meet new people, you learn new things. But have you ever found yourself back in the hotel after a day at a conference, thinking what to do now? Or were you ever stuck in one session, wishing you had gone for that other one? These moments are the perfect opportunity to open up your laptop and compete with your peers in a coding challenge.\n\nAttendees of the three-day conference PyCon DE & PyData Berlin 2022 had the possibility to do so, with our coding challenge DropBlox.\n\nParticipants had a bit over one day to submit their solutions. After the deadline, we had received over 100 submissions and rewarded the well-deserved winner a Lego R2D2 in front of a great crowd.\n\nRead on to learn more about this challenge. We will discuss the following:\n\n * What was the challenge exactly, and what trade-offs were made in the design?\n * What was happening behind the screens to make this challenge possible?\n * How did we create hype at the conference itself?\n * What strategies were adopted by the participants to crack the problem?\n\nParticipants used a public repository that we made available here.\n\n\nChallenge\n\nParticipants of the challenge were given the following:\n\n * A 100 x 100 field\n * 1500 blocks of various colors and shapes, each with a unique identifier (see Fig. 1)\n * A rewards table, specifying the points and multipliers per color (see Table 1)\n\nThe rules are as follows:\n\n * Blocks can be dropped in the field from the top at a specified x-coordinate, without changing their rotation (see Fig. 2)\n * Each block can be used at most once\n * The score of a solution is computed using the rewards table. For each row, we add the points of each tile to the score. If the row consists of tiles of a single color only, we multiply the points of that row by the corresponding multiplier. The final score is the sum of the scores of the rows. (see Fig. 3)\n\n\n\nThe solution is a list of block IDs with corresponding x-coordinates. This list specifies which blocks to drop and where, in order to come to the final solution.\nThe goal of the challenge? Getting the most points possible.\n\n\nThe design\n\nWhen designing the challenge, we came up with a few simple requirements to follow:\n\n * The challenge should be easy to get started with\n * The progress and final solution should be easy to visualize\n * It should be difficult to approach the optimum solution\n\nIdeas about N-dimensional versions of this challenge came along, but only the ‚Äòsimple‚Äô 2D design ticked all the boxes. It‚Äôs easy to visualize, because it‚Äôs 2D, and (therefore) easy to get started with. Still, a 100 x 100 field with 1500 blocks allows for enough freedom to play this game in more ways than there are atoms in the universe!\n\n\nBehind the screens\n\nParticipants could, and anyone still can, submit their solutions on the submission page, as well as see the leaderboard with submissions of all other participants. To make this possible, several things happened behind the screens, which are worth noting here.\n\nMost importantly, we worked with a team of excited people with complementing skill sets. Together we created the framework that is visualized in Fig. 4.\n\nWe have a separate private repository, in which we keep all the logic that is hidden to the participant. In there, we have the ground-truth scoring function, and all logic necessary to run our web app. When participants submit their solution or check out the leaderboard, an Azure Function spins up to run the logic of our web app. The Azure Function is connected to an SQL database, where we store or retrieve submissions from. ¬†We store images, such as the visualization of the final solution, in the blob storage. To create the leaderboard, we retrieve the top-scoring submissions of each user and combine them with the corresponding images in the blob storage.\n\n\nCreating hype\n\nWhat‚Äôs the use of a competition if nobody knows about it? Spread the word!\n\nTo attract attention to our coding competition, we did two things. First, we set up an appealing booth at the company stands. We put our prize right in front, and a real-time dashboard showing the highscores beside. Surely, anyone walking past will at least question themselves what that Lego toy is doing at a Python conference.\n\nSecond, we went out to the conference Lightning Talks and announced our competition there. Really, the audience was great. Gotta love the energy at conferences like these.\n\nWith our promotion set up, competitors started trickling in. Let the games begin!\n\n\nStrategies\n\nStrategies varied from near brute-force approaches to the use of convolutional kernels and clever heuristics. In the following, we discuss some interesting and top-scoring submissions of participants.\n\n#14\n\nS. Tsch√∂ke named his first approach ‚ÄúBreakfast cereal‚Äù, as he was inspired by smaller cereal pieces collecting at the bottom and larger ones in the top of a cereal bag. Pieces were dropped from left to right, smaller ones before larger ones, until none could fit anymore. This approach, resulting in around 25k points, was however not successful enough.\n\nAfter a less successful, but brave approach using a genetic algorithm, he extended the breakfast cereal approach. This time, instead of the size of a block he used the block‚Äôs density, or the percentage of filled tiles within the height and width of a block; and he sorted the blocks by color. Taking a similar approach as before, but now including the different color orderings, resulted in 46k points. (See Fig. 6)\n\n#2\n\nWe jump up a few places to R. Garnier, who was #1 up until the last moments of the challenge. He went along with a small group of dedicated participants who started exploiting the row multipliers. Unexpectedly, this led to an interesting and exciting development in the competition.\n\nHis strategy consisted of two parts. The first is to manually construct some rows of the same color of the same height. This way, he created 3 full orange rows, one red and one purple. Subsequently, he uses a greedy algorithm, following the steps:\n\n 1. Assign a score to each block: score = (block values) / (block surface)\n 2. Sort the blocks by score\n 3. For each block, drop the block where it falls down the furthest\n\nThis strategy resulted in 62k points. (See Fig. 7)\n\n\n#1\n\nWith a single last-minute submission, G. Chanturia answered the question to ‚ÄúHow far can we go?‚Äù. He carefully constructed pairs of blocks that fit together, to manually engineer the bottom half of his solution, taking the row multipliers to the next level.\n\nG. is doing a PhD in Physics and a MSc in Computer Science, and fittingly splits his solution into a ‚Äúphysicist‚Äôs solution‚Äù and a ‚Äúprogrammer‚Äôs solution‚Äù.\n\nThe physicist‚Äôs solution refers to the bottom part of the field. The strategy used here, as summarized by G., was (1) taking a look at the blocks, and (2) placing them in a smart way. Whether you are a theoretical or experimental physicist, data serves as a starting point. G. noticed there is an order in the blocks. First of all, a lot of orange blocks had their top and bottom rows filled. Placing these in a clever way already results in six completely filled rows.\nSecond, there were ‚ÄúW‚Äù and ‚ÄúM‚Äù-shaped blocks that fit perfectly together. He kept going on like this to manually construct the bottom 1/3 of the field, accounting for 57% of the total score.\n\nThe programmer‚Äôs solution refers to the rest of the field. The problem with the first approach is that it is not scalable. Even more, if the blocks would change he would have to start all over. This second approach is more robust, and is similar to R. Garnier‚Äôs approach. The steps are:\n\n 1. Filter blocks based on their height. Blocks above height = 5 are filtered out, because many of these blocks have too weird shapes to work with.\n 2. Sort the blocks by points (or similarly, by color). Blocks with higher scoring colors are listed first.\n 3. Pick the first n available blocks in the sorted list. The larger n, the better the solution, but the longer it takes to run. The chosen number was around 50.\n 4. Find out which block can fall the furthest down in the field\n 5. Drop that block and remove it from the list\n 6. Repeat from 3\n\nAnd most importantly, the #1 place does not go home empty-handed. The #1 contender wins the infamous Lego R2D2! And we can acknowledge that indeed, yes, the R2D2 turned quite some heads at the conference. The award was given to the winner at the last Lightning Talk series of the conference.\n\n\nConclusion\n\nOrganising the coding challenge has been lots of fun. We created a framework to host a high score leaderboard, process submissions and display the puzzles online. To sum up the process in a few words:\n\n * Hosting a coding challenge at a conference is fun!\n * Gather participants by promoting the challenge\n * Hand over the prize to the winner\n\nIt was interesting to see what strategies our participants came up with, and how the high score constantly improved even though this seemed unlikely at some point. We learned that starting off with a simple heuristic and expanding upon that is a good way to get your algorithm to solve a problem quickly. However, to win in our case, a hybrid solution involving a bit of manual engineering was needed to outperform all strategies relying solely on generalizing algorithms.\n\nQuite likely, we will be re-using the framework we built to host some more code challenges. Will you look out for them?\nUntil that time, check out the repository of the DropBlox challenge here.\n\nWe made the figures in this post using Excalidraw.\n\nAt GoDataDriven, we use data & AI to solve complex problems. But we share our knowledge too! If you liked this blogpost and want to learn more about Data Science, maybe the Data Science Learning Journey is something for you.","feature_image":"__GHOST_URL__/content/images/2024/01/DropBlox-at-PyData-Berlin-2022-1.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-01-06 10:57:55","created_by":"1","updated_at":"2025-04-09 13:50:17","updated_by":"1","published_at":"2022-07-27 09:58:00","published_by":"1","custom_excerpt":"Conferences are great. You meet new people, you learn new things. But have you ever found yourself back in the hotel after a day at a conference, thinking what to do now? Or were you ever stuck in one session, wishing you had gone for that other one?","codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; url=https://xebia.com/blog/dropblox-coding-challenge-at-pycon-de-pydata-berlin-2022/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Conferences are great. You meet new people, you learn new things. But have you ever found yourself back in the hotel after a day at a conference, thinking what to do now? Or were you ever stuck in one session, wishing you had gone for that other one? These moments are the perfect opportunity to open up your laptop and compete with your peers in a coding challenge.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Attendees of the three-day conference PyCon DE & PyData Berlin 2022 had the possibility to do so, with our coding challenge DropBlox.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Participants had a bit over one day to submit their solutions. After the deadline, we had received over 100 submissions and rewarded the well-deserved winner a Lego R2D2 in front of a great crowd.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Read on to learn more about this challenge. We will discuss the following:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What was the challenge exactly, and what trade-offs were made in the design?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What was happening behind the screens to make this challenge possible?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How did we create hype at the conference itself?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What strategies were adopted by the participants to crack the problem?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Participants used a public repository that we made available \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"here\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/godatadriven/dropblox\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Challenge\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Participants of the challenge were given the following:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A 100 x 100 field\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"1500 blocks of various colors and shapes, each with a unique identifier (see Fig. 1)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A rewards table, specifying the points and multipliers per color (see Table 1)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/type-of-blocks.png.webp\",\"width\":500,\"height\":137,\"alt\":\"DropBlox: Type of Blocks\",\"caption\":\"Figure 1: A random subset of blocks from the challenge and their corresponding IDs.\"},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/point-scheme.png.webp\",\"width\":300,\"height\":349,\"alt\":\"Score scheme\",\"caption\":\"Table 1: The rewards table, specifying how each color contributes to the score of a solution of the challenge. We assign points to each tile in the final solution, while the multiplier only applies to rows filled with tiles of the same color.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The rules are as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Blocks can be dropped in the field from the top at a specified x-coordinate, without changing their rotation (see Fig. 2)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Each block can be used at most once\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The score of a solution is computed using the rewards table. For each row, we add the points of each tile to the score. If the row consists of tiles of a single color only, we multiply the points of that row by the corresponding multiplier. The final score is the sum of the scores of the rows. (see Fig. 3)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/situation-one.png.webp\",\"width\":500,\"height\":237,\"alt\":\"DropBlox: Example 1\",\"caption\":\"Figure 2: An example 6√ó6 field and a blue block dropped at x-coordinate 1.\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/situation-2.png.webp\",\"width\":500,\"height\":215,\"alt\":\"DropBlox: Example 2\",\"caption\":\"Figure 3: An example of the computation of the score of a solution. The points and multipliers per color are specified in Table 1.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The solution is a list of block IDs with corresponding x-coordinates. This list specifies which blocks to drop and where, in order to come to the final solution.\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The goal of the challenge? Getting the most points possible.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The design\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"When designing the challenge, we came up with a few simple requirements to follow:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The challenge should be easy to get started with\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The progress and final solution should be easy to visualize\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"It should be difficult to approach the optimum solution\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ideas about N-dimensional versions of this challenge came along, but only the ‚Äòsimple‚Äô 2D design ticked all the boxes. It‚Äôs easy to visualize, because it‚Äôs 2D, and (therefore) easy to get started with. Still, a 100 x 100 field with 1500 blocks allows for enough freedom to play this game in more ways than there are atoms in the universe!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Behind the screens\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Participants could, and anyone still can, submit their solutions on the \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"submission page\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://dropblox.azurewebsites.net/submit\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", as well as see the leaderboard with submissions of all other participants. To make this possible, several things happened behind the screens, which are worth noting here.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Most importantly, we worked with a team of excited people with complementing skill sets. Together we created the framework that is visualized in Fig. 4.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We have a separate private repository, in which we keep all the logic that is hidden to the participant. In there, we have the ground-truth scoring function, and all logic necessary to run our web app. When participants submit their solution or check out the leaderboard, an Azure Function spins up to run the logic of our web app. The Azure Function is connected to an SQL database, where we store or retrieve submissions from.  We store images, such as the visualization of the final solution, in the blob storage. To create the leaderboard, we retrieve the top-scoring submissions of each user and combine them with the corresponding images in the blob storage.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/components.png.webp\",\"width\":600,\"height\":237,\"alt\":\"Components of DropBlox\",\"caption\":\"Figure 4: The different components of the challenge, including those hidden to the participants.\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Creating hype\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What‚Äôs the use of a competition if nobody knows about it? Spread the word!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"To attract attention to our coding competition, we did two things. First, we set up an appealing booth at the company stands. We put our prize right in front, and a real-time dashboard showing the highscores beside. Surely, anyone walking past will at least question themselves what that Lego toy is doing at a Python conference.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/booth_gdd.png.webp\",\"width\":500,\"height\":331,\"alt\":\"booth_gdd\",\"caption\":\"Figure 5: Our company booth at PyCon DE &amp; PyData Berlin 2022\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Second, we went out to the conference Lightning Talks and announced our competition there. Really, the audience was great. Gotta love the energy at conferences like these.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/Screenshot-2022-07-27-at-14.56.10.png.webp\",\"width\":500,\"height\":284,\"alt\":\"DropBlox Explained\",\"caption\":\"Figure 6: Promoting the challenge at a Lightning Talk\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With our promotion set up, competitors started trickling in. Let the games begin!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Strategies\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Strategies varied from near brute-force approaches to the use of convolutional kernels and clever heuristics. In the following, we discuss some interesting and top-scoring submissions of participants.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"#14\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h4\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"S. Tsch√∂ke named his first approach ‚ÄúBreakfast cereal‚Äù, as he was inspired by smaller cereal pieces collecting at the bottom and larger ones in the top of a cereal bag. Pieces were dropped from left to right, smaller ones before larger ones, until none could fit anymore. This approach, resulting in around 25k points, was however not successful enough.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"After a less successful, but brave approach using a genetic algorithm, he extended the breakfast cereal approach. This time, instead of the size of a block he used the block‚Äôs density, or the percentage of filled tiles within the height and width of a block; and he sorted the blocks by color. Taking a similar approach as before, but now including the different color orderings, resulted in 46k points. (See Fig. 6)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/number-14.png\",\"width\":450,\"height\":450,\"alt\":\"DropBlox solution of the number 12\",\"caption\":\"Figure 6: Final solution of the #14 in the challenge, S. Tsch√∂ke, with 45844 points.\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"#2\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h4\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We jump up a few places to R. Garnier, who was #1 up until the last moments of the challenge. He went along with a small group of dedicated participants who started exploiting the row multipliers. Unexpectedly, this led to an interesting and exciting development in the competition.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"His strategy consisted of two parts. The first is to manually construct some rows of the same color of the same height. This way, he created 3 full orange rows, one red and one purple. Subsequently, he uses a greedy algorithm, following the steps:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Assign a score to each block: score = (block values) / (block surface)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Sort the blocks by score\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"For each block, drop the block where it falls down the furthest‚Äã‚Äã\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This strategy resulted in 62k points. (See Fig. 7)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/number-2.png\",\"width\":450,\"height\":450,\"alt\":\"DropBlox solution of the number 2\",\"caption\":\"Figure 7: Final solution of the #2 in the challenge, R. Garnier, with 62032 points.\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"#1\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With a single last-minute submission, G. Chanturia answered the question to ‚ÄúHow far can we go?‚Äù. He carefully constructed pairs of blocks that fit together, to manually engineer the bottom half of his solution, taking the row multipliers to the next level.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"G. is doing a PhD in Physics and a MSc in Computer Science, and fittingly splits his solution into a ‚Äúphysicist‚Äôs solution‚Äù and a ‚Äúprogrammer‚Äôs solution‚Äù.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The physicist‚Äôs solution refers to the bottom part of the field. The strategy used here, as summarized by G., was (1) taking a look at the blocks, and (2) placing them in a smart way. Whether you are a theoretical or experimental physicist, data serves as a starting point. G. noticed there is an order in the blocks. First of all, a lot of orange blocks had their top and bottom rows filled. Placing these in a clever way already results in six completely filled rows.\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Second, there were ‚ÄúW‚Äù and ‚ÄúM‚Äù-shaped blocks that fit perfectly together. He kept going on like this to manually construct the bottom 1/3 of the field, accounting for 57% of the total score.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The programmer‚Äôs solution refers to the rest of the field. The problem with the first approach is that it is not scalable. Even more, if the blocks would change he would have to start all over. This second approach is more robust, and is similar to R. Garnier‚Äôs approach. The steps are:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Filter blocks based on their height. Blocks above height = 5 are filtered out, because many of these blocks have too weird shapes to work with.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Sort the blocks by points (or similarly, by color). Blocks with higher scoring colors are listed first.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Pick the first n available blocks in the sorted list. The larger n, the better the solution, but the longer it takes to run. The chosen number was around 50.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Find out which block can fall the furthest down in the field\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Drop that block and remove it from the list\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":5,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Repeat from 3\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":6,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/number-one.png\",\"width\":450,\"height\":450,\"alt\":\"DropBlox solution of the number 1\",\"caption\":\"Figure 8: Final solution of the #1 in the challenge, G. Chanturia, with 68779 points.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"And most importantly, the #1 place does not go home empty-handed. The #1 contender wins the infamous Lego R2D2! And we can acknowledge that indeed, yes, the R2D2 turned quite some heads at the conference. The award was given to the winner at the last Lightning Talk series of the conference.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2023/02/award-handover.jpg.webp\",\"width\":501,\"height\":278,\"alt\":\"award handover\",\"caption\":\"Figure 9: Our winner receives his coveted prize!\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Conclusion\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Organising the coding challenge has been lots of fun. We created a framework to host a high score leaderboard, process submissions and display the puzzles online. To sum up the process in a few words:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hosting a coding challenge at a conference is fun!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gather participants by promoting the challenge\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hand over the prize to the winner\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"It was interesting to see what strategies our participants came up with, and how the high score constantly improved even though this seemed unlikely at some point. We learned that starting off with a simple heuristic and expanding upon that is a good way to get your algorithm to solve a problem quickly. However, to win in our case, a hybrid solution involving a bit of manual engineering was needed to outperform all strategies relying solely on generalizing algorithms.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Quite likely, we will be re-using the framework we built to host some more code challenges. Will you look out for them?\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Until that time, check out the repository of the DropBlox challenge \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"here\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/godatadriven/dropblox\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We made the figures in this post using \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Excalidraw\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://excalidraw.com/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"At GoDataDriven, we use data & AI to solve complex problems. But we share our knowledge too! If you liked this blogpost and want to learn more about Data Science, maybe the \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Data Science Learning Journey\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://xebia.com/academy/nl/learning-journeys/data-scientist-learning-journeys/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" is something for you.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"65993347cdf971000182225e","uuid":"ee4dd13e-8c28-46e0-885f-af26b76ffd41","title":"Scaling up: bringing your Azure DevOps CI/CD setup to the next level üöÄ (on Xebia.com ‚ßâ)","slug":"scaling-up-bringing-your-azure-devops-ci-cd-setup-to-the-next-level","mobiledoc":null,"html":"<p> <strong><strong>Introduction</strong></strong></p><p>Azure DevOps pipelines are a great way to automate your CI/CD process. Most often, they are configured on a per project basis. This works fine when you have few projects. But what if you have many projects? In this blog post, we will show you how you can scale up your Azure DevOps CI/CD setup for reusability and easy maintenance.</p><h2 id=\"your-typical-devops-pipeline\"><strong><strong>Your typical DevOps pipeline</strong></strong></h2><p>A typical DevOps pipeline is placed inside the project repository. Let‚Äôs consider a pipeline for a Python project. It includes the following steps:</p><ul><li><strong>quality checks</strong> such as code formatting and linting</li><li><strong>building a package</strong> such as a Python wheel</li><li><strong>releasing a package</strong> to Python package registry (such as <a href=\"https://learn.microsoft.com/en-us/azure/devops/artifacts/start-using-azure-artifacts?view=azure-devops&amp;tabs=nugettfs%2Cnuget%2Corgstorage\">Azure Artifacts</a> or PyPi)</li></ul><p>Using an Azure DevOps pipeline, we can achieve this like so:</p><pre><code class=\"language-yaml\">trigger:\n- main\n\nsteps:\n# Python setup &amp; dependencies\n- task: UsePythonVersion@0\n  inputs:\n    versionSpec: 3.10\n\n- script: |\n    pip install .[dev,build,release]\n  displayName: 'Install dependencies'\n\n# Code Quality\n- script: |\n    black --check .\n  displayName: 'Formatting'\n\n- script: |\n    flake8 .\n  displayName: 'Linting'\n\n- script: |\n    pytest .\n  displayName: 'Testing'\n\n# Build\n- script: |\n    echo $(Build.BuildNumber) &gt; version.txt\n  displayName: 'Set version number'\n\n- script: |\n    pip wheel \\\n      --no-deps \\\n      --wheel-dir dist/ \\\n      .\n  displayName: 'Build wheel'\n\n# Publish\n- task: TwineAuthenticate@1\n  inputs:\n    artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\n  displayName: 'Authenticate pip with twine'\n\n- script: |\n    twine upload \\\n      --config-file $(PYPIRC_PATH) \\\n      --repository devops-pipelines-blogpost \\\n      dist/*.whl\n  displayName: 'Publish wheel with twine'\n</code></pre><p>Well, that is great, right? We have achieved all the goals we desired:</p><ul><li>Code quality checks using <em>black</em>, <em>flake8</em> and <em>pytest</em>.</li><li>Build and package the project as a Python <em>wheel</em>.</li><li>Publish the package to a registry of choice, in this case <em>Azure Artifacts</em>.</li></ul><h2 id=\"growing-pains\"><strong><strong>Growing pains</strong></strong></h2><p>A DevOps pipeline like the above works fine for a single project. But, ‚Ä¶ what if we want to scale up? Say our company grows, we create more repositories and more projects need to be packaged and released. Will we simply copy this pipeline and paste it into a new repository? Given that we are growing in size, can we be more efficient than just running this pipeline from start to finish?</p><p>The answer is no ‚Äì we do not have to copy/paste all these pipelines into a new repo, and the answer is yes ‚Äì we can be more efficient in running these pipelines. Let‚Äôs see how.</p><h2 id=\"scaling-up-properly\"><strong><strong>Scaling up properly</strong></strong></h2><p>Let‚Äôs see how we can create scalable DevOps pipelines. First, we are going to introduce <strong>DevOps pipeline <a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops&amp;pivots=templates-includes\">templates</a></strong>. These are modular pieces of pipeline that we can reuse across various pipelines and also across various projects residing in different repositories.</p><p>Let‚Äôs see how we can use pipeline templates to our advantage.</p><h3 id=\"1-devops-template-setup\"><strong><strong>1. DevOps template setup</strong></strong></h3><p>Let‚Äôs rewrite pieces of our pipeline into DevOps pipeline templates. Important to know here is that you can write templates for either <code>stages</code>, <code>jobs</code> or <code>steps</code>. The hierarchy is as follows:</p><pre><code class=\"language-yaml\">stages:\n- stage: Stage1\n  jobs:\n  - job: Job1\n    steps:\n    - step: Step1\n</code></pre><p>This can be illustrated in an image like so:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-stages-jobs-steps-illustration.jpg.webp\" class=\"kg-image\" alt=\"Stages jobs steps illustration\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-stages-jobs-steps-illustration.jpg.webp 600w, __GHOST_URL__/content/images/size/w1000/2024/04/scaling-up-your-azure-devops-ci-cd-setup-stages-jobs-steps-illustration.jpg.webp 1000w, __GHOST_URL__/content/images/size/w1600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-stages-jobs-steps-illustration.jpg.webp 1600w, __GHOST_URL__/content/images/size/w2400/2024/04/scaling-up-your-azure-devops-ci-cd-setup-stages-jobs-steps-illustration.jpg.webp 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>We can then create a template in one file, for example for <code>steps</code>:</p><p><strong>templates/code-quality.yml</strong></p><pre><code class=\"language-yaml\">steps:\n- script: |\n    echo \"Hello world!\"\n</code></pre><p>.. and reuse it in our former pipeline:</p><pre><code class=\"language-yaml\">stages:\n- stage: Stage1\n  jobs:\n  - job: Job1\n    steps:\n    - template: templates/code-quality.yml\n</code></pre><p>‚Ä¶ or for those who prefer a more visual way of displaying it:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-using-templates-in-your-own-repo.png.webp\" class=\"kg-image\" alt=\"Using templates in your own repo illustration\" loading=\"lazy\" width=\"1551\" height=\"608\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-using-templates-in-your-own-repo.png.webp 600w, __GHOST_URL__/content/images/size/w1000/2024/04/scaling-up-your-azure-devops-ci-cd-setup-using-templates-in-your-own-repo.png.webp 1000w, __GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-using-templates-in-your-own-repo.png.webp 1551w\" sizes=\"(min-width: 720px) 720px\"></figure><p>That‚Äôs how easy it is to use DevOps pipeline templates! Let‚Äôs now apply it to our own usecase.</p><h4 id=\"code-quality-checks-template\"><strong><strong>Code quality checks template</strong></strong></h4><p>First, let‚Äôs put the code quality checks pipeline into a template. We are also making the pipeline more extensive so it outputs test results and coverage reports. Remember, we are only defining this template once and then reusing it in other places.</p><p><strong>templates/code-quality.yml</strong></p><pre><code class=\"language-yaml\">steps:\n# Code Quality\n- script: |\n    black --check .\n  displayName: 'Formatting'\n\n- script: |\n    flake8 .\n  displayName: 'Linting'\n\n- script: |\n    pytest \\\n      --junitxml=junit/test-results.xml \\\n      --cov=. \\\n      --cov-report=xml:coverage.xml \\\n      .\n  displayName: 'Testing'\n\n# Publish test results + coverage\n- task: PublishTestResults@2\n  condition: succeededOrFailed()\n  inputs:\n    testResultsFiles: '**/test-*.xml'\n    testRunTitle: 'Publish test results'\n    failTaskOnFailedTests: true\n  displayName: 'Publish test results'\n\n- task: PublishCodeCoverageResults@1\n  inputs:\n    codeCoverageTool: 'Cobertura'\n    summaryFileLocation: '**/coverage.xml'\n  displayName: 'Publish test coverage'\n</code></pre><p>‚Ä¶ which we are using like so:</p><pre><code class=\"language-yaml\">steps:\n- template: templates/code-quality.yml\n</code></pre><p>Easy! Also note we included two additional tasks: one to publish the test results and another to publish code coverage reports. That information is super useful to display inside DevOps. Lucky for us, DevOps has support for that:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-checks.png.webp\" class=\"kg-image\" alt=\"Code Quality checks display in DevOps\" loading=\"lazy\" width=\"526\" height=\"306\"></figure><p>‚Ä¶ clicking on the test results brings us to the <em>Tests</em> view, where we can exactly which test failed (if any failed):</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-test-view.png.webp\" class=\"kg-image\" alt=\"Code quality test view\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-test-view.png.webp 600w, __GHOST_URL__/content/images/size/w1000/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-test-view.png.webp 1000w, __GHOST_URL__/content/images/size/w1600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-test-view.png.webp 1600w, __GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-test-view.png.webp 2078w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Lastly, there‚Äôs also a view explaining which lines of code you covered with tests and which you did not:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-coverage-view.jpg.webp\" class=\"kg-image\" alt=\"Code quality coverage view\" loading=\"lazy\" width=\"2000\" height=\"1618\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-coverage-view.jpg.webp 600w, __GHOST_URL__/content/images/size/w1000/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-coverage-view.jpg.webp 1000w, __GHOST_URL__/content/images/size/w1600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-coverage-view.jpg.webp 1600w, __GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-coverage-view.jpg.webp 2054w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Those come in very useful when you are working on testing your code!</p><p>Now, we have defined this all in DevOps <em>templates</em>. That gives us a more comfortable position to define more elaborate pipeline steps because we will <em>import</em> those templates instead of copy/pasting them.</p><p>That said, we can summarise the benefits of using DevOps templates like so:</p><ul><li>Define once, reuse everywhere<br>We can reuse this code quality checks pipeline in both the same project multiple times but also from other repositories. If you are importing from another repo, see ‚Äò<a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops&amp;pivots=templates-includes#use-other-repositories\">Use other repositories</a>‚Äò for setup.</li><li>Make it failproof<br>You can invest into making just this template very good; instead of having multiple bad versions hanging around in your organisation.</li><li>Reduce complexity<br>Abstracting away commonly used code can be efficient for the readability of your pipeline. This allows newcomers to easily understand the different parts of your CI/CD setup using DevOps pipelines.</li></ul><h3 id=\"2-passing-data-between-templates\"><strong><strong>2. Passing data between templates</strong></strong></h3><p>Let‚Äôs go a step further and also abstract away the build and release steps into templates. We are going to use the following template for <strong>building a Python wheel</strong>:</p><pre><code class=\"language-yaml\">steps:\n# Build wheel\n- script: |\n    echo $(Build.BuildNumber) &gt; version.txt\n  displayName: 'Set version number'\n\n- script: |\n    pip wheel \\\n      --no-deps \\\n      --wheel-dir dist/ \\\n      .\n  displayName: 'Build wheel'\n\n# Upload wheel as artifact\n- task: CopyFiles@2\n  inputs:\n    contents: dist/**\n    targetFolder: $(Build.ArtifactStagingDirectory)\n  displayName: 'Copy wheel to artifacts directory'\n\n- publish: '$(Build.ArtifactStagingDirectory)/dist'\n  artifact: wheelFiles\n  displayName: 'Upload wheel as artifact'\n</code></pre><p>This definition is slightly different than the one we defined before, in the initial pipeline. This pipeline uses <strong><a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/artifacts/pipeline-artifacts?view=azure-devops&amp;tabs=yaml\">artifacts</a></strong>. These allow us to <em>pass</em> data between jobs or stages. This is useful when we want to split up our pipeline into smaller pieces. Splitting up the process into smaller segments gives us more visibility and control over the process. Another benefit of this, is that when we split the Python wheel build and release process, is that we give ourselves the ability to <em>release to multiple providers at once</em>.</p><p>When this pipeline is ran we can see an artifact (a wheel file) has been added:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example.png.webp\" class=\"kg-image\" alt=\"Build artifact example\" loading=\"lazy\" width=\"506\" height=\"480\"></figure><p>‚Ä¶ with the actual wheel file in there:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example-wheel-file.png.webp\" class=\"kg-image\" alt=\"Build artifact wheel file\" loading=\"lazy\" width=\"1756\" height=\"668\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example-wheel-file.png.webp 600w, __GHOST_URL__/content/images/size/w1000/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example-wheel-file.png.webp 1000w, __GHOST_URL__/content/images/size/w1600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example-wheel-file.png.webp 1600w, __GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example-wheel-file.png.webp 1756w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This is also useful so we can inspect what the build pipeline has produced. We can now <em>download</em> this wheel file from the artifacts again. We will do this in the publish pipeline.</p><p><strong>template/publish-wheel.yml</strong></p><pre><code class=\"language-yaml\">parameters:\n- name: artifactFeed\n  type: string\n- name: repositoryName\n  type: string\n\nsteps:\n# Retrieve wheel\n- download: current\n  artifact: wheelFiles\n  displayName: 'Download artifacts'\n\n# Publish wheel\n- task: TwineAuthenticate@1\n  inputs:\n    artifactFeed: ${{ parameters.artifactFeed }}\n  displayName: 'Authenticate pip with twine'\n\n- script: |\n    twine upload \\\n      --config-file $(PYPIRC_PATH) \\\n      --repository ${{ parameters.repositoryName }} \\\n      $(Pipeline.Workspace)/wheelFiles/*.whl\n  displayName: 'Publish wheel with twine'\n</code></pre><p>‚Ä¶ both the build- and release pipeline can be used like so:</p><pre><code class=\"language-yaml\">- stage: Build\n  jobs:\n  - job: BuildWheel\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: 3.10\n\n    - script: |\n        pip install .[build]\n      displayName: 'Install dependencies'\n\n    - template: templates/build-wheel.yml\n\n\n- stage: Publish\n  jobs:\n  - job: PublishWheel\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\n        repositoryName: 'devops-pipelines-blogpost'\n</code></pre><p>And here we have another new feature coming in. <a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/stages?view=azure-devops&amp;tabs=yaml\">Stages</a>. These allow us to execute pipelines that depend on each other. We have now split up our pipeline into 2 stages:</p><ol><li>Build stage</li><li>Publish stage</li></ol><p>Using stages makes it easy to see what is going on. It provides transparency and allows you to easily track the progress of the pipeline. You can also launch stages separately, skipping previous stages, as long as the necessary dependencies are in place. For example, dependencies can include artifacts, which were generated in previous stage.</p><h4 id=\"improving-the-release-process\"><strong><strong>Improving the release process</strong></strong></h4><p>So what is another advantage of this setup? Say that you are releasing your package to <strong>two pip registries</strong>. Doing that is easy using this setup by creating two jobs in the publish stage:</p><pre><code class=\"language-yaml\">- stage: Publish\n  jobs:\n  - job: PublishToRegistryOne\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/registry-1'\n        repositoryName: 'devops-pipelines-blogpost'\n\n  - job: PublishToRegistryTwo\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/registry-2'\n        repositoryName: 'devops-pipelines-blogpost'\n</code></pre><p>As you can see, we can use the defined templates to scale our pipelines. What is essential here, is that thanks to using the artifacts, we can build our wheel <em>once</em> and consume that same wheel <em>multiple</em> times.</p><p>Additionally, the publishing jobs launch <strong>in parallel</strong> by default (unless dependencies are explicitly defined). This speeds up your release process.</p><h3 id=\"3-automate-using-a-strategy-matrix\"><strong><strong>3. Automate using a strategy matrix</strong></strong></h3><p>Let‚Äôs go back to the code quality stage for a minute. In the code quality stage, we are first installing a certain Python version, and then running all quality checks. However, we might need guarantees that our code works for multiple Python versions. This is often the case when releasing a package, for example. How can we easily automate running our Code Quality pipeline using our pipeline <em>templates</em>? One option is to manually define a couple jobs and install the correct python version in each job. Another option is to use a <strong><a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/jobs-job-strategy?view=azure-pipelines\">strategy matrix</a></strong>. This allows us to define a matrix of variables that we can use in our pipeline.</p><p>We can improve our <code>CodeQualityChecks</code> job like so:</p><pre><code class=\"language-yaml\">jobs:\n- job: CodeQualityChecks\n  strategy:\n    matrix:\n      Python38:\n        python.version: '3.8'\n      Python39:\n        python.version: '3.9'\n      Python310:\n        python.version: '3.10'\n      Python311:\n        python.version: '3.11'\n\n  steps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: $(python.version)\n\n  - script: |\n      pip install .[dev]\n    displayName: 'Install dependencies'\n\n  - template: templates/code-quality.yml\n</code></pre><p>Awesome! The pipeline now runs the entire code quality pipeline for each Python version. Looking at how our pipeline runs now, we can see multiple jobs, one for each Python version:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-strategy-matrix-example.png.webp\" class=\"kg-image\" alt=\"Strategy matrix jobs example in parallel\" loading=\"lazy\" width=\"1574\" height=\"866\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/04/scaling-up-your-azure-devops-ci-cd-setup-strategy-matrix-example.png.webp 600w, __GHOST_URL__/content/images/size/w1000/2024/04/scaling-up-your-azure-devops-ci-cd-setup-strategy-matrix-example.png.webp 1000w, __GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-strategy-matrix-example.png.webp 1574w\" sizes=\"(min-width: 720px) 720px\"></figure><p>.. as you can see 4 jobs are launched. If no job dependencies are explicitly set, jobs within one stage run in parallel! That greatly speed up the pipeline and lets you iterate faster! That‚Äôs definitely a win.</p><h2 id=\"final-result\"><strong><strong>Final result</strong></strong></h2><p>Let‚Äôs wrap it up! Our entire pipeline, using <strong>templates</strong>:</p><pre><code class=\"language-yaml\">trigger:\n- main\n\nstages:\n- stage: CodeQuality\n  jobs:\n  - job: CodeQualityChecks\n    strategy:\n      matrix:\n        Python38:\n          python.version: '3.8'\n        Python39:\n          python.version: '3.9'\n        Python310:\n          python.version: '3.10'\n        Python311:\n          python.version: '3.11'\n\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: $(python.version)\n\n    - script: |\n        pip install .[dev]\n      displayName: 'Install dependencies'\n\n    - template: templates/code-quality.yml\n\n\n- stage: Build\n  jobs:\n  - job: BuildWheel\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: 3.10\n\n    - script: |\n        pip install .[build]\n      displayName: 'Install dependencies'\n\n    - template: templates/build-wheel.yml\n\n\n- stage: Publish\n  jobs:\n  - job: PublishWheel\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\n        repositoryName: 'devops-pipelines-blogpost'\n</code></pre><p>‚Ä¶ which uses these templates:</p><ul><li><a href=\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost?path=/02_better_pipeline/templates/code-quality.yml\">templates/code-quality.yml</a></li><li><a href=\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost?path=/02_better_pipeline/templates/build-wheel.yml\">templates/build-wheel.yml</a></li><li><a href=\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost?path=/02_better_pipeline/templates/publish-wheel.yml\">templates/publish-wheel.yml</a></li></ul><p>‚Ä¶ for the entire source code see the <a href=\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost\">better-devops-pipelines-blogpost</a> repo. The repository contains pipelines that apply above explained principles. The pipelines provide testing, building and releasing for a Python project ‚úì.</p><h2 id=\"conclusion\"><strong><strong>Conclusion</strong></strong></h2><p>We demonstrated how to scale up your Azure DevOps CI/CD setup making it reusable, maintainable and modular. This helps you maintain a good CI/CD setup as your company grows.</p><p>In short, we achieved the following:</p><ul><li>Create modular DevOps pipelines using <a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops&amp;pivots=templates-includes\">templates</a>. This makes it more easy to reuse pipelines across projects and repositories</li><li>Pass data between DevOps pipeline jobs using <a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/artifacts/pipeline-artifacts?view=azure-devops&amp;tabs=yaml\">artifacts</a>. This allows us to split up our pipeline into smaller pieces, that can consume artifacts from previous jobs.</li><li>Split up your pipeline in <a href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/stages?view=azure-devops&amp;tabs=yaml\">stages</a> to create more visibility and control over your CI/CD</li></ul><blockquote>An example repository containing good-practice pipelines is available at:<br><br><strong><a href=\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost\">https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost</a></strong></blockquote><p>Cheers üôè</p>","comment_id":"65993347cdf971000182225e","plaintext":"Introduction\n\nAzure DevOps pipelines are a great way to automate your CI/CD process. Most often, they are configured on a per project basis. This works fine when you have few projects. But what if you have many projects? In this blog post, we will show you how you can scale up your Azure DevOps CI/CD setup for reusability and easy maintenance.\n\n\nYour typical DevOps pipeline\n\nA typical DevOps pipeline is placed inside the project repository. Let‚Äôs consider a pipeline for a Python project. It includes the following steps:\n\n * quality checks such as code formatting and linting\n * building a package such as a Python wheel\n * releasing a package to Python package registry (such as Azure Artifacts or PyPi)\n\nUsing an Azure DevOps pipeline, we can achieve this like so:\n\ntrigger:\n- main\n\nsteps:\n# Python setup & dependencies\n- task: UsePythonVersion@0\n  inputs:\n    versionSpec: 3.10\n\n- script: |\n    pip install .[dev,build,release]\n  displayName: 'Install dependencies'\n\n# Code Quality\n- script: |\n    black --check .\n  displayName: 'Formatting'\n\n- script: |\n    flake8 .\n  displayName: 'Linting'\n\n- script: |\n    pytest .\n  displayName: 'Testing'\n\n# Build\n- script: |\n    echo $(Build.BuildNumber) > version.txt\n  displayName: 'Set version number'\n\n- script: |\n    pip wheel \\\n      --no-deps \\\n      --wheel-dir dist/ \\\n      .\n  displayName: 'Build wheel'\n\n# Publish\n- task: TwineAuthenticate@1\n  inputs:\n    artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\n  displayName: 'Authenticate pip with twine'\n\n- script: |\n    twine upload \\\n      --config-file $(PYPIRC_PATH) \\\n      --repository devops-pipelines-blogpost \\\n      dist/*.whl\n  displayName: 'Publish wheel with twine'\n\n\nWell, that is great, right? We have achieved all the goals we desired:\n\n * Code quality checks using black, flake8 and pytest.\n * Build and package the project as a Python wheel.\n * Publish the package to a registry of choice, in this case Azure Artifacts.\n\n\nGrowing pains\n\nA DevOps pipeline like the above works fine for a single project. But, ‚Ä¶ what if we want to scale up? Say our company grows, we create more repositories and more projects need to be packaged and released. Will we simply copy this pipeline and paste it into a new repository? Given that we are growing in size, can we be more efficient than just running this pipeline from start to finish?\n\nThe answer is no ‚Äì we do not have to copy/paste all these pipelines into a new repo, and the answer is yes ‚Äì we can be more efficient in running these pipelines. Let‚Äôs see how.\n\n\nScaling up properly\n\nLet‚Äôs see how we can create scalable DevOps pipelines. First, we are going to introduce DevOps pipeline templates. These are modular pieces of pipeline that we can reuse across various pipelines and also across various projects residing in different repositories.\n\nLet‚Äôs see how we can use pipeline templates to our advantage.\n\n\n1. DevOps template setup\n\nLet‚Äôs rewrite pieces of our pipeline into DevOps pipeline templates. Important to know here is that you can write templates for either stages, jobs or steps. The hierarchy is as follows:\n\nstages:\n- stage: Stage1\n  jobs:\n  - job: Job1\n    steps:\n    - step: Step1\n\n\nThis can be illustrated in an image like so:\n\nWe can then create a template in one file, for example for steps:\n\ntemplates/code-quality.yml\n\nsteps:\n- script: |\n    echo \"Hello world!\"\n\n\n.. and reuse it in our former pipeline:\n\nstages:\n- stage: Stage1\n  jobs:\n  - job: Job1\n    steps:\n    - template: templates/code-quality.yml\n\n\n‚Ä¶ or for those who prefer a more visual way of displaying it:\n\nThat‚Äôs how easy it is to use DevOps pipeline templates! Let‚Äôs now apply it to our own usecase.\n\nCode quality checks template\n\nFirst, let‚Äôs put the code quality checks pipeline into a template. We are also making the pipeline more extensive so it outputs test results and coverage reports. Remember, we are only defining this template once and then reusing it in other places.\n\ntemplates/code-quality.yml\n\nsteps:\n# Code Quality\n- script: |\n    black --check .\n  displayName: 'Formatting'\n\n- script: |\n    flake8 .\n  displayName: 'Linting'\n\n- script: |\n    pytest \\\n      --junitxml=junit/test-results.xml \\\n      --cov=. \\\n      --cov-report=xml:coverage.xml \\\n      .\n  displayName: 'Testing'\n\n# Publish test results + coverage\n- task: PublishTestResults@2\n  condition: succeededOrFailed()\n  inputs:\n    testResultsFiles: '**/test-*.xml'\n    testRunTitle: 'Publish test results'\n    failTaskOnFailedTests: true\n  displayName: 'Publish test results'\n\n- task: PublishCodeCoverageResults@1\n  inputs:\n    codeCoverageTool: 'Cobertura'\n    summaryFileLocation: '**/coverage.xml'\n  displayName: 'Publish test coverage'\n\n\n‚Ä¶ which we are using like so:\n\nsteps:\n- template: templates/code-quality.yml\n\n\nEasy! Also note we included two additional tasks: one to publish the test results and another to publish code coverage reports. That information is super useful to display inside DevOps. Lucky for us, DevOps has support for that:\n\n‚Ä¶ clicking on the test results brings us to the Tests view, where we can exactly which test failed (if any failed):\n\nLastly, there‚Äôs also a view explaining which lines of code you covered with tests and which you did not:\n\nThose come in very useful when you are working on testing your code!\n\nNow, we have defined this all in DevOps templates. That gives us a more comfortable position to define more elaborate pipeline steps because we will import those templates instead of copy/pasting them.\n\nThat said, we can summarise the benefits of using DevOps templates like so:\n\n * Define once, reuse everywhere\n   We can reuse this code quality checks pipeline in both the same project multiple times but also from other repositories. If you are importing from another repo, see ‚ÄòUse other repositories‚Äò for setup.\n * Make it failproof\n   You can invest into making just this template very good; instead of having multiple bad versions hanging around in your organisation.\n * Reduce complexity\n   Abstracting away commonly used code can be efficient for the readability of your pipeline. This allows newcomers to easily understand the different parts of your CI/CD setup using DevOps pipelines.\n\n\n2. Passing data between templates\n\nLet‚Äôs go a step further and also abstract away the build and release steps into templates. We are going to use the following template for building a Python wheel:\n\nsteps:\n# Build wheel\n- script: |\n    echo $(Build.BuildNumber) > version.txt\n  displayName: 'Set version number'\n\n- script: |\n    pip wheel \\\n      --no-deps \\\n      --wheel-dir dist/ \\\n      .\n  displayName: 'Build wheel'\n\n# Upload wheel as artifact\n- task: CopyFiles@2\n  inputs:\n    contents: dist/**\n    targetFolder: $(Build.ArtifactStagingDirectory)\n  displayName: 'Copy wheel to artifacts directory'\n\n- publish: '$(Build.ArtifactStagingDirectory)/dist'\n  artifact: wheelFiles\n  displayName: 'Upload wheel as artifact'\n\n\nThis definition is slightly different than the one we defined before, in the initial pipeline. This pipeline uses artifacts. These allow us to pass data between jobs or stages. This is useful when we want to split up our pipeline into smaller pieces. Splitting up the process into smaller segments gives us more visibility and control over the process. Another benefit of this, is that when we split the Python wheel build and release process, is that we give ourselves the ability to release to multiple providers at once.\n\nWhen this pipeline is ran we can see an artifact (a wheel file) has been added:\n\n‚Ä¶ with the actual wheel file in there:\n\nThis is also useful so we can inspect what the build pipeline has produced. We can now download this wheel file from the artifacts again. We will do this in the publish pipeline.\n\ntemplate/publish-wheel.yml\n\nparameters:\n- name: artifactFeed\n  type: string\n- name: repositoryName\n  type: string\n\nsteps:\n# Retrieve wheel\n- download: current\n  artifact: wheelFiles\n  displayName: 'Download artifacts'\n\n# Publish wheel\n- task: TwineAuthenticate@1\n  inputs:\n    artifactFeed: ${{ parameters.artifactFeed }}\n  displayName: 'Authenticate pip with twine'\n\n- script: |\n    twine upload \\\n      --config-file $(PYPIRC_PATH) \\\n      --repository ${{ parameters.repositoryName }} \\\n      $(Pipeline.Workspace)/wheelFiles/*.whl\n  displayName: 'Publish wheel with twine'\n\n\n‚Ä¶ both the build- and release pipeline can be used like so:\n\n- stage: Build\n  jobs:\n  - job: BuildWheel\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: 3.10\n\n    - script: |\n        pip install .[build]\n      displayName: 'Install dependencies'\n\n    - template: templates/build-wheel.yml\n\n\n- stage: Publish\n  jobs:\n  - job: PublishWheel\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\n        repositoryName: 'devops-pipelines-blogpost'\n\n\nAnd here we have another new feature coming in. Stages. These allow us to execute pipelines that depend on each other. We have now split up our pipeline into 2 stages:\n\n 1. Build stage\n 2. Publish stage\n\nUsing stages makes it easy to see what is going on. It provides transparency and allows you to easily track the progress of the pipeline. You can also launch stages separately, skipping previous stages, as long as the necessary dependencies are in place. For example, dependencies can include artifacts, which were generated in previous stage.\n\nImproving the release process\n\nSo what is another advantage of this setup? Say that you are releasing your package to two pip registries. Doing that is easy using this setup by creating two jobs in the publish stage:\n\n- stage: Publish\n  jobs:\n  - job: PublishToRegistryOne\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/registry-1'\n        repositoryName: 'devops-pipelines-blogpost'\n\n  - job: PublishToRegistryTwo\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/registry-2'\n        repositoryName: 'devops-pipelines-blogpost'\n\n\nAs you can see, we can use the defined templates to scale our pipelines. What is essential here, is that thanks to using the artifacts, we can build our wheel once and consume that same wheel multiple times.\n\nAdditionally, the publishing jobs launch in parallel by default (unless dependencies are explicitly defined). This speeds up your release process.\n\n\n3. Automate using a strategy matrix\n\nLet‚Äôs go back to the code quality stage for a minute. In the code quality stage, we are first installing a certain Python version, and then running all quality checks. However, we might need guarantees that our code works for multiple Python versions. This is often the case when releasing a package, for example. How can we easily automate running our Code Quality pipeline using our pipeline templates? One option is to manually define a couple jobs and install the correct python version in each job. Another option is to use a strategy matrix. This allows us to define a matrix of variables that we can use in our pipeline.\n\nWe can improve our CodeQualityChecks job like so:\n\njobs:\n- job: CodeQualityChecks\n  strategy:\n    matrix:\n      Python38:\n        python.version: '3.8'\n      Python39:\n        python.version: '3.9'\n      Python310:\n        python.version: '3.10'\n      Python311:\n        python.version: '3.11'\n\n  steps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: $(python.version)\n\n  - script: |\n      pip install .[dev]\n    displayName: 'Install dependencies'\n\n  - template: templates/code-quality.yml\n\n\nAwesome! The pipeline now runs the entire code quality pipeline for each Python version. Looking at how our pipeline runs now, we can see multiple jobs, one for each Python version:\n\n.. as you can see 4 jobs are launched. If no job dependencies are explicitly set, jobs within one stage run in parallel! That greatly speed up the pipeline and lets you iterate faster! That‚Äôs definitely a win.\n\n\nFinal result\n\nLet‚Äôs wrap it up! Our entire pipeline, using templates:\n\ntrigger:\n- main\n\nstages:\n- stage: CodeQuality\n  jobs:\n  - job: CodeQualityChecks\n    strategy:\n      matrix:\n        Python38:\n          python.version: '3.8'\n        Python39:\n          python.version: '3.9'\n        Python310:\n          python.version: '3.10'\n        Python311:\n          python.version: '3.11'\n\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: $(python.version)\n\n    - script: |\n        pip install .[dev]\n      displayName: 'Install dependencies'\n\n    - template: templates/code-quality.yml\n\n\n- stage: Build\n  jobs:\n  - job: BuildWheel\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: 3.10\n\n    - script: |\n        pip install .[build]\n      displayName: 'Install dependencies'\n\n    - template: templates/build-wheel.yml\n\n\n- stage: Publish\n  jobs:\n  - job: PublishWheel\n    steps:\n    - script: |\n        pip install twine==4.0.2\n      displayName: 'Install twine'\n\n    - template: templates/publish-wheel.yml\n      parameters:\n        artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\n        repositoryName: 'devops-pipelines-blogpost'\n\n\n‚Ä¶ which uses these templates:\n\n * templates/code-quality.yml\n * templates/build-wheel.yml\n * templates/publish-wheel.yml\n\n‚Ä¶ for the entire source code see the better-devops-pipelines-blogpost repo. The repository contains pipelines that apply above explained principles. The pipelines provide testing, building and releasing for a Python project ‚úì.\n\n\nConclusion\n\nWe demonstrated how to scale up your Azure DevOps CI/CD setup making it reusable, maintainable and modular. This helps you maintain a good CI/CD setup as your company grows.\n\nIn short, we achieved the following:\n\n * Create modular DevOps pipelines using templates. This makes it more easy to reuse pipelines across projects and repositories\n * Pass data between DevOps pipeline jobs using artifacts. This allows us to split up our pipeline into smaller pieces, that can consume artifacts from previous jobs.\n * Split up your pipeline in stages to create more visibility and control over your CI/CD\n\nAn example repository containing good-practice pipelines is available at:\n\nhttps://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost\n\nCheers üôè","feature_image":"__GHOST_URL__/content/images/2024/01/scaling-up-your-azure-devops-ci-cd-setup-banner.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-01-06 11:02:31","created_by":"1","updated_at":"2025-04-09 13:50:30","updated_by":"1","published_at":"2023-12-08 11:02:00","published_by":"1","custom_excerpt":"Azure DevOps pipelines are a great way to automate your CI/CD process. But what if you have many projects? In this blog post, we will show you how you can scale up your Azure DevOps CI/CD setup for reusability and easy maintenance.","codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; URL=https://xebia.com/blog/scaling-up-your-azure-devops-ci-cd-setup/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Introduction\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Azure DevOps pipelines are a great way to automate your CI/CD process. Most often, they are configured on a per project basis. This works fine when you have few projects. But what if you have many projects? In this blog post, we will show you how you can scale up your Azure DevOps CI/CD setup for reusability and easy maintenance.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your typical DevOps pipeline\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A typical DevOps pipeline is placed inside the project repository. Let‚Äôs consider a pipeline for a Python project. It includes the following steps:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"quality checks\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" such as code formatting and linting\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"building a package\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" such as a Python wheel\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"releasing a package\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" to Python package registry (such as \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Azure Artifacts\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/artifacts/start-using-azure-artifacts?view=azure-devops&tabs=nugettfs%2Cnuget%2Corgstorage\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" or PyPi)\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Using an Azure DevOps pipeline, we can achieve this like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"trigger:\\n- main\\n\\nsteps:\\n# Python setup & dependencies\\n- task: UsePythonVersion@0\\n  inputs:\\n    versionSpec: 3.10\\n\\n- script: |\\n    pip install .[dev,build,release]\\n  displayName: 'Install dependencies'\\n\\n# Code Quality\\n- script: |\\n    black --check .\\n  displayName: 'Formatting'\\n\\n- script: |\\n    flake8 .\\n  displayName: 'Linting'\\n\\n- script: |\\n    pytest .\\n  displayName: 'Testing'\\n\\n# Build\\n- script: |\\n    echo $(Build.BuildNumber) > version.txt\\n  displayName: 'Set version number'\\n\\n- script: |\\n    pip wheel \\\\\\n      --no-deps \\\\\\n      --wheel-dir dist/ \\\\\\n      .\\n  displayName: 'Build wheel'\\n\\n# Publish\\n- task: TwineAuthenticate@1\\n  inputs:\\n    artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\\n  displayName: 'Authenticate pip with twine'\\n\\n- script: |\\n    twine upload \\\\\\n      --config-file $(PYPIRC_PATH) \\\\\\n      --repository devops-pipelines-blogpost \\\\\\n      dist/*.whl\\n  displayName: 'Publish wheel with twine'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Well, that is great, right? We have achieved all the goals we desired:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Code quality checks using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"black\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"flake8\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pytest\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Build and package the project as a Python \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"wheel\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Publish the package to a registry of choice, in this case \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Azure Artifacts\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Growing pains\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"A DevOps pipeline like the above works fine for a single project. But, ‚Ä¶ what if we want to scale up? Say our company grows, we create more repositories and more projects need to be packaged and released. Will we simply copy this pipeline and paste it into a new repository? Given that we are growing in size, can we be more efficient than just running this pipeline from start to finish?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The answer is no ‚Äì we do not have to copy/paste all these pipelines into a new repo, and the answer is yes ‚Äì we can be more efficient in running these pipelines. Let‚Äôs see how.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Scaling up properly\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs see how we can create scalable DevOps pipelines. First, we are going to introduce \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"DevOps pipeline \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops&pivots=templates-includes\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". These are modular pieces of pipeline that we can reuse across various pipelines and also across various projects residing in different repositories.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs see how we can use pipeline templates to our advantage.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"1. DevOps template setup\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs rewrite pieces of our pipeline into DevOps pipeline templates. Important to know here is that you can write templates for either \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"stages\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"jobs\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" or \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"steps\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". The hierarchy is as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"stages:\\n- stage: Stage1\\n  jobs:\\n  - job: Job1\\n    steps:\\n    - step: Step1\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This can be illustrated in an image like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-stages-jobs-steps-illustration.jpg.webp\",\"alt\":\"Stages jobs steps illustration\",\"width\":2800,\"height\":1186},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can then create a template in one file, for example for \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"steps\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates/code-quality.yml\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"steps:\\n- script: |\\n    echo \\\"Hello world!\\\"\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".. and reuse it in our former pipeline:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"stages:\\n- stage: Stage1\\n  jobs:\\n  - job: Job1\\n    steps:\\n    - template: templates/code-quality.yml\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ or for those who prefer a more visual way of displaying it:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-using-templates-in-your-own-repo.png.webp\",\"alt\":\"Using templates in your own repo illustration\",\"width\":1551,\"height\":608},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That‚Äôs how easy it is to use DevOps pipeline templates! Let‚Äôs now apply it to our own usecase.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Code quality checks template\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h4\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"First, let‚Äôs put the code quality checks pipeline into a template. We are also making the pipeline more extensive so it outputs test results and coverage reports. Remember, we are only defining this template once and then reusing it in other places.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates/code-quality.yml\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"steps:\\n# Code Quality\\n- script: |\\n    black --check .\\n  displayName: 'Formatting'\\n\\n- script: |\\n    flake8 .\\n  displayName: 'Linting'\\n\\n- script: |\\n    pytest \\\\\\n      --junitxml=junit/test-results.xml \\\\\\n      --cov=. \\\\\\n      --cov-report=xml:coverage.xml \\\\\\n      .\\n  displayName: 'Testing'\\n\\n# Publish test results + coverage\\n- task: PublishTestResults@2\\n  condition: succeededOrFailed()\\n  inputs:\\n    testResultsFiles: '**/test-*.xml'\\n    testRunTitle: 'Publish test results'\\n    failTaskOnFailedTests: true\\n  displayName: 'Publish test results'\\n\\n- task: PublishCodeCoverageResults@1\\n  inputs:\\n    codeCoverageTool: 'Cobertura'\\n    summaryFileLocation: '**/coverage.xml'\\n  displayName: 'Publish test coverage'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ which we are using like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"steps:\\n- template: templates/code-quality.yml\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Easy! Also note we included two additional tasks: one to publish the test results and another to publish code coverage reports. That information is super useful to display inside DevOps. Lucky for us, DevOps has support for that:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-checks.png.webp\",\"alt\":\"Code Quality checks display in DevOps\",\"width\":526,\"height\":306},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ clicking on the test results brings us to the \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Tests\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" view, where we can exactly which test failed (if any failed):\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-test-view.png.webp\",\"alt\":\"Code quality test view\",\"width\":2078,\"height\":1658},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Lastly, there‚Äôs also a view explaining which lines of code you covered with tests and which you did not:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-code-quality-coverage-view.jpg.webp\",\"alt\":\"Code quality coverage view\",\"width\":2054,\"height\":1662},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Those come in very useful when you are working on testing your code!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now, we have defined this all in DevOps \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". That gives us a more comfortable position to define more elaborate pipeline steps because we will \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"import\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" those templates instead of copy/pasting them.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That said, we can summarise the benefits of using DevOps templates like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Define once, reuse everywhere\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can reuse this code quality checks pipeline in both the same project multiple times but also from other repositories. If you are importing from another repo, see ‚Äò\",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Use other repositories\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops&pivots=templates-includes#use-other-repositories\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äò for setup.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Make it failproof\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"You can invest into making just this template very good; instead of having multiple bad versions hanging around in your organisation.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Reduce complexity\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Abstracting away commonly used code can be efficient for the readability of your pipeline. This allows newcomers to easily understand the different parts of your CI/CD setup using DevOps pipelines.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"2. Passing data between templates\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs go a step further and also abstract away the build and release steps into templates. We are going to use the following template for \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"building a Python wheel\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"steps:\\n# Build wheel\\n- script: |\\n    echo $(Build.BuildNumber) > version.txt\\n  displayName: 'Set version number'\\n\\n- script: |\\n    pip wheel \\\\\\n      --no-deps \\\\\\n      --wheel-dir dist/ \\\\\\n      .\\n  displayName: 'Build wheel'\\n\\n# Upload wheel as artifact\\n- task: CopyFiles@2\\n  inputs:\\n    contents: dist/**\\n    targetFolder: $(Build.ArtifactStagingDirectory)\\n  displayName: 'Copy wheel to artifacts directory'\\n\\n- publish: '$(Build.ArtifactStagingDirectory)/dist'\\n  artifact: wheelFiles\\n  displayName: 'Upload wheel as artifact'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This definition is slightly different than the one we defined before, in the initial pipeline. This pipeline uses \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"artifacts\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/artifacts/pipeline-artifacts?view=azure-devops&tabs=yaml\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". These allow us to \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pass\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" data between jobs or stages. This is useful when we want to split up our pipeline into smaller pieces. Splitting up the process into smaller segments gives us more visibility and control over the process. Another benefit of this, is that when we split the Python wheel build and release process, is that we give ourselves the ability to \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"release to multiple providers at once\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"When this pipeline is ran we can see an artifact (a wheel file) has been added:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example.png.webp\",\"alt\":\"Build artifact example\",\"width\":506,\"height\":480},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ with the actual wheel file in there:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-build-artifact-example-wheel-file.png.webp\",\"alt\":\"Build artifact wheel file\",\"width\":1756,\"height\":668},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This is also useful so we can inspect what the build pipeline has produced. We can now \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"download\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" this wheel file from the artifacts again. We will do this in the publish pipeline.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"template/publish-wheel.yml\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"parameters:\\n- name: artifactFeed\\n  type: string\\n- name: repositoryName\\n  type: string\\n\\nsteps:\\n# Retrieve wheel\\n- download: current\\n  artifact: wheelFiles\\n  displayName: 'Download artifacts'\\n\\n# Publish wheel\\n- task: TwineAuthenticate@1\\n  inputs:\\n    artifactFeed: ${{ parameters.artifactFeed }}\\n  displayName: 'Authenticate pip with twine'\\n\\n- script: |\\n    twine upload \\\\\\n      --config-file $(PYPIRC_PATH) \\\\\\n      --repository ${{ parameters.repositoryName }} \\\\\\n      $(Pipeline.Workspace)/wheelFiles/*.whl\\n  displayName: 'Publish wheel with twine'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ both the build- and release pipeline can be used like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"- stage: Build\\n  jobs:\\n  - job: BuildWheel\\n    steps:\\n    - task: UsePythonVersion@0\\n      inputs:\\n        versionSpec: 3.10\\n\\n    - script: |\\n        pip install .[build]\\n      displayName: 'Install dependencies'\\n\\n    - template: templates/build-wheel.yml\\n\\n\\n- stage: Publish\\n  jobs:\\n  - job: PublishWheel\\n    steps:\\n    - script: |\\n        pip install twine==4.0.2\\n      displayName: 'Install twine'\\n\\n    - template: templates/publish-wheel.yml\\n      parameters:\\n        artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\\n        repositoryName: 'devops-pipelines-blogpost'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"And here we have another new feature coming in. \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Stages\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/stages?view=azure-devops&tabs=yaml\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". These allow us to execute pipelines that depend on each other. We have now split up our pipeline into 2 stages:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Build stage\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Publish stage\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ol\",\"type\":\"list\",\"listType\":\"number\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Using stages makes it easy to see what is going on. It provides transparency and allows you to easily track the progress of the pipeline. You can also launch stages separately, skipping previous stages, as long as the necessary dependencies are in place. For example, dependencies can include artifacts, which were generated in previous stage.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Improving the release process\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h4\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So what is another advantage of this setup? Say that you are releasing your package to \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"two pip registries\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Doing that is easy using this setup by creating two jobs in the publish stage:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"- stage: Publish\\n  jobs:\\n  - job: PublishToRegistryOne\\n    steps:\\n    - script: |\\n        pip install twine==4.0.2\\n      displayName: 'Install twine'\\n\\n    - template: templates/publish-wheel.yml\\n      parameters:\\n        artifactFeed: 'better-devops-pipelines-blogpost/registry-1'\\n        repositoryName: 'devops-pipelines-blogpost'\\n\\n  - job: PublishToRegistryTwo\\n    steps:\\n    - script: |\\n        pip install twine==4.0.2\\n      displayName: 'Install twine'\\n\\n    - template: templates/publish-wheel.yml\\n      parameters:\\n        artifactFeed: 'better-devops-pipelines-blogpost/registry-2'\\n        repositoryName: 'devops-pipelines-blogpost'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"As you can see, we can use the defined templates to scale our pipelines. What is essential here, is that thanks to using the artifacts, we can build our wheel \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"once\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and consume that same wheel \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"multiple\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" times.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Additionally, the publishing jobs launch \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"in parallel\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" by default (unless dependencies are explicitly defined). This speeds up your release process.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"3. Automate using a strategy matrix\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs go back to the code quality stage for a minute. In the code quality stage, we are first installing a certain Python version, and then running all quality checks. However, we might need guarantees that our code works for multiple Python versions. This is often the case when releasing a package, for example. How can we easily automate running our Code Quality pipeline using our pipeline \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"? One option is to manually define a couple jobs and install the correct python version in each job. Another option is to use a \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"strategy matrix\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/jobs-job-strategy?view=azure-pipelines\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". This allows us to define a matrix of variables that we can use in our pipeline.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can improve our \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"CodeQualityChecks\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" job like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"jobs:\\n- job: CodeQualityChecks\\n  strategy:\\n    matrix:\\n      Python38:\\n        python.version: '3.8'\\n      Python39:\\n        python.version: '3.9'\\n      Python310:\\n        python.version: '3.10'\\n      Python311:\\n        python.version: '3.11'\\n\\n  steps:\\n  - task: UsePythonVersion@0\\n    inputs:\\n      versionSpec: $(python.version)\\n\\n  - script: |\\n      pip install .[dev]\\n    displayName: 'Install dependencies'\\n\\n  - template: templates/code-quality.yml\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Awesome! The pipeline now runs the entire code quality pipeline for each Python version. Looking at how our pipeline runs now, we can see multiple jobs, one for each Python version:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"__GHOST_URL__/content/images/2024/04/scaling-up-your-azure-devops-ci-cd-setup-strategy-matrix-example.png.webp\",\"alt\":\"Strategy matrix jobs example in parallel\",\"width\":1574,\"height\":866},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".. as you can see 4 jobs are launched. If no job dependencies are explicitly set, jobs within one stage run in parallel! That greatly speed up the pipeline and lets you iterate faster! That‚Äôs definitely a win.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Final result\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs wrap it up! Our entire pipeline, using \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"codeblock\",\"code\":\"trigger:\\n- main\\n\\nstages:\\n- stage: CodeQuality\\n  jobs:\\n  - job: CodeQualityChecks\\n    strategy:\\n      matrix:\\n        Python38:\\n          python.version: '3.8'\\n        Python39:\\n          python.version: '3.9'\\n        Python310:\\n          python.version: '3.10'\\n        Python311:\\n          python.version: '3.11'\\n\\n    steps:\\n    - task: UsePythonVersion@0\\n      inputs:\\n        versionSpec: $(python.version)\\n\\n    - script: |\\n        pip install .[dev]\\n      displayName: 'Install dependencies'\\n\\n    - template: templates/code-quality.yml\\n\\n\\n- stage: Build\\n  jobs:\\n  - job: BuildWheel\\n    steps:\\n    - task: UsePythonVersion@0\\n      inputs:\\n        versionSpec: 3.10\\n\\n    - script: |\\n        pip install .[build]\\n      displayName: 'Install dependencies'\\n\\n    - template: templates/build-wheel.yml\\n\\n\\n- stage: Publish\\n  jobs:\\n  - job: PublishWheel\\n    steps:\\n    - script: |\\n        pip install twine==4.0.2\\n      displayName: 'Install twine'\\n\\n    - template: templates/publish-wheel.yml\\n      parameters:\\n        artifactFeed: 'better-devops-pipelines-blogpost/devops-pipelines-blogpost'\\n        repositoryName: 'devops-pipelines-blogpost'\\n\",\"language\":\"yaml\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ which uses these templates:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates/code-quality.yml\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost?path=/02_better_pipeline/templates/code-quality.yml\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates/build-wheel.yml\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost?path=/02_better_pipeline/templates/build-wheel.yml\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates/publish-wheel.yml\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost?path=/02_better_pipeline/templates/publish-wheel.yml\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ for the entire source code see the \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"better-devops-pipelines-blogpost\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" repo. The repository contains pipelines that apply above explained principles. The pipelines provide testing, building and releasing for a Python project ‚úì.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Conclusion\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We demonstrated how to scale up your Azure DevOps CI/CD setup making it reusable, maintainable and modular. This helps you maintain a good CI/CD setup as your company grows.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In short, we achieved the following:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Create modular DevOps pipelines using \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"templates\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops&pivots=templates-includes\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". This makes it more easy to reuse pipelines across projects and repositories\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Pass data between DevOps pipeline jobs using \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"artifacts\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/artifacts/pipeline-artifacts?view=azure-devops&tabs=yaml\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". This allows us to split up our pipeline into smaller pieces, that can consume artifacts from previous jobs.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Split up your pipeline in \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"stages\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://learn.microsoft.com/en-us/azure/devops/pipelines/process/stages?view=azure-devops&tabs=yaml\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" to create more visibility and control over your CI/CD\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"An example repository containing good-practice pipelines is available at:\",\"type\":\"text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://dev.azure.com/godatadriven/_git/better-devops-pipelines-blogpost\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"quote\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Cheers üôè\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"65993426cdf9710001822269","uuid":"56d42d68-9169-41c3-bc20-eb30ae4e8874","title":"Dataset enrichment using LLM‚Äôs ‚ú® (on Xebia.com ‚ßâ)","slug":"dataset-enrichment-using-llms","mobiledoc":"{\"version\":\"0.3.1\",\"ghostVersion\":\"4.0\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"65993426cdf9710001822269","plaintext":null,"feature_image":"__GHOST_URL__/content/images/2024/01/dataset-enrichment-using-llms-banner.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-01-06 11:06:14","created_by":"1","updated_at":"2024-01-07 14:58:42","updated_by":"1","published_at":"2023-12-28 11:06:00","published_by":"1","custom_excerpt":"Large Language Models (LLM's) have proven to be useful for numerous tasks. LLM's can summarise text, generate text, classify text or translate text. What LLM's can also be used for is converting unstructured data to structured data.","codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; URL=https://xebia.com/blog/dataset-enrichment-using-llms/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":null,"show_title_and_feature_image":1},{"id":"661bb31631b0250001990697","uuid":"42b2e863-e3d7-483f-8a85-d0aee3029bb3","title":"RAG on GCP: production-ready GenAI on Google Cloud Platform (on Xebia.com ‚ßâ)","slug":"rag-on-gcp-production-ready-genai-on-google-cloud-platform","mobiledoc":null,"html":"<h2 id=\"introduction\">Introduction</h2><p>Google Cloud Platform has an increasing set of managed services that can help you build production-ready <em>Retrieval-Augmented Generation</em> applications. Services like <em>Vertex AI Search &amp; Conversation</em> and <em>Vertex AI Vector Search</em> give us scalability and ease of use. How can you best leverage them to build RAG applications? Let‚Äôs explore together. Read along!</p><h2 id=\"retrieval-augmented-generation\"><strong>Retrieval Augmented Generation</strong></h2><p>Even though <em>Retrieval-Augmented Generation</em> (<strong>RAG</strong>) was coined already in 2020, the technique got supercharged since the rise of <em>Large Language Models</em> (<strong>LLMs</strong>). With RAG, LLMs are combined with search techniques like vector search to enable realtime and efficient lookup of information that is outside the model‚Äôs knowledge. This opens up many exciting new possibilities. Whereas previously interactions with LLMs were limited to the model‚Äôs knowledge, with RAG it is now possible to load in company internal data like knowledge bases. Additionally, by instructing the LLM to always ‚Äòground‚Äô its answer based on factual data, hallucinations can be reduced.</p><h3 id=\"why-rag\"><strong>Why RAG?</strong></h3><p>Let‚Äôs first take a step back. How exactly can RAG benefit us? When we interact with an LLM, all its factual knowledge is stored inside the model weights. The model weights are set during its training phase, which can be a while ago. In fact, this can be more than a year.</p><!--kg-card-begin: html--><table class=\"fancy-table\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse; width: 416px; margin-left: auto; margin-right: auto; color: rgb(53, 68, 90); font-family: Proxima, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><thead style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><th style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left; background-color: rgb(242, 242, 242);\">LLM</th><th style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left; background-color: rgb(242, 242, 242);\">Knowledge cut-off</th></tr></thead><tbody style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\">Gemini 1.0 Pro</td><td style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\">Early 2023<span>&nbsp;</span><a href=\"https://ai.google.dev/models/gemini\" style=\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\">[1]</a></td></tr><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\">GPT-3.5 turbo</td><td style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\">September 2021<span>&nbsp;</span><a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35-turbo-model-availability\" style=\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\">[2]</a></td></tr><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\">GPT-4</td><td style=\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\">September 2021<span>&nbsp;</span><a href=\"https://openai.com/research/gpt-4\" style=\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\">[3]</a></td></tr></tbody></table><span style=\"text-align:center;color:#ccc;\"><small>Knowledge cut-offs as of March 2024.</small></span><!--kg-card-end: html--><p>Additionally, these publically offered models are trained on mostly <em>public</em> data. If you want to use company internal data, an option is to fine-tune or retrain the model, which can be expensive and time-consuming.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide1.png.webp\" class=\"kg-image\" alt=\"The limitations of LLM interactions without using RAG\" loading=\"lazy\"><figcaption>The limitations of LLM interactions without using RAG.</figcaption></figure><p>This boils down to three main limitations: the models knowledge is outdated, the model has no access to internal data, and the model can hallucinate answers.</p><p>With RAG we can circumvent these limitations. Given the question a user has, information relevant to that question can be retrieved first to then be presented to the LLM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide2.png.webp\" class=\"kg-image\" alt=\"How RAG can help an LLM provide more factual answers based on internal data\" loading=\"lazy\"><figcaption>How RAG can help an LLM provide more factual answers based on internal data.</figcaption></figure><p>The LLM can then augment its answer with the retrieved information to generate a factual, up-to-date and yet human-readable answer. The LLM is to be instructed to at all times ground its answer in the retrieved information, which can help reduce hallucinations.</p><p>These benefits are great. So how do we actually build a RAG system?</p><h2 id=\"building-a-rag-system\"><strong>Building a RAG system</strong></h2><p>In a RAG system, there are two main steps: 1) <em>Document retrieval</em> and 2) <em>Answer generation</em>. Whereas the document retrieval is responsible for finding the most relevant information given the user‚Äôs question, the answer generation is responsible for generating a human-readable answer based on information found in the retrieval step. Let‚Äôs take a look at both in more detail.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide3.png.webp\" class=\"kg-image\" alt=\"The two main steps in a RAG system: Document retrieval and Answer generation\" loading=\"lazy\"><figcaption>The two main steps in a RAG system: Document retrieval and Answer generation.</figcaption></figure><h3 id=\"document-retrieval\"><strong>Document retrieval</strong></h3><p>First, <strong>Document retrieval</strong>. Documents are converted to plain text and <a href=\"https://python.langchain.com/docs/modules/data_connection/document_transformers/\">chunked</a>. The chunks are then embedded and stored in a vector database. User questions are also embedded, enabling a <em>vector similarity search</em> to obtain the best matching documents. Optionally, a step can be added to extract document metadata like title, author, summary, keywords, etc, which can subsequently be used to perform a <em>keyword</em> search. This can all be illustrated like so:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/03/rag-on-gcp-Slide4.jpg.webp\" class=\"kg-image\" alt=\"Document retrieval step in a RAG system. Documents are converted to text and converted to embeddings. A user's question is converted to an embedding such that a vector similarity search can be performed.\" loading=\"lazy\"><figcaption>Document retrieval step in a RAG system. Documents are converted to text and converted to embeddings. A user‚Äôs question is converted to an embedding such that a vector similarity search can be performed.</figcaption></figure><p>Neat. But what about GCP. We can map the former to GCP as follows:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide5.png.webp\" class=\"kg-image\" alt=\"Document retrieval using GCP services including Document AI, <code>textembedding-gecko</code> and Vertex AI Vector Search.\" loading=\"lazy\"><figcaption>Document retrieval using GCP services including Document AI, <code style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace; font-size: 1em;\">textembedding-gecko</code> and Vertex AI Vector Search.</figcaption></figure><p><em>Document AI</em> is used to process documents and extract text, <em>Gemini</em> and <code>textembedding-gecko</code> to generate metadata and embeddings respectively and <em>Vertex AI Vector Search</em> is used to store the embeddings and perform similarity search. By using these services, we can build a scalable retrieval step.</p><h3 id=\"answer-generation\"><strong>Answer generation</strong></h3><p>Then, <strong>Answer generation</strong>. We will need an LLM for this and instruct it to use the provided documents. We can illustrate this like so:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide6.png.webp\" class=\"kg-image\" alt=\"Answer generation step using Gemini, with an example prompt. Both the user's question and snippets of documents relevant to that question are inserted in the prompt.\" loading=\"lazy\"><figcaption>Answer generation step using Gemini, with an example prompt. Both the user‚Äôs question and snippets of documents relevant to that question are inserted in the prompt.</figcaption></figure><p>Here, the documents can be formatted using an arbitrary function that generates valid markdown.</p><p>We have already come across multiple GCP services that can help us build a RAG system. So now, what other offerings does GCP have to help us build a RAG system and what flavours are there to combine services?</p><h2 id=\"the-rag-flavours-on-gcp\"><strong>The RAG flavours on GCP</strong></h2><p>So far, we have seen GCP services that can help us build a RAG system. These include <em>Document AI</em>, <em>Vertex AI Vector Search</em>, <em>Gemini Pro</em>, <em>Cloud Storage</em> and <em>Cloud Run</em>. But GCP also has Vertex AI Search &amp; Conversation.</p><p><strong>Vertex AI Search &amp; Conversation</strong> is a service tailored to GenAI usecases, built to do some of the heavy lifting for us. It can ingest documents, create embeddings and manage the vector database. You just have to focus on ingesting data in the correct format. Then, you can use Search &amp; Conversation in multiple ways. You can either get only <em>search results</em>, given a search query, or you can let Search &amp; Conversation generate a full answer for you with source citations.</p><p>Even though Vertex AI Search &amp; Conversation is very powerful, there can be scenarios when you want more control. Let‚Äôs take a look on these levels of going either <em>managed</em> or remaining in-control.</p><p>The easiest way to get started with RAG on GCP is to use <strong>Search &amp; Conversation</strong>. The service can ingest documents from multiple sources like <em>Big Query</em> and <em>Google Cloud Storage</em>. Once those are ingested, it can generate answers backed by citations for you. This is simply illustrated like so:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide7.png.webp\" class=\"kg-image\" alt=\"Fully managed RAG using Search &amp; Conversation for document retrieval and answer generation.\" loading=\"lazy\"><figcaption>Fully managed RAG using Search &amp; Conversation for document retrieval and answer generation.</figcaption></figure><p>If you want more control, you can use <strong>Gemini</strong> for answer generation instead of letting Search &amp; Conversation do it for us. This way, you can have more control to do any prompt engineering you like.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide8.png.webp\" class=\"kg-image\" alt=\"Partly managed RAG using Search &amp; Conversation for document retrieval and Gemini for answer generation.\" loading=\"lazy\"><figcaption>Partly managed RAG using Search &amp; Conversation for document retrieval and Gemini for answer generation.</figcaption></figure><p>Lastly, you can have full control over the RAG system. This means you have to manage both the document retrieval and answer generation yourself. This does mean more manual work in engineering the system. Documents can be processed by <em>Document AI</em>, chunked, embedded and its vectors stored in <em>Vertex AI Vector Search</em>. Then <em>Gemini</em> can be used to generate the final answers.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide9.png.webp\" class=\"kg-image\" alt=\"Full control RAG. Manual document processing, embedding creation and vector database management.\" loading=\"lazy\"><figcaption>Full control RAG. Manual document processing, embedding creation and vector database management.</figcaption></figure><p>The advantage here is that you are able to fully control yourself <em>how</em> you process the documents an convert them to embeddings. You can use all of Document AI‚Äôs <a href=\"https://cloud.google.com/document-ai/docs/processors-list\">Processors offering</a> to process the documents in different ways.</p><p>Do take the the managed versus manual approach tradeoffs into consideration. Ask yourself questions like:</p><ul><li>How much time and energy do you want to invest building something custom for the flexibility that you need?</li><li>Do you really need the flexibility for your solution to give in to extra maintenance cost?</li><li>Do you have the engineering capacity to build- and maintain a custom solution?</li><li>Are the initially invested build costs worth the money saved in not using a managed solution?</li></ul><p>So then, you can decide what works best for you ‚úì.</p><h2 id=\"concluding\"><strong>Concluding</strong></h2><p>RAG is a powerful way to augment LLMs with external data. This can help reduce hallucinations and provide more factual answers. At the core of RAG systems are document processors, vector databases and LLM‚Äôs.</p><p>Google Cloud Platform offers services that can help build production-ready RAG solutions. We have described three levels of control in deploying a RAG application on GCP:</p><ul><li><strong>Fully managed</strong>: using Search &amp; Conversation.</li><li><strong>Partly managed</strong>: managed search using Search &amp; Conversation but manual prompt-engineering using Gemini.</li><li><strong>Full control</strong>: manual document processing using Document AI, embedding creation and vector database management using Vertex AI Vector Search.</li></ul><p>That said, I wish you good luck implementing your own RAG system. Use RAG for great good! ‚ô°</p>","comment_id":"661bb31631b0250001990697","plaintext":"Introduction\n\nGoogle Cloud Platform has an increasing set of managed services that can help you build production-ready Retrieval-Augmented Generation applications. Services like Vertex AI Search & Conversation and Vertex AI Vector Search give us scalability and ease of use. How can you best leverage them to build RAG applications? Let‚Äôs explore together. Read along!\n\n\nRetrieval Augmented Generation\n\nEven though Retrieval-Augmented Generation (RAG) was coined already in 2020, the technique got supercharged since the rise of Large Language Models (LLMs). With RAG, LLMs are combined with search techniques like vector search to enable realtime and efficient lookup of information that is outside the model‚Äôs knowledge. This opens up many exciting new possibilities. Whereas previously interactions with LLMs were limited to the model‚Äôs knowledge, with RAG it is now possible to load in company internal data like knowledge bases. Additionally, by instructing the LLM to always ‚Äòground‚Äô its answer based on factual data, hallucinations can be reduced.\n\n\nWhy RAG?\n\nLet‚Äôs first take a step back. How exactly can RAG benefit us? When we interact with an LLM, all its factual knowledge is stored inside the model weights. The model weights are set during its training phase, which can be a while ago. In fact, this can be more than a year.\n\nLLMKnowledge cut-offGemini 1.0 ProEarly 2023¬†[1]GPT-3.5 turboSeptember 2021¬†[2]GPT-4September 2021¬†[3]\n\nKnowledge cut-offs as of March 2024.\n\nAdditionally, these publically offered models are trained on mostly public data. If you want to use company internal data, an option is to fine-tune or retrain the model, which can be expensive and time-consuming.\n\nThis boils down to three main limitations: the models knowledge is outdated, the model has no access to internal data, and the model can hallucinate answers.\n\nWith RAG we can circumvent these limitations. Given the question a user has, information relevant to that question can be retrieved first to then be presented to the LLM.\n\nThe LLM can then augment its answer with the retrieved information to generate a factual, up-to-date and yet human-readable answer. The LLM is to be instructed to at all times ground its answer in the retrieved information, which can help reduce hallucinations.\n\nThese benefits are great. So how do we actually build a RAG system?\n\n\nBuilding a RAG system\n\nIn a RAG system, there are two main steps: 1) Document retrieval and 2) Answer generation. Whereas the document retrieval is responsible for finding the most relevant information given the user‚Äôs question, the answer generation is responsible for generating a human-readable answer based on information found in the retrieval step. Let‚Äôs take a look at both in more detail.\n\n\nDocument retrieval\n\nFirst, Document retrieval. Documents are converted to plain text and chunked. The chunks are then embedded and stored in a vector database. User questions are also embedded, enabling a vector similarity search to obtain the best matching documents. Optionally, a step can be added to extract document metadata like title, author, summary, keywords, etc, which can subsequently be used to perform a keyword search. This can all be illustrated like so:\n\nNeat. But what about GCP. We can map the former to GCP as follows:\n\nDocument AI is used to process documents and extract text, Gemini and textembedding-gecko to generate metadata and embeddings respectively and Vertex AI Vector Search is used to store the embeddings and perform similarity search. By using these services, we can build a scalable retrieval step.\n\n\nAnswer generation\n\nThen, Answer generation. We will need an LLM for this and instruct it to use the provided documents. We can illustrate this like so:\n\nHere, the documents can be formatted using an arbitrary function that generates valid markdown.\n\nWe have already come across multiple GCP services that can help us build a RAG system. So now, what other offerings does GCP have to help us build a RAG system and what flavours are there to combine services?\n\n\nThe RAG flavours on GCP\n\nSo far, we have seen GCP services that can help us build a RAG system. These include Document AI, Vertex AI Vector Search, Gemini Pro, Cloud Storage and Cloud Run. But GCP also has Vertex AI Search & Conversation.\n\nVertex AI Search & Conversation is a service tailored to GenAI usecases, built to do some of the heavy lifting for us. It can ingest documents, create embeddings and manage the vector database. You just have to focus on ingesting data in the correct format. Then, you can use Search & Conversation in multiple ways. You can either get only search results, given a search query, or you can let Search & Conversation generate a full answer for you with source citations.\n\nEven though Vertex AI Search & Conversation is very powerful, there can be scenarios when you want more control. Let‚Äôs take a look on these levels of going either managed or remaining in-control.\n\nThe easiest way to get started with RAG on GCP is to use Search & Conversation. The service can ingest documents from multiple sources like Big Query and Google Cloud Storage. Once those are ingested, it can generate answers backed by citations for you. This is simply illustrated like so:\n\nIf you want more control, you can use Gemini for answer generation instead of letting Search & Conversation do it for us. This way, you can have more control to do any prompt engineering you like.\n\nLastly, you can have full control over the RAG system. This means you have to manage both the document retrieval and answer generation yourself. This does mean more manual work in engineering the system. Documents can be processed by Document AI, chunked, embedded and its vectors stored in Vertex AI Vector Search. Then Gemini can be used to generate the final answers.\n\nThe advantage here is that you are able to fully control yourself how you process the documents an convert them to embeddings. You can use all of Document AI‚Äôs Processors offering to process the documents in different ways.\n\nDo take the the managed versus manual approach tradeoffs into consideration. Ask yourself questions like:\n\n * How much time and energy do you want to invest building something custom for the flexibility that you need?\n * Do you really need the flexibility for your solution to give in to extra maintenance cost?\n * Do you have the engineering capacity to build- and maintain a custom solution?\n * Are the initially invested build costs worth the money saved in not using a managed solution?\n\nSo then, you can decide what works best for you ‚úì.\n\n\nConcluding\n\nRAG is a powerful way to augment LLMs with external data. This can help reduce hallucinations and provide more factual answers. At the core of RAG systems are document processors, vector databases and LLM‚Äôs.\n\nGoogle Cloud Platform offers services that can help build production-ready RAG solutions. We have described three levels of control in deploying a RAG application on GCP:\n\n * Fully managed: using Search & Conversation.\n * Partly managed: managed search using Search & Conversation but manual prompt-engineering using Gemini.\n * Full control: manual document processing using Document AI, embedding creation and vector database management using Vertex AI Vector Search.\n\nThat said, I wish you good luck implementing your own RAG system. Use RAG for great good! ‚ô°","feature_image":"__GHOST_URL__/content/images/2024/04/pexels-ilo-frey-2317904--1-.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-04-14 10:42:30","created_by":"1","updated_at":"2024-11-22 13:36:48","updated_by":"1","published_at":"2024-03-26 11:47:00","published_by":"1","custom_excerpt":"GCP has an increasing set of managed services that can help you build production-ready Retrieval Augmented Generation (RAG) applications. How can you best leverage them to build RAG applications? Let‚Äôs explore together. Read along!","codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; URL=https://xebia.com/blog/rag-on-gcp/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Introduction\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Google Cloud Platform has an increasing set of managed services that can help you build production-ready \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Retrieval-Augmented Generation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" applications. Services like \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vertex AI Search & Conversation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vertex AI Vector Search\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" give us scalability and ease of use. How can you best leverage them to build RAG applications? Let‚Äôs explore together. Read along!\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Retrieval Augmented Generation\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Even though \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Retrieval-Augmented Generation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\") was coined already in 2020, the technique got supercharged since the rise of \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Large Language Models\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" (\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LLMs\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"). With RAG, LLMs are combined with search techniques like vector search to enable realtime and efficient lookup of information that is outside the model‚Äôs knowledge. This opens up many exciting new possibilities. Whereas previously interactions with LLMs were limited to the model‚Äôs knowledge, with RAG it is now possible to load in company internal data like knowledge bases. Additionally, by instructing the LLM to always ‚Äòground‚Äô its answer based on factual data, hallucinations can be reduced.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Why RAG?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs first take a step back. How exactly can RAG benefit us? When we interact with an LLM, all its factual knowledge is stored inside the model weights. The model weights are set during its training phase, which can be a while ago. In fact, this can be more than a year.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"html\":\"<table class=\\\"fancy-table\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse; width: 416px; margin-left: auto; margin-right: auto; color: rgb(53, 68, 90); font-family: Proxima, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\\\"><thead style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><th style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left; background-color: rgb(242, 242, 242);\\\">LLM</th><th style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left; background-color: rgb(242, 242, 242);\\\">Knowledge cut-off</th></tr></thead><tbody style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\\\">Gemini 1.0 Pro</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\\\">Early 2023<span>&nbsp;</span><a href=\\\"https://ai.google.dev/models/gemini\\\" style=\\\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\\\">[1]</a></td></tr><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\\\">GPT-3.5 turbo</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\\\">September 2021<span>&nbsp;</span><a href=\\\"https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35-turbo-model-availability\\\" style=\\\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\\\">[2]</a></td></tr><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\\\">GPT-4</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(221, 221, 221); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 8px; text-align: left;\\\">September 2021<span>&nbsp;</span><a href=\\\"https://openai.com/research/gpt-4\\\" style=\\\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\\\">[3]</a></td></tr></tbody></table><span style=\\\"text-align:center;color:#ccc;\\\"><small>Knowledge cut-offs as of March 2024.</small></span>\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Additionally, these publically offered models are trained on mostly \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"public\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" data. If you want to use company internal data, an option is to fine-tune or retrain the model, which can be expensive and time-consuming.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide1.png.webp\",\"alt\":\"The limitations of LLM interactions without using RAG\",\"caption\":\"The limitations of LLM interactions without using RAG.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This boils down to three main limitations: the models knowledge is outdated, the model has no access to internal data, and the model can hallucinate answers.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With RAG we can circumvent these limitations. Given the question a user has, information relevant to that question can be retrieved first to then be presented to the LLM.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide2.png.webp\",\"alt\":\"How RAG can help an LLM provide more factual answers based on internal data\",\"caption\":\"How RAG can help an LLM provide more factual answers based on internal data.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The LLM can then augment its answer with the retrieved information to generate a factual, up-to-date and yet human-readable answer. The LLM is to be instructed to at all times ground its answer in the retrieved information, which can help reduce hallucinations.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"These benefits are great. So how do we actually build a RAG system?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Building a RAG system\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In a RAG system, there are two main steps: 1) \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and 2) \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Answer generation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Whereas the document retrieval is responsible for finding the most relevant information given the user‚Äôs question, the answer generation is responsible for generating a human-readable answer based on information found in the retrieval step. Let‚Äôs take a look at both in more detail.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide3.png.webp\",\"alt\":\"The two main steps in a RAG system: Document retrieval and Answer generation\",\"caption\":\"The two main steps in a RAG system: Document retrieval and Answer generation.\"},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"First, \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Documents are converted to plain text and \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"chunked\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://python.langchain.com/docs/modules/data_connection/document_transformers/\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". The chunks are then embedded and stored in a vector database. User questions are also embedded, enabling a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"vector similarity search\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" to obtain the best matching documents. Optionally, a step can be added to extract document metadata like title, author, summary, keywords, etc, which can subsequently be used to perform a \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"keyword\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" search. This can all be illustrated like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/03/rag-on-gcp-Slide4.jpg.webp\",\"alt\":\"Document retrieval step in a RAG system. Documents are converted to text and converted to embeddings. A user's question is converted to an embedding such that a vector similarity search can be performed.\",\"caption\":\"Document retrieval step in a RAG system. Documents are converted to text and converted to embeddings. A user‚Äôs question is converted to an embedding such that a vector similarity search can be performed.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Neat. But what about GCP. We can map the former to GCP as follows:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide5.png.webp\",\"alt\":\"Document retrieval using GCP services including Document AI, <code>textembedding-gecko</code> and Vertex AI Vector Search.\",\"caption\":\"Document retrieval using GCP services including Document AI, <code style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / 0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace; font-size: 1em;\\\">textembedding-gecko</code> and Vertex AI Vector Search.\"},{\"children\":[{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document AI\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" is used to process documents and extract text, \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gemini\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"textembedding-gecko\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" to generate metadata and embeddings respectively and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vertex AI Vector Search\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" is used to store the embeddings and perform similarity search. By using these services, we can build a scalable retrieval step.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Answer generation\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h3\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Then, \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Answer generation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". We will need an LLM for this and instruct it to use the provided documents. We can illustrate this like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide6.png.webp\",\"alt\":\"Answer generation step using Gemini, with an example prompt. Both the user's question and snippets of documents relevant to that question are inserted in the prompt.\",\"caption\":\"Answer generation step using Gemini, with an example prompt. Both the user‚Äôs question and snippets of documents relevant to that question are inserted in the prompt.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Here, the documents can be formatted using an arbitrary function that generates valid markdown.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We have already come across multiple GCP services that can help us build a RAG system. So now, what other offerings does GCP have to help us build a RAG system and what flavours are there to combine services?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The RAG flavours on GCP\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So far, we have seen GCP services that can help us build a RAG system. These include \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document AI\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vertex AI Vector Search\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gemini Pro\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Cloud Storage\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Cloud Run\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". But GCP also has Vertex AI Search & Conversation.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vertex AI Search & Conversation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" is a service tailored to GenAI usecases, built to do some of the heavy lifting for us. It can ingest documents, create embeddings and manage the vector database. You just have to focus on ingesting data in the correct format. Then, you can use Search & Conversation in multiple ways. You can either get only \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"search results\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", given a search query, or you can let Search & Conversation generate a full answer for you with source citations.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Even though Vertex AI Search & Conversation is very powerful, there can be scenarios when you want more control. Let‚Äôs take a look on these levels of going either \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"managed\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" or remaining in-control.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The easiest way to get started with RAG on GCP is to use \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Search & Conversation\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". The service can ingest documents from multiple sources like \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Big Query\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" and \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Google Cloud Storage\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Once those are ingested, it can generate answers backed by citations for you. This is simply illustrated like so:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide7.png.webp\",\"alt\":\"Fully managed RAG using Search & Conversation for document retrieval and answer generation.\",\"caption\":\"Fully managed RAG using Search &amp; Conversation for document retrieval and answer generation.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you want more control, you can use \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gemini\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" for answer generation instead of letting Search & Conversation do it for us. This way, you can have more control to do any prompt engineering you like.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide8.png.webp\",\"alt\":\"Partly managed RAG using Search & Conversation for document retrieval and Gemini for answer generation.\",\"caption\":\"Partly managed RAG using Search &amp; Conversation for document retrieval and Gemini for answer generation.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Lastly, you can have full control over the RAG system. This means you have to manage both the document retrieval and answer generation yourself. This does mean more manual work in engineering the system. Documents can be processed by \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document AI\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", chunked, embedded and its vectors stored in \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vertex AI Vector Search\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Then \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gemini\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" can be used to generate the final answers.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"src\":\"https://xebia.com/wp-content/uploads/2024/04/rag-on-gcp-Slide9.png.webp\",\"alt\":\"Full control RAG. Manual document processing, embedding creation and vector database management.\",\"caption\":\"Full control RAG. Manual document processing, embedding creation and vector database management.\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The advantage here is that you are able to fully control yourself \",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"how\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" you process the documents an convert them to embeddings. You can use all of Document AI‚Äôs \",\"type\":\"text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Processors offering\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://cloud.google.com/document-ai/docs/processors-list\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" to process the documents in different ways.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Do take the the managed versus manual approach tradeoffs into consideration. Ask yourself questions like:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How much time and energy do you want to invest building something custom for the flexibility that you need?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Do you really need the flexibility for your solution to give in to extra maintenance cost?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Do you have the engineering capacity to build- and maintain a custom solution?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Are the initially invested build costs worth the money saved in not using a managed solution?\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":4,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So then, you can decide what works best for you ‚úì.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Concluding\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"heading\",\"tag\":\"h2\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG is a powerful way to augment LLMs with external data. This can help reduce hallucinations and provide more factual answers. At the core of RAG systems are document processors, vector databases and LLM‚Äôs.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Google Cloud Platform offers services that can help build production-ready RAG solutions. We have described three levels of control in deploying a RAG application on GCP:\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Fully managed\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": using Search & Conversation.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Partly managed\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": managed search using Search & Conversation but manual prompt-engineering using Gemini.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":2,\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Full control\",\"type\":\"text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": manual document processing using Document AI, embedding creation and vector database management using Vertex AI Vector Search.\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"value\":3,\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"tag\":\"ul\",\"type\":\"list\",\"listType\":\"bullet\",\"start\":1,\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That said, I wish you good luck implementing your own RAG system. Use RAG for great good! ‚ô°\",\"type\":\"text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"type\":\"linebreak\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"66f6ae120aba3300014c3c45","uuid":"ff4b86d6-fc01-4102-a8f9-9444e0ce68c0","title":"Paying attention.","slug":"paying-attention","mobiledoc":null,"html":"<p></p><p>Do you ever <strong>slow down</strong>...?</p><figure class=\"kg-card kg-video-card kg-width-wide kg-card-hascaption\" data-kg-thumbnail=\"http://localhost:2368/content/media/2024/11/Hamburg-timelapse_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"__GHOST_URL__/content/media/2024/11/Hamburg-timelapse.mp4\" poster=\"https://img.spacergif.org/v1/1280x720/0a/spacer.png\" width=\"1280\" height=\"720\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/media/2024/11/Hamburg-timelapse_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:27</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Sunrise in Hamburg, 2022.</span></p></figcaption>\n        </figure><p>I wake up by the buzzing sound of my alarm. That annoying sound. I had set the alarm just to be sure, in case I wasn't awake already. Well, I was <u>not</u> awake already.  I had sacrificed my morning mindfulness for some extra sleep. I now have to be quick. Shower, get dressed, eat breakfast, drink tea ‚Äì fast, brush my teeth and go to work. I jump on my bike and make my way to work. My sole focus is to get me and my bike across from A to B as fast as possible. Evading pedestrians, cars, other cyclists. Brake as little as possible. Squeeze my way through an orange traffic light, or perhaps one or two coloured darker orange. My focus is narrow and the landscape slides all past me. I'm biking through the city but I'm not there. My head is not there. My head is busy preparing the day ahead of me. The meeting in half an hour. The presentation I am to give in the afternoon. The deadline next week. The colleague that I need to address.<br>I wasn't quite there.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://images.unsplash.com/photo-1604987306200-c6212468ebcd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHdhaXQlMjBwZWRlc3RyaWFufGVufDB8fHx8MTczMjI4NTk4N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000\" class=\"kg-image\" alt=\"black and yellow street lights\" loading=\"lazy\" width=\"6240\" height=\"4160\" srcset=\"https://images.unsplash.com/photo-1604987306200-c6212468ebcd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHdhaXQlMjBwZWRlc3RyaWFufGVufDB8fHx8MTczMjI4NTk4N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1604987306200-c6212468ebcd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHdhaXQlMjBwZWRlc3RyaWFufGVufDB8fHx8MTczMjI4NTk4N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1604987306200-c6212468ebcd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHdhaXQlMjBwZWRlc3RyaWFufGVufDB8fHx8MTczMjI4NTk4N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1604987306200-c6212468ebcd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHdhaXQlMjBwZWRlc3RyaWFufGVufDB8fHx8MTczMjI4NTk4N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2400 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Photo by </span><a href=\"https://unsplash.com/@grievek1610begur\"><span style=\"white-space: pre-wrap;\">Kevin Grieve</span></a><span style=\"white-space: pre-wrap;\"> / </span><a href=\"https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit\"><span style=\"white-space: pre-wrap;\">Unsplash</span></a></figcaption></figure><p>Later, I finished work. I made my way home down the same road I had came here. But this time, I saw more. I saw a construction worker, impressively operating a huge machine towering many stories above the ground. Lots of people, working at the same time, collaborating to accomplish one task.</p><p></p><p>How refreshing to take the time <strong>looking</strong>. Taking some time appreciating things and slowing yourself down.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/11/IMG_7076.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1500\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMG_7076.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMG_7076.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMG_7076.jpeg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMG_7076.jpeg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><p>The funny thing is, I had reminded myself so often already. Slow down. Prioritise. Eliminate distractions. Take joy in the small things. Pay attention to details but don't go for perfectionist. 80% is enough.<br>I had read all these books telling me these things. Books like Digital Minimalism, Deep Work, Stolen Focus, Four Thousand Weeks, Hyper Focus, The 4-Hour Work Week. You can say a big part of reading productivity books is feeling productive when you read them. Actually following their advice is something different altogether.</p><p></p><p><sub>--Isn't the world only so busy as you make it? So much as you follow the world's tendency to go fast?--</sub></p><p>Do you ever listen </p><p></p><p>Do you </p><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/ING-Netherlands-2-1.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">‚òïÔ∏è</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">20.844333333333335</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><p>dd</p><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/Jacob-Bontiusplaats.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">üßº</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">1.747</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><p>dd</p><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/New-Recording-6.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">üö´</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">1.2146666666666666</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><p>dd</p><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/Printer-.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">Printer </div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">31.486041666666665</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><p></p>\n<!--kg-card-begin: html-->\n<audio controls src=\"__GHOST_URL__/content/media/2024/09/Jacob-Bontiusplaats.m4a\" preload=\"metadata\"></audio>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/Slack-message.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">Slack message</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">1.696</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/Standing-desk.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">Standing desk</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">8.343520833333333</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"></path></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2024/09/Quocker.m4a\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">Quocker</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\" aria-label=\"Play audio\"><svg viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\" aria-label=\"Pause audio\"><svg viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">14.496625</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button><button class=\"kg-audio-unmute-icon\" aria-label=\"Unmute\"><svg viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\" aria-label=\"Mute\"><svg viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/09/data-src-image-92ae21e5-64bf-47c3-9403-6445950adba6.png\" class=\"kg-image\" alt=\"Edited image result\" loading=\"lazy\" width=\"1408\" height=\"768\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/09/data-src-image-92ae21e5-64bf-47c3-9403-6445950adba6.png 600w, __GHOST_URL__/content/images/size/w1000/2024/09/data-src-image-92ae21e5-64bf-47c3-9403-6445950adba6.png 1000w, __GHOST_URL__/content/images/2024/09/data-src-image-92ae21e5-64bf-47c3-9403-6445950adba6.png 1408w\" sizes=\"(min-width: 720px) 720px\"></figure>","comment_id":"66f6ae120aba3300014c3c45","plaintext":"Do you ever slow down...?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0:00\n\n/0:27\n\n\n1√ó\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI wake up by the buzzing sound of my alarm. That annoying sound. I had set the alarm just to be sure, in case I wasn't awake already. Well, I was not awake already. I had sacrificed my morning mindfulness for some extra sleep. I now have to be quick. Shower, get dressed, eat breakfast, drink tea ‚Äì fast, brush my teeth and go to work. I jump on my bike and make my way to work. My sole focus is to get me and my bike across from A to B as fast as possible. Evading pedestrians, cars, other cyclists. Brake as little as possible. Squeeze my way through an orange traffic light, or perhaps one or two coloured darker orange. My focus is narrow and the landscape slides all past me. I'm biking through the city but I'm not there. My head is not there. My head is busy preparing the day ahead of me. The meeting in half an hour. The presentation I am to give in the afternoon. The deadline next week. The colleague that I need to address.\nI wasn't quite there.\n\nLater, I finished work. I made my way home down the same road I had came here. But this time, I saw more. I saw a construction worker, impressively operating a huge machine towering many stories above the ground. Lots of people, working at the same time, collaborating to accomplish one task.\n\n\n\nHow refreshing to take the time looking. Taking some time appreciating things and slowing yourself down.\n\n\n\nThe funny thing is, I had reminded myself so often already. Slow down. Prioritise. Eliminate distractions. Take joy in the small things. Pay attention to details but don't go for perfectionist. 80% is enough.\nI had read all these books telling me these things. Books like Digital Minimalism, Deep Work, Stolen Focus, Four Thousand Weeks, Hyper Focus, The 4-Hour Work Week. You can say a big part of reading productivity books is feeling productive when you read them. Actually following their advice is something different altogether.\n\n\n\n--Isn't the world only so busy as you make it? So much as you follow the world's tendency to go fast?--\n\nDo you ever listen\n\n\n\nDo you\n\n‚òïÔ∏è0:00/20.8443333333333351√ó\n\ndd\n\nüßº0:00/1.7471√ó\n\ndd\n\nüö´0:00/1.21466666666666661√ó\n\ndd\n\nPrinter 0:00/31.4860416666666651√ó\n\n\n\n\n\n\n\nSlack message0:00/1.6961√óStanding desk0:00/8.3435208333333331√óQuocker0:00/14.4966251√ó","feature_image":"https://images.unsplash.com/photo-1641141109253-6c1fc4f53e66?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHBlZGVzdHJpYW4lMjBnaWZ8ZW58MHx8fHwxNzMyMjg1NTU5fDA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-09-27 13:07:30","created_by":"1","updated_at":"2024-11-22 15:44:18","updated_by":"1","published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":"custom-full-feature-image","canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Do you ever \",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"slow down\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"...?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"video\",\"version\":1,\"src\":\"__GHOST_URL__/content/media/2024/11/Hamburg-timelapse.mp4\",\"caption\":\"<p dir=\\\"ltr\\\"><span style=\\\"white-space: pre-wrap;\\\">Sunrise in Hamburg, 2022.</span></p>\",\"fileName\":\"Hamburg timelapse.mp4\",\"mimeType\":\"video/mp4\",\"width\":1280,\"height\":720,\"duration\":27.866666666666667,\"thumbnailSrc\":\"__GHOST_URL__/content/media/2024/11/Hamburg-timelapse_thumb.jpg\",\"customThumbnailSrc\":\"\",\"thumbnailWidth\":1280,\"thumbnailHeight\":720,\"cardWidth\":\"wide\",\"loop\":true},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I wake up by the buzzing sound of my alarm. That annoying sound. I had set the alarm just to be sure, in case I wasn't awake already. Well, I was \",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":8,\"mode\":\"normal\",\"style\":\"\",\"text\":\"not\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" awake already.  I had sacrificed my morning mindfulness for some extra sleep. I now have to be quick. Shower, get dressed, eat breakfast, drink tea ‚Äì fast, brush my teeth and go to work. I jump on my bike and make my way to work. My sole focus is to get me and my bike across from A to B as fast as possible. Evading pedestrians, cars, other cyclists. Brake as little as possible. Squeeze my way through an orange traffic light, or perhaps one or two coloured darker orange. My focus is narrow and the landscape slides all past me. I'm biking through the city but I'm not there. My head is not there. My head is busy preparing the day ahead of me. The meeting in half an hour. The presentation I am to give in the afternoon. The deadline next week. The colleague that I need to address.\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I wasn't quite there.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://images.unsplash.com/photo-1604987306200-c6212468ebcd?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHdhaXQlMjBwZWRlc3RyaWFufGVufDB8fHx8MTczMjI4NTk4N3ww&ixlib=rb-4.0.3&q=80&w=2000\",\"width\":6240,\"height\":4160,\"title\":\"\",\"alt\":\"black and yellow street lights\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">Photo by </span><a href=\\\"https://unsplash.com/@grievek1610begur\\\"><span style=\\\"white-space: pre-wrap;\\\">Kevin Grieve</span></a><span style=\\\"white-space: pre-wrap;\\\"> / </span><a href=\\\"https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit\\\"><span style=\\\"white-space: pre-wrap;\\\">Unsplash</span></a>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Later, I finished work. I made my way home down the same road I had came here. But this time, I saw more. I saw a construction worker, impressively operating a huge machine towering many stories above the ground. Lots of people, working at the same time, collaborating to accomplish one task.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"How refreshing to take the time \",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"looking\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Taking some time appreciating things and slowing yourself down.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/IMG_7076.jpeg\",\"width\":4032,\"height\":3024,\"title\":\"\",\"alt\":\"\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The funny thing is, I had reminded myself so often already. Slow down. Prioritise. Eliminate distractions. Take joy in the small things. Pay attention to details but don't go for perfectionist. 80% is enough.\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I had read all these books telling me these things. Books like Digital Minimalism, Deep Work, Stolen Focus, Four Thousand Weeks, Hyper Focus, The 4-Hour Work Week. You can say a big part of reading productivity books is feeling productive when you read them. Actually following their advice is something different altogether.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":32,\"mode\":\"normal\",\"style\":\"\",\"text\":\"--Isn't the world only so busy as you make it? So much as you follow the world's tendency to go fast?--\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Do you ever listen \",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Do you \",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"audio\",\"version\":1,\"duration\":20.844333333333335,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/ING-Netherlands-2-1.m4a\",\"title\":\"‚òïÔ∏è\",\"thumbnailSrc\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"dd\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"audio\",\"version\":1,\"duration\":1.747,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/Jacob-Bontiusplaats.m4a\",\"title\":\"üßº\",\"thumbnailSrc\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"dd\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"audio\",\"version\":1,\"duration\":1.2146666666666666,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/New-Recording-6.m4a\",\"title\":\"üö´\",\"thumbnailSrc\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"dd\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"audio\",\"version\":1,\"duration\":31.486041666666665,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/Printer-.m4a\",\"title\":\"Printer \",\"thumbnailSrc\":\"\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<audio controls src=\\\"__GHOST_URL__/content/media/2024/09/Jacob-Bontiusplaats.m4a\\\" preload=\\\"metadata\\\"></audio>\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"type\":\"audio\",\"version\":1,\"duration\":1.696,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/Slack-message.m4a\",\"title\":\"Slack message\",\"thumbnailSrc\":\"\"},{\"type\":\"audio\",\"version\":1,\"duration\":8.343520833333333,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/Standing-desk.m4a\",\"title\":\"Standing desk\",\"thumbnailSrc\":\"\"},{\"type\":\"audio\",\"version\":1,\"duration\":14.496625,\"mimeType\":\"audio/x-m4a\",\"src\":\"__GHOST_URL__/content/media/2024/09/Quocker.m4a\",\"title\":\"Quocker\",\"thumbnailSrc\":\"\"},{\"type\":\"image\",\"version\":1,\"src\":\"__GHOST_URL__/content/images/2024/09/data-src-image-92ae21e5-64bf-47c3-9403-6445950adba6.png\",\"width\":1408,\"height\":768,\"title\":\"\",\"alt\":\"Edited image result\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"66f6b3910aba3300014c3c67","uuid":"c464bef9-e193-42bc-918f-2fcaa7690bc7","title":"The Levels of RAG (on Xebia.com ‚ßâ)","slug":"the-levels-of-rag","mobiledoc":null,"html":"<h2 id=\"introduction\">Introduction</h2><p>LLM‚Äôs can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use cases work well and which are more challenging? Let‚Äôs find out together!</p><h2 id=\"why-rag\">Why RAG</h2><p>Retrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM‚Äôs). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. In short,&nbsp;<strong>RAG is valuable because</strong>:</p><ul><li>Access to up-to-date knowledge</li><li>Access to internal company data</li><li>More factual answers</li></ul><p>That all sounds great. But how does RAG work? Let‚Äôs introduce the Levels of RAG to help us understand RAG in increasing grades of complexity.</p><h2 id=\"the-levels-of-rag\">The Levels of RAG</h2><p>RAG has many different facets. To help more easily understand RAG, let‚Äôs break it down into four levels:</p>\n<!--kg-card-begin: html-->\n<table style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse;\"><caption class=\"text-center text-grey-500 text-sm\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center; font-size: 0.875rem; --tw-text-opacity: 1; color: rgb(119 131 148/var(--tw-text-opacity));\">The Levels of RAG.</caption><thead style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-1-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 1</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Basic RAG</th><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-2-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 2</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Hybrid Search</th><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-3-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 3</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Advanced data formats</th><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-4-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 4</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Multimodal</th></tr></thead><tbody style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"></tbody></table>\n<!--kg-card-end: html-->\n<p></p><p>Each level adds new complexity. Besides explaining the techniques, we will also look into justifying the introduced complexities. That is important, because you want to have good reasons to do so: we want to make everything as simple as possible, but no simpler&nbsp;<a href=\"https://www.goodreads.com/quotes/7133605-make-things-as-simple-as-possible-but-no-simpler\"><sup>[1]</sup></a>.</p><p>We will start with building a RAG from the beginning and understand which components are required to do so. So let‚Äôs jump right into&nbsp;<strong>Level 1: Basic RAG</strong>.</p><h2 id=\"level-1-basic-rag\">Level 1: Basic RAG</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-1-high-res.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"1024\" height=\"562\"></figure><p>Let‚Äôs build a RAG. To do RAG, we need two main components:&nbsp;<strong>document retrieval</strong>&nbsp;and&nbsp;<strong>answer generation</strong>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide1-3.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"924\"></figure><p>The two main components of RAG: document retrieval and answer generation.</p><p>In contrast with a normal LLM interaction, we are now first retrieving relevant context to only then answer the question using that context. That allows us to&nbsp;<em>ground</em>&nbsp;our answer in the retrieved context, making the answer more factually reliable.</p><p>Let‚Äôs look at both components in more detail, starting with the&nbsp;<em>Document retrieval</em>&nbsp;step. One of the main techniques powering our retrieval step is&nbsp;<strong>Vector Search</strong>.</p><h3 id=\"vector-search\">Vector Search</h3><p>To retrieve documents relevant to our user query, we will use Vector Search. This technique is based on&nbsp;<em>vector embeddings</em>. What are those? Imagine we embed words. Then words that are semantically similar should be closer together in the embedding space. We can do the same for sentences, paragraphs, or even whole documents. Such an embedding is typically represented by vectors of 768-, 1024-, or even 3072 dimensions. Though we as humans cannot visualize such high-dimensional spaces: we can only see 3 dimensions! For example sake let us compress such an embedding space into 2 dimensions so we visualize it:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide2-2.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"688\"></figure><p>Embeddings that are similar in meaning are closer to each other. In practice not in 2D but higher dimensions like 768D.</p><p>Note this is a drastically oversimplified explanation of vector embeddings. Creating vector embeddings of text: from words up to entire documents, is quite a study on its own. Most important to note though is that with embeddings we capture the&nbsp;<em>meaning</em>&nbsp;of the embedded text!</p><p>So how to use this for RAG? Well, instead of embedding words we can embed our source documents instead. We can then&nbsp;<em>also</em>&nbsp;embed the user question and then perform a&nbsp;<em>vector similarity search</em>&nbsp;on those:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide3.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"688\"></figure><p>Embedding both our source documents and the search query allows us to do a&nbsp;<em>vector similarity search</em>.</p><p>Great! We now have the ingredients necessary to construct our first&nbsp;<em>Document retrieval</em>&nbsp;setup:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide4.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"959\"></figure><p>The full retrieval step with&nbsp;<em>vector search</em>.</p><p>Next is the&nbsp;<em>Answer generation</em>&nbsp;step. This will entail passing the found pieces of context to an LLM to form a final answer out of it. We will keep that simple so will use a single prompt for that:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide5.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"982\"></figure><p>The RAG answer generation step. Prompt from LangChain hub (<a href=\"https://smith.langchain.com/hub/rlm/rag-prompt\">rlm/rag-prompt</a>).</p><p>Cool. That concludes our full first version of the Basic RAG. To test our RAG system, we need some data. I recently went to PyData and figured how cool would it be to create a RAG based on their schedule. Let‚Äôs design a RAG using the schedule of PyData Eindhoven 2024.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide6.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"775\"></figure><p>The RAG system loaded with data from the&nbsp;<a href=\"https://pydata.org/eindhoven2024/schedule\">PyData Eindhoven 2024 schedule</a>.</p><p>So how do we ingest such a schedule in a vector database? We will take each session and format it as Markdown, respecting the structure of the schedule by using headers.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide7.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"1123\"></figure><p>Each session is formatted as Markdown and then converted to an embedding.</p><p>Our RAG system is now fully functional. We embedded all sessions and ingested them into a vector database. We can then find sessions similar to the user question using vector similarity search and answer the question based on a predefined prompt. Let‚Äôs test it out!</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide8.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"948\"></figure><p>Testing out our basic RAG system. The RAG retrieves talks related to the user question and correctly answers the question.</p><p>That is cool! Our RAG was able to correctly answer the question. Upon inspection of the schedule we can see that the listed talks are indeed all related to sports. We just built a first RAG system üéâ.</p><p>There are points of improvement, however. We are now embedding the sessions in its entirety. But embedding large pieces of text can be problematic, because:</p><ul><li>‚ùå Embeddings can get saturated and lose meaning</li><li>‚ùå Imprecise citations</li><li>‚ùå Large context ‚Üí high cost</li></ul><p>So what we can do to solve this problem is to divide up text in smaller pieces, then embed those instead. This is&nbsp;<strong>Chunking</strong>.</p><h3 id=\"chunking\">Chunking</h3><p>In chunking the challenge lies in determining&nbsp;<em>how</em>&nbsp;to divide up the text to then embed those smaller pieces. There are&nbsp;<em>many</em>&nbsp;ways to chunk text. Let‚Äôs first look at a simple one. We will create chunks of fixed length:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide9.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"633\"></figure><p>Splitting a text into fixed-sized pieces does not create meaningful chunks. Words and sentences are split in random places.&nbsp;<br><a href=\"https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html#langchain_text_splitters.character.CharacterTextSplitter\">Character Text Splitter</a>&nbsp;with&nbsp;<code>chunk_size = 25</code>. This is an unrealistically small chunk size but is merely used for example sake.</p><p>This is not ideal. Words, sentences and paragraphs are not respected and are split at inconvenient locations. This decreases the quality of our embeddings. We can do better than this. Let‚Äôs try another splitter that tries to better respect the structure of the text by taking into account line breaks (¬∂):</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide10.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"633\"></figure><p>Text is split, respecting line breaks (¬∂). This way, chunks do not span across paragraphs and split better.&nbsp;<br><a href=\"https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter\">Recursive Character Text Splitter</a>&nbsp;with&nbsp;<code>chunk_size = 25</code>. This is an unrealistically small chunk size but is merely used for example sake.</p><p>This is better. The quality of our embeddings is now better due to better splits. Note that&nbsp;<code>chunk_size = 25</code>&nbsp;is only used for example sake. In practice we will use larger chunk sizes like 100, 500, or 1000. Try and see what works best on your data. But most notably, also do experiment with different text splitters. The&nbsp;<a href=\"https://api.python.langchain.com/en/latest/text_splitters_api_reference.html#\">LangChain Text Splitters</a>&nbsp;section has many available and the internet is full of others.</p><p>Now we have chunked our text, we can embed those chunks and ingest them in our vector database. Then, when we let the LLM answer our question we can also ask it to state which chunks it used in answering the question. This way, we can pinpoint accurately which information the LLM was grounded in whilst answering the question, allowing us to provide the user with&nbsp;<strong>Citations</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide11.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"1006\"></figure><p>Chunks can be used to show user where the information came from to generate the final answer, allowing us to show the user source&nbsp;<strong>citations</strong>.</p><p>That is great. Citations can be very powerful in a RAG application to improve transparency and thereby also user trust. Summarized, chunking has the following benefits:</p><ul><li>Embeddings are more meaningful</li><li>Precise citations</li><li>Shorter context ‚Üí lower cost</li></ul><p>We can now extend our&nbsp;<em>Document retrieval</em>&nbsp;step with chunking:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide12.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"959\"></figure><p>RAG retrieval step with&nbsp;<strong>chunking</strong>.</p><p>We have our Basic RAG set up with&nbsp;<em>Vector search</em>&nbsp;and&nbsp;<em>Chunking</em>. We also saw our system can answer questions correctly. But how well actually does it do? Let‚Äôs take the question ‚Äú<em>What‚Äôs the talk starting with Maximizing about?</em>‚Äù and launch it at our RAG:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide13.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"727\"></figure><p>For this question the incorrect context is retrieved, causing the LLM to give a&nbsp;<em>wrong</em>&nbsp;answer.<br>UI shown is&nbsp;<a href=\"https://www.langchain.com/langsmith\">LangSmith</a>, a GenAI monitoring tool. Open source alternative is&nbsp;<a href=\"https://langfuse.com/\">LangFuse</a>.</p><p>Ouch! This answer is wildly wrong. This is not the talk starting with&nbsp;<em>Maximizing</em>. The talk described has the title&nbsp;<em>Scikit-Learn can do THAT?!</em>, which clearly does not start with the word&nbsp;<em>Maximizing</em>.</p><p>For this reason, we need another method of search, like&nbsp;<em>keyword search</em>. Because we would also still like to keep the benefits of vector search, we can combine the two methods to create a&nbsp;<strong>Hybrid Search</strong>.</p><h2 id=\"level-2-hybrid-search\">Level 2: Hybrid Search</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-2-high-res.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"1024\" height=\"562\"></figure><p>In Hybrid Search, we aim to combine two ranking methods to get the best of both worlds. We will combine&nbsp;<em>Vector search</em>&nbsp;with&nbsp;<em>Keyword search</em>&nbsp;to create a Hybrid Search setup. To do so, we must pick a suitable ranking method. Common ranking algorithms are:</p><ul><li><a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\"><strong>TF-IDF</strong></a>: Term Frequency-Inverse Document Frequency</li><li><a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\"><strong>Okapi BM-25</strong></a>: Okapi Best Matching 25</li></ul><p>‚Ä¶ of which we can consider BM-25 an improved version of TF-IDF. Now, how do we combine Vector Search with an algorithm like BM-25? We now have two separate rankings we want to&nbsp;<em>fuse</em>&nbsp;together into one. For this, we can use&nbsp;<strong>Reciprocal Rank Fusion</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide14.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"872\"></figure><p>Reciprocal Rank Fusion is used to merge two ranking methods into one (see&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html\">Elastic Search docs</a>).</p><p>Reciprocal Rank Fusion is massively useful to combine two rankings. This way, we can now use both Vector search and Keyword search to retrieve documents. We can now extend again our&nbsp;<em>Document retrieval</em>&nbsp;step to create a Hybrid Search setup:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide15.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"959\"></figure><p>The RAG retrieval step with&nbsp;<strong>hybrid search</strong>.</p><p>Most notably, when a user performs a search, a query is made both to our vector database and to our keyword search. Once the results are merged using Reciprocal Rank Fusion, the top results are taken and passed back to our LLM.</p><p>Let‚Äôs take again the question ‚Äú<em>What‚Äôs the talk starting with Maximizing about?</em>‚Äù like we did in Level 1 and see how our RAG handles it with Hybrid Search:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide16.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"866\"></figure><p>Hybrid Search allows us to combine Vector search and Keyword search, allowing us to retrieve the desired document containing the keyword.</p><p>That is much better! üëèüèª The document we were looking for was now ranked high enough that it shows up in our retrieval step. It did so by boosting the terms used in the search query. Without the document available in our prompt context, the LLM could impossibly give us the correct answer. Let‚Äôs see both the retrieval step and generation step to see what our LLM now answers on this question:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide17.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"727\"></figure><p>With the correct context, we also got the correct answer!</p><p>That is the correct answer ‚úì. With the right context available to the LLM we also get the right answer. Retrieving the right context is the most important feature of our RAG: without it, the LLM can impossibly give us the right answer.</p><p>We now learned how to build a Basic RAG system and how to improve it with Hybrid Search. The data we loaded in comes from the PyData Eindhoven 2024 schedule: which was actually conveniently available in JSON format. But what about other data formats? In the real world, we can be asked to ingest other formats into our RAG like HTML, Word, and PDF.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide18.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"656\"></figure><p>Having structured data available like JSON is great, but that is not always the case in real life‚Ä¶</p><p>Formats like HTML, Word and especially PDF can be very unpredictable in terms of structure, making it hard for us to parse them consistently. PDF documents can contain images, graphs, tables, text, basically anything. So let us take on this challenge and level up to Level 3:&nbsp;<strong>Advanced data formats</strong>.</p><h2 id=\"level-3-advanced-data-formats\">Level 3: Advanced data formats</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-3-high-res.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"1024\" height=\"562\"></figure><p>This level revolves around ingesting challenging data formats like HTML, Word or PDF into our RAG. This requires extra considerations to do properly. For now, we will focus on&nbsp;<strong>PDF‚Äôs</strong>.</p><p>So, let us take some PDF‚Äôs as an example. I found some documents related to Space X‚Äôs Falcon 9 rocket:</p><p>Falcon 9 related documents we are going to build a RAG with.</p>\n<!--kg-card-begin: html-->\n<table style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse;\"><thead style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><th style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-falcon-users-guide-high-res.png\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"></th><th style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-falcon-nasa-document-high-res.png\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"></th><th style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-falcon-capabilities-overview-high-res.png\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"></th></tr></thead><tbody style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">User‚Äôs guide<span class=\"Apple-converted-space\">&nbsp;</span><a href=\"https://www.spacex.com/media/falcon-users-guide-2021-09.pdf\" style=\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\">(pdf)</a></small></td><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">Cost estimates<a href=\"https://www.nasa.gov/wp-content/uploads/2015/01/586023main_8-3-11_NAFCOM.pdf\" style=\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\"><span class=\"Apple-converted-space\">&nbsp;</span>(pdf)</a></small></td><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">Capabilities &amp; Services<span class=\"Apple-converted-space\">&nbsp;</span><a href=\"https://www.spacex.com/media/Capabilities&amp;Services.pdf\" style=\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\">(pdf)</a></small></td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>We will now want to first parse those documents into raw text, such that we can then chunk- and embed that text. To do so, we will use a PDF parser for Python like&nbsp;<a href=\"https://pypdf.readthedocs.io/en/stable/\">pypdf</a>. Conveniently, there‚Äôs a LangChain loader available for pypdf:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide20.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"792\"></figure><p>PDF to text parsing using&nbsp;<a href=\"https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html#langchain_community.document_loaders.pdf.PyPDFLoader\">PyPDFLoader</a>.</p><p>Using pypdf we could convert these PDF‚Äôs into&nbsp;<em>raw text</em>. Note there‚Äôs many more available, check out the&nbsp;<a href=\"https://api.python.langchain.com/en/latest/search.html?q=pdf\">LangChain API reference</a>. Both offline, local and Cloud solutions are offered like&nbsp;<a href=\"https://cloud.google.com/document-ai\">GCP Document AI</a>,&nbsp;<a href=\"https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence\">Azure Document Intelligence</a>&nbsp;or&nbsp;<a href=\"https://aws.amazon.com/textract/\">Amazon Textract</a>.</p><p>With such a document parsing step set up, we will need to extend our&nbsp;<em>Document retrieval</em>&nbsp;step to cover this new component:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide21.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"959\"></figure><p>RAG retrieval with a&nbsp;<em>document parsing step</em>, like converting PDF to raw text.</p><p>Time to test out our RAG! We embedded and ingested the raw text into our vector database and can now make queries against it. Let‚Äôs ask about the cost of a Falcon 9 rocket launch:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide22.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"735\"></figure><p>Our RAG can now answer questions about the PDF‚Äôs.</p><p>Awesome, that is the correct answer. Let‚Äôs try another question: ‚Äú<em>What is the payload I can carry with the Falcon 9 rocket to Mars?</em>‚Äú:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide23.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"740\"></figure><p>Ouch! Our RAG got the answer wrong. It suggests that we can bring 8,300 kg of payload to Mars with a Falcon 9 rocket whilst in reality, this is 4,020 kg. That‚Äôs no small error.</p><p>Ow! Our RAG got that answer all wrong. It suggests we can bring double the payload to Mars than what is allowed. That is pretty inconvenient, in case you were preparing for a trip to Mars üòâ.</p><p>We need to debug what went wrong. Let‚Äôs look at the context that was passed to the LLM:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide24.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"834\"></figure><p>The context provided to the LLM contains jumbled and unformatted text originating from a table. It should therefore come as no surprise that our LLM has difficulties answering questions about this table.</p><p>That explains a bunch. The context we‚Äôre passing to the LLM is hard to read and has a table encoded in a jumbled way. Like us the LLM has a hard time making sense of this. Therefore, we need to better encode this information in the prompt so our LLM can understand it.</p><p>If we want to support tables we can introduce an extra processing step. One option is to use Computer vision models to detect tables inside our documents, like&nbsp;<a href=\"https://github.com/microsoft/table-transformer\"><code>table-transformer</code></a>. If a table gets detected we can then give it a special treatment. What we can for example do, is encode tables in our prompt as&nbsp;<strong>Markdown</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide25.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"420\"></figure><p>First parsing our table into a native format and then converting it to Markdown allows our LLM to much better understand the table.</p><p>Having detected the table and parsed it into a native format in our Python code allows us to then encode it in Markdown. Let‚Äôs pass that to our LLM instead and see what it answers this time:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide26.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"849\"></figure><p>Our RAG got the answer correct this time.</p><p>Hurray! We got it correct this time. The LLM we used could easily interpret the Markdown table and pinpoint the correct value to use in answering the question. Note that still, we need to be able to retrieve the table in our retrieval step. This setup assumes we built a retrieval step that is able to retrieve the table given the user question.</p><p>However, I have to admit something. The model we have been using for this task was GPT-3.5 turbo, which is a text-only model. Newer models have been released that can handle more than just text, which are&nbsp;<em>Multimodal</em>&nbsp;models. After all, we are dealing with PDF‚Äôs which can also be seen as a series of images. Can we leverage such multimodal models to better answer our questions? Let‚Äôs find out in Level 4:&nbsp;<strong>Multimodal</strong>.</p><h2 id=\"level-4-multimodal\">Level 4: Multimodal</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-4-high-res.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"1024\" height=\"562\"></figure><p>In this final level, we will look into leveraging the possibilities of Multimodal models. One of them is GPT-4o, which was announced May 2024:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide27.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"531\"></figure><p>GPT-4o is a multimodal model that can reason across audio, vision and text. Launched by OpenAI in May 2024.</p><p>This is a very powerful model that can understand audio, vision and text. This means we can feed it images as part of an input prompt. Given that we can in our retrieval step retrieve the right PDF pages to use, we can insert those images in the prompt and ask the LLM our original question. This has the advantage that we can understand content that was previously&nbsp;<em>very</em>&nbsp;challenging to encode in just text. Also, content we interpret and encode as text is exposed to more conversion steps exposing us to risk of information getting lost in translation.</p><p>For example sake we will take the same table we had before but then answer the question using a Multimodal model. We can take the retrieved PDF pages encoded as images and insert them right into the prompt:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide28.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"874\"></figure><p>With a Multimodal model, we can insert an image in the prompt and let the LLM answer questions about it.</p><p>Impressive. The LLM got the answer correct. We should be aware though, that inserting images in the prompt comes with a very different token usage than the Markdown table we inserted as text before:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide29.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" width=\"2112\" height=\"327\"></figure><p>When using Multimodal models, do be aware of the extra cost that comes with it.</p><p>That is an immense increase in cost. Multimodal models can be incredibly powerful to interpret content that is otherwise very difficult to encode in text, as long as it is worth the cost ‚úì.</p><h2 id=\"concluding\">Concluding</h2><p>We have explored RAG in 4 levels of complexity. We went from building our first basic RAG to a RAG that leverages Multimodal models to answer questions based on complex documents. Each level introduces new complexities which are justified in each their own way. Summarising, the Levels of RAG are:</p>\n<!--kg-card-begin: html-->\n<table style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse;\"><caption class=\"text-center text-grey-500 text-sm\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center; font-size: 0.875rem; --tw-text-opacity: 1; color: rgb(119 131 148/var(--tw-text-opacity));\">The Levels of RAG, summarised.</caption><thead style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-1-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 1</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Basic RAG</th><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-2-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 2</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Hybrid Search</th><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-3-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 3</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Advanced data formats</th><th width=\"25%\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left;\"><img decoding=\"async\" alt=\"alt text\" src=\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-4-high-res.jpg\" style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\"><span style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\">Level 4</span><br style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\">Multimodal</th></tr></thead><tbody style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\">RAG‚Äôs main steps are 1) retrieval and 2) generation. Important components to do so are Embedding, Vector Search using a Vector database, Chunking and a Large Language Model (LLM).</td><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\">Combining vector search and keyword search can improve retrieval performance. Sparse text search can be done using: TF-IDF and BM- 25. Reciprocal Rank Fusion can be used to merge two search engine rankings.</td><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\">Support formats like HTML, Word and PDF. PDF can contain images, graphs but also tables. Tables need a separate treatment, for example with Computer Vision, to then expose the table to the LLM as Markdown.</td><td style=\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\">Multimodal models can reason across audio, images and even video. Such models can help process complex data formats, for example by exposing PDF‚Äôs as images to the model. Given that the extra cost is worth the benefit, such models can be incredibly powerful.</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p></p><p>RAG is a very powerful technique which can open up many new possibilities at companies. The Levels of RAG help you reason about the complexity of your RAG and allow you to understand what is difficult to do with RAG and what is easier. So: what is&nbsp;<em>your</em>&nbsp;level? ü´µ</p><p>We wish you all the best with building your own RAG üëèüèª.</p>","comment_id":"66f6b3910aba3300014c3c67","plaintext":"Introduction\n\nLLM‚Äôs can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use cases work well and which are more challenging? Let‚Äôs find out together!\n\n\nWhy RAG\n\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM‚Äôs). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. In short,¬†RAG is valuable because:\n\n * Access to up-to-date knowledge\n * Access to internal company data\n * More factual answers\n\nThat all sounds great. But how does RAG work? Let‚Äôs introduce the Levels of RAG to help us understand RAG in increasing grades of complexity.\n\n\nThe Levels of RAG\n\nRAG has many different facets. To help more easily understand RAG, let‚Äôs break it down into four levels:\n\n\n\nThe Levels of RAG.Level 1\nBasic RAGLevel 2\nHybrid SearchLevel 3\nAdvanced data formatsLevel 4\nMultimodal\n\n\n\n\n\nEach level adds new complexity. Besides explaining the techniques, we will also look into justifying the introduced complexities. That is important, because you want to have good reasons to do so: we want to make everything as simple as possible, but no simpler¬†[1].\n\nWe will start with building a RAG from the beginning and understand which components are required to do so. So let‚Äôs jump right into¬†Level 1: Basic RAG.\n\n\nLevel 1: Basic RAG\n\nLet‚Äôs build a RAG. To do RAG, we need two main components:¬†document retrieval¬†and¬†answer generation.\n\nThe two main components of RAG: document retrieval and answer generation.\n\nIn contrast with a normal LLM interaction, we are now first retrieving relevant context to only then answer the question using that context. That allows us to¬†ground¬†our answer in the retrieved context, making the answer more factually reliable.\n\nLet‚Äôs look at both components in more detail, starting with the¬†Document retrieval¬†step. One of the main techniques powering our retrieval step is¬†Vector Search.\n\n\nVector Search\n\nTo retrieve documents relevant to our user query, we will use Vector Search. This technique is based on¬†vector embeddings. What are those? Imagine we embed words. Then words that are semantically similar should be closer together in the embedding space. We can do the same for sentences, paragraphs, or even whole documents. Such an embedding is typically represented by vectors of 768-, 1024-, or even 3072 dimensions. Though we as humans cannot visualize such high-dimensional spaces: we can only see 3 dimensions! For example sake let us compress such an embedding space into 2 dimensions so we visualize it:\n\nEmbeddings that are similar in meaning are closer to each other. In practice not in 2D but higher dimensions like 768D.\n\nNote this is a drastically oversimplified explanation of vector embeddings. Creating vector embeddings of text: from words up to entire documents, is quite a study on its own. Most important to note though is that with embeddings we capture the¬†meaning¬†of the embedded text!\n\nSo how to use this for RAG? Well, instead of embedding words we can embed our source documents instead. We can then¬†also¬†embed the user question and then perform a¬†vector similarity search¬†on those:\n\nEmbedding both our source documents and the search query allows us to do a¬†vector similarity search.\n\nGreat! We now have the ingredients necessary to construct our first¬†Document retrieval¬†setup:\n\nThe full retrieval step with¬†vector search.\n\nNext is the¬†Answer generation¬†step. This will entail passing the found pieces of context to an LLM to form a final answer out of it. We will keep that simple so will use a single prompt for that:\n\nThe RAG answer generation step. Prompt from LangChain hub (rlm/rag-prompt).\n\nCool. That concludes our full first version of the Basic RAG. To test our RAG system, we need some data. I recently went to PyData and figured how cool would it be to create a RAG based on their schedule. Let‚Äôs design a RAG using the schedule of PyData Eindhoven 2024.\n\nThe RAG system loaded with data from the¬†PyData Eindhoven 2024 schedule.\n\nSo how do we ingest such a schedule in a vector database? We will take each session and format it as Markdown, respecting the structure of the schedule by using headers.\n\nEach session is formatted as Markdown and then converted to an embedding.\n\nOur RAG system is now fully functional. We embedded all sessions and ingested them into a vector database. We can then find sessions similar to the user question using vector similarity search and answer the question based on a predefined prompt. Let‚Äôs test it out!\n\nTesting out our basic RAG system. The RAG retrieves talks related to the user question and correctly answers the question.\n\nThat is cool! Our RAG was able to correctly answer the question. Upon inspection of the schedule we can see that the listed talks are indeed all related to sports. We just built a first RAG system üéâ.\n\nThere are points of improvement, however. We are now embedding the sessions in its entirety. But embedding large pieces of text can be problematic, because:\n\n * ‚ùå Embeddings can get saturated and lose meaning\n * ‚ùå Imprecise citations\n * ‚ùå Large context ‚Üí high cost\n\nSo what we can do to solve this problem is to divide up text in smaller pieces, then embed those instead. This is¬†Chunking.\n\n\nChunking\n\nIn chunking the challenge lies in determining¬†how¬†to divide up the text to then embed those smaller pieces. There are¬†many¬†ways to chunk text. Let‚Äôs first look at a simple one. We will create chunks of fixed length:\n\nSplitting a text into fixed-sized pieces does not create meaningful chunks. Words and sentences are split in random places.¬†\nCharacter Text Splitter¬†with¬†chunk_size = 25. This is an unrealistically small chunk size but is merely used for example sake.\n\nThis is not ideal. Words, sentences and paragraphs are not respected and are split at inconvenient locations. This decreases the quality of our embeddings. We can do better than this. Let‚Äôs try another splitter that tries to better respect the structure of the text by taking into account line breaks (¬∂):\n\nText is split, respecting line breaks (¬∂). This way, chunks do not span across paragraphs and split better.¬†\nRecursive Character Text Splitter¬†with¬†chunk_size = 25. This is an unrealistically small chunk size but is merely used for example sake.\n\nThis is better. The quality of our embeddings is now better due to better splits. Note that¬†chunk_size = 25¬†is only used for example sake. In practice we will use larger chunk sizes like 100, 500, or 1000. Try and see what works best on your data. But most notably, also do experiment with different text splitters. The¬†LangChain Text Splitters¬†section has many available and the internet is full of others.\n\nNow we have chunked our text, we can embed those chunks and ingest them in our vector database. Then, when we let the LLM answer our question we can also ask it to state which chunks it used in answering the question. This way, we can pinpoint accurately which information the LLM was grounded in whilst answering the question, allowing us to provide the user with¬†Citations:\n\nChunks can be used to show user where the information came from to generate the final answer, allowing us to show the user source¬†citations.\n\nThat is great. Citations can be very powerful in a RAG application to improve transparency and thereby also user trust. Summarized, chunking has the following benefits:\n\n * Embeddings are more meaningful\n * Precise citations\n * Shorter context ‚Üí lower cost\n\nWe can now extend our¬†Document retrieval¬†step with chunking:\n\nRAG retrieval step with¬†chunking.\n\nWe have our Basic RAG set up with¬†Vector search¬†and¬†Chunking. We also saw our system can answer questions correctly. But how well actually does it do? Let‚Äôs take the question ‚ÄúWhat‚Äôs the talk starting with Maximizing about?‚Äù and launch it at our RAG:\n\nFor this question the incorrect context is retrieved, causing the LLM to give a¬†wrong¬†answer.\nUI shown is¬†LangSmith, a GenAI monitoring tool. Open source alternative is¬†LangFuse.\n\nOuch! This answer is wildly wrong. This is not the talk starting with¬†Maximizing. The talk described has the title¬†Scikit-Learn can do THAT?!, which clearly does not start with the word¬†Maximizing.\n\nFor this reason, we need another method of search, like¬†keyword search. Because we would also still like to keep the benefits of vector search, we can combine the two methods to create a¬†Hybrid Search.\n\n\nLevel 2: Hybrid Search\n\nIn Hybrid Search, we aim to combine two ranking methods to get the best of both worlds. We will combine¬†Vector search¬†with¬†Keyword search¬†to create a Hybrid Search setup. To do so, we must pick a suitable ranking method. Common ranking algorithms are:\n\n * TF-IDF: Term Frequency-Inverse Document Frequency\n * Okapi BM-25: Okapi Best Matching 25\n\n‚Ä¶ of which we can consider BM-25 an improved version of TF-IDF. Now, how do we combine Vector Search with an algorithm like BM-25? We now have two separate rankings we want to¬†fuse¬†together into one. For this, we can use¬†Reciprocal Rank Fusion:\n\nReciprocal Rank Fusion is used to merge two ranking methods into one (see¬†Elastic Search docs).\n\nReciprocal Rank Fusion is massively useful to combine two rankings. This way, we can now use both Vector search and Keyword search to retrieve documents. We can now extend again our¬†Document retrieval¬†step to create a Hybrid Search setup:\n\nThe RAG retrieval step with¬†hybrid search.\n\nMost notably, when a user performs a search, a query is made both to our vector database and to our keyword search. Once the results are merged using Reciprocal Rank Fusion, the top results are taken and passed back to our LLM.\n\nLet‚Äôs take again the question ‚ÄúWhat‚Äôs the talk starting with Maximizing about?‚Äù like we did in Level 1 and see how our RAG handles it with Hybrid Search:\n\nHybrid Search allows us to combine Vector search and Keyword search, allowing us to retrieve the desired document containing the keyword.\n\nThat is much better! üëèüèª The document we were looking for was now ranked high enough that it shows up in our retrieval step. It did so by boosting the terms used in the search query. Without the document available in our prompt context, the LLM could impossibly give us the correct answer. Let‚Äôs see both the retrieval step and generation step to see what our LLM now answers on this question:\n\nWith the correct context, we also got the correct answer!\n\nThat is the correct answer ‚úì. With the right context available to the LLM we also get the right answer. Retrieving the right context is the most important feature of our RAG: without it, the LLM can impossibly give us the right answer.\n\nWe now learned how to build a Basic RAG system and how to improve it with Hybrid Search. The data we loaded in comes from the PyData Eindhoven 2024 schedule: which was actually conveniently available in JSON format. But what about other data formats? In the real world, we can be asked to ingest other formats into our RAG like HTML, Word, and PDF.\n\nHaving structured data available like JSON is great, but that is not always the case in real life‚Ä¶\n\nFormats like HTML, Word and especially PDF can be very unpredictable in terms of structure, making it hard for us to parse them consistently. PDF documents can contain images, graphs, tables, text, basically anything. So let us take on this challenge and level up to Level 3:¬†Advanced data formats.\n\n\nLevel 3: Advanced data formats\n\nThis level revolves around ingesting challenging data formats like HTML, Word or PDF into our RAG. This requires extra considerations to do properly. For now, we will focus on¬†PDF‚Äôs.\n\nSo, let us take some PDF‚Äôs as an example. I found some documents related to Space X‚Äôs Falcon 9 rocket:\n\nFalcon 9 related documents we are going to build a RAG with.\n\n\n\nUser‚Äôs guide¬†(pdf)Cost estimates¬†(pdf)Capabilities & Services¬†(pdf)\n\n\n\nWe will now want to first parse those documents into raw text, such that we can then chunk- and embed that text. To do so, we will use a PDF parser for Python like¬†pypdf. Conveniently, there‚Äôs a LangChain loader available for pypdf:\n\nPDF to text parsing using¬†PyPDFLoader.\n\nUsing pypdf we could convert these PDF‚Äôs into¬†raw text. Note there‚Äôs many more available, check out the¬†LangChain API reference. Both offline, local and Cloud solutions are offered like¬†GCP Document AI,¬†Azure Document Intelligence¬†or¬†Amazon Textract.\n\nWith such a document parsing step set up, we will need to extend our¬†Document retrieval¬†step to cover this new component:\n\nRAG retrieval with a¬†document parsing step, like converting PDF to raw text.\n\nTime to test out our RAG! We embedded and ingested the raw text into our vector database and can now make queries against it. Let‚Äôs ask about the cost of a Falcon 9 rocket launch:\n\nOur RAG can now answer questions about the PDF‚Äôs.\n\nAwesome, that is the correct answer. Let‚Äôs try another question: ‚ÄúWhat is the payload I can carry with the Falcon 9 rocket to Mars?‚Äú:\n\nOuch! Our RAG got the answer wrong. It suggests that we can bring 8,300 kg of payload to Mars with a Falcon 9 rocket whilst in reality, this is 4,020 kg. That‚Äôs no small error.\n\nOw! Our RAG got that answer all wrong. It suggests we can bring double the payload to Mars than what is allowed. That is pretty inconvenient, in case you were preparing for a trip to Mars üòâ.\n\nWe need to debug what went wrong. Let‚Äôs look at the context that was passed to the LLM:\n\nThe context provided to the LLM contains jumbled and unformatted text originating from a table. It should therefore come as no surprise that our LLM has difficulties answering questions about this table.\n\nThat explains a bunch. The context we‚Äôre passing to the LLM is hard to read and has a table encoded in a jumbled way. Like us the LLM has a hard time making sense of this. Therefore, we need to better encode this information in the prompt so our LLM can understand it.\n\nIf we want to support tables we can introduce an extra processing step. One option is to use Computer vision models to detect tables inside our documents, like¬†table-transformer. If a table gets detected we can then give it a special treatment. What we can for example do, is encode tables in our prompt as¬†Markdown:\n\nFirst parsing our table into a native format and then converting it to Markdown allows our LLM to much better understand the table.\n\nHaving detected the table and parsed it into a native format in our Python code allows us to then encode it in Markdown. Let‚Äôs pass that to our LLM instead and see what it answers this time:\n\nOur RAG got the answer correct this time.\n\nHurray! We got it correct this time. The LLM we used could easily interpret the Markdown table and pinpoint the correct value to use in answering the question. Note that still, we need to be able to retrieve the table in our retrieval step. This setup assumes we built a retrieval step that is able to retrieve the table given the user question.\n\nHowever, I have to admit something. The model we have been using for this task was GPT-3.5 turbo, which is a text-only model. Newer models have been released that can handle more than just text, which are¬†Multimodal¬†models. After all, we are dealing with PDF‚Äôs which can also be seen as a series of images. Can we leverage such multimodal models to better answer our questions? Let‚Äôs find out in Level 4:¬†Multimodal.\n\n\nLevel 4: Multimodal\n\nIn this final level, we will look into leveraging the possibilities of Multimodal models. One of them is GPT-4o, which was announced May 2024:\n\nGPT-4o is a multimodal model that can reason across audio, vision and text. Launched by OpenAI in May 2024.\n\nThis is a very powerful model that can understand audio, vision and text. This means we can feed it images as part of an input prompt. Given that we can in our retrieval step retrieve the right PDF pages to use, we can insert those images in the prompt and ask the LLM our original question. This has the advantage that we can understand content that was previously¬†very¬†challenging to encode in just text. Also, content we interpret and encode as text is exposed to more conversion steps exposing us to risk of information getting lost in translation.\n\nFor example sake we will take the same table we had before but then answer the question using a Multimodal model. We can take the retrieved PDF pages encoded as images and insert them right into the prompt:\n\nWith a Multimodal model, we can insert an image in the prompt and let the LLM answer questions about it.\n\nImpressive. The LLM got the answer correct. We should be aware though, that inserting images in the prompt comes with a very different token usage than the Markdown table we inserted as text before:\n\nWhen using Multimodal models, do be aware of the extra cost that comes with it.\n\nThat is an immense increase in cost. Multimodal models can be incredibly powerful to interpret content that is otherwise very difficult to encode in text, as long as it is worth the cost ‚úì.\n\n\nConcluding\n\nWe have explored RAG in 4 levels of complexity. We went from building our first basic RAG to a RAG that leverages Multimodal models to answer questions based on complex documents. Each level introduces new complexities which are justified in each their own way. Summarising, the Levels of RAG are:\n\n\n\nThe Levels of RAG, summarised.Level 1\nBasic RAGLevel 2\nHybrid SearchLevel 3\nAdvanced data formatsLevel 4\nMultimodalRAG‚Äôs main steps are 1) retrieval and 2) generation. Important components to do so are Embedding, Vector Search using a Vector database, Chunking and a Large Language Model (LLM).Combining vector search and keyword search can improve retrieval performance. Sparse text search can be done using: TF-IDF and BM- 25. Reciprocal Rank Fusion can be used to merge two search engine rankings.Support formats like HTML, Word and PDF. PDF can contain images, graphs but also tables. Tables need a separate treatment, for example with Computer Vision, to then expose the table to the LLM as Markdown.Multimodal models can reason across audio, images and even video. Such models can help process complex data formats, for example by exposing PDF‚Äôs as images to the model. Given that the extra cost is worth the benefit, such models can be incredibly powerful.\n\n\n\n\n\nRAG is a very powerful technique which can open up many new possibilities at companies. The Levels of RAG help you reason about the complexity of your RAG and allow you to understand what is difficult to do with RAG and what is easier. So: what is¬†your¬†level? ü´µ\n\nWe wish you all the best with building your own RAG üëèüèª.","feature_image":"__GHOST_URL__/content/images/2024/09/banner.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-09-27 13:30:57","created_by":"1","updated_at":"2024-11-22 13:40:28","updated_by":"1","published_at":"2024-09-27 13:34:38","published_by":"1","custom_excerpt":"LLM‚Äôs can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. But what can we expect from RAG? What use cases work well and which are more challenging? Let‚Äôs find out together!","codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; URL=https://xebia.com/blog/levels-of-rag/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Introduction\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LLM‚Äôs can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use cases work well and which are more challenging? Let‚Äôs find out together!\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Why RAG\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Retrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM‚Äôs). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. In short,¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG is valuable because\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Access to up-to-date knowledge\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Access to internal company data\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"More factual answers\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That all sounds great. But how does RAG work? Let‚Äôs introduce the Levels of RAG to help us understand RAG in increasing grades of complexity.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The Levels of RAG\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG has many different facets. To help more easily understand RAG, let‚Äôs break it down into four levels:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<table style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse;\\\"><caption class=\\\"text-center text-grey-500 text-sm\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center; font-size: 0.875rem; --tw-text-opacity: 1; color: rgb(119 131 148/var(--tw-text-opacity));\\\">The Levels of RAG.</caption><thead style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-1-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 1</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Basic RAG</th><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-2-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 2</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Hybrid Search</th><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-3-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 3</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Advanced data formats</th><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-4-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 4</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Multimodal</th></tr></thead><tbody style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"></tbody></table>\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Each level adds new complexity. Besides explaining the techniques, we will also look into justifying the introduced complexities. That is important, because you want to have good reasons to do so: we want to make everything as simple as possible, but no simpler¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":64,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[1]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.goodreads.com/quotes/7133605-make-things-as-simple-as-possible-but-no-simpler\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We will start with building a RAG from the beginning and understand which components are required to do so. So let‚Äôs jump right into¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 1: Basic RAG\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 1: Basic RAG\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-1-high-res.jpg\",\"width\":1024,\"height\":562,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs build a RAG. To do RAG, we need two main components:¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"document retrieval\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†and¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"answer generation\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide1-3.png\",\"width\":2112,\"height\":924,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The two main components of RAG: document retrieval and answer generation.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In contrast with a normal LLM interaction, we are now first retrieving relevant context to only then answer the question using that context. That allows us to¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"ground\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†our answer in the retrieved context, making the answer more factually reliable.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs look at both components in more detail, starting with the¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†step. One of the main techniques powering our retrieval step is¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vector Search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vector Search\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"To retrieve documents relevant to our user query, we will use Vector Search. This technique is based on¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"vector embeddings\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". What are those? Imagine we embed words. Then words that are semantically similar should be closer together in the embedding space. We can do the same for sentences, paragraphs, or even whole documents. Such an embedding is typically represented by vectors of 768-, 1024-, or even 3072 dimensions. Though we as humans cannot visualize such high-dimensional spaces: we can only see 3 dimensions! For example sake let us compress such an embedding space into 2 dimensions so we visualize it:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide2-2.png\",\"width\":2112,\"height\":688,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Embeddings that are similar in meaning are closer to each other. In practice not in 2D but higher dimensions like 768D.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Note this is a drastically oversimplified explanation of vector embeddings. Creating vector embeddings of text: from words up to entire documents, is quite a study on its own. Most important to note though is that with embeddings we capture the¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"meaning\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†of the embedded text!\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So how to use this for RAG? Well, instead of embedding words we can embed our source documents instead. We can then¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"also\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†embed the user question and then perform a¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"vector similarity search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†on those:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide3.png\",\"width\":2112,\"height\":688,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Embedding both our source documents and the search query allows us to do a¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"vector similarity search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Great! We now have the ingredients necessary to construct our first¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†setup:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide4.png\",\"width\":2112,\"height\":959,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The full retrieval step with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"vector search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Next is the¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Answer generation\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†step. This will entail passing the found pieces of context to an LLM to form a final answer out of it. We will keep that simple so will use a single prompt for that:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide5.png\",\"width\":2112,\"height\":982,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The RAG answer generation step. Prompt from LangChain hub (\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"rlm/rag-prompt\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://smith.langchain.com/hub/rlm/rag-prompt\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\").\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Cool. That concludes our full first version of the Basic RAG. To test our RAG system, we need some data. I recently went to PyData and figured how cool would it be to create a RAG based on their schedule. Let‚Äôs design a RAG using the schedule of PyData Eindhoven 2024.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide6.png\",\"width\":2112,\"height\":775,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The RAG system loaded with data from the¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyData Eindhoven 2024 schedule\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://pydata.org/eindhoven2024/schedule\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So how do we ingest such a schedule in a vector database? We will take each session and format it as Markdown, respecting the structure of the schedule by using headers.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide7.jpg\",\"width\":2112,\"height\":1123,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Each session is formatted as Markdown and then converted to an embedding.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Our RAG system is now fully functional. We embedded all sessions and ingested them into a vector database. We can then find sessions similar to the user question using vector similarity search and answer the question based on a predefined prompt. Let‚Äôs test it out!\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide8.jpg\",\"width\":2112,\"height\":948,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Testing out our basic RAG system. The RAG retrieves talks related to the user question and correctly answers the question.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is cool! Our RAG was able to correctly answer the question. Upon inspection of the schedule we can see that the listed talks are indeed all related to sports. We just built a first RAG system üéâ.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"There are points of improvement, however. We are now embedding the sessions in its entirety. But embedding large pieces of text can be problematic, because:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚ùå Embeddings can get saturated and lose meaning\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚ùå Imprecise citations\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚ùå Large context ‚Üí high cost\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So what we can do to solve this problem is to divide up text in smaller pieces, then embed those instead. This is¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Chunking\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Chunking\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In chunking the challenge lies in determining¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"how\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†to divide up the text to then embed those smaller pieces. There are¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"many\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†ways to chunk text. Let‚Äôs first look at a simple one. We will create chunks of fixed length:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide9.png\",\"width\":2112,\"height\":633,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Splitting a text into fixed-sized pieces does not create meaningful chunks. Words and sentences are split in random places.¬†\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Character Text Splitter\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html#langchain_text_splitters.character.CharacterTextSplitter\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"chunk_size = 25\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". This is an unrealistically small chunk size but is merely used for example sake.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This is not ideal. Words, sentences and paragraphs are not respected and are split at inconvenient locations. This decreases the quality of our embeddings. We can do better than this. Let‚Äôs try another splitter that tries to better respect the structure of the text by taking into account line breaks (¬∂):\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide10.png\",\"width\":2112,\"height\":633,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Text is split, respecting line breaks (¬∂). This way, chunks do not span across paragraphs and split better.¬†\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Recursive Character Text Splitter\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"chunk_size = 25\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". This is an unrealistically small chunk size but is merely used for example sake.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This is better. The quality of our embeddings is now better due to better splits. Note that¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"chunk_size = 25\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†is only used for example sake. In practice we will use larger chunk sizes like 100, 500, or 1000. Try and see what works best on your data. But most notably, also do experiment with different text splitters. The¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LangChain Text Splitters\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://api.python.langchain.com/en/latest/text_splitters_api_reference.html#\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†section has many available and the internet is full of others.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Now we have chunked our text, we can embed those chunks and ingest them in our vector database. Then, when we let the LLM answer our question we can also ask it to state which chunks it used in answering the question. This way, we can pinpoint accurately which information the LLM was grounded in whilst answering the question, allowing us to provide the user with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Citations\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide11.jpg\",\"width\":2112,\"height\":1006,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Chunks can be used to show user where the information came from to generate the final answer, allowing us to show the user source¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"citations\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is great. Citations can be very powerful in a RAG application to improve transparency and thereby also user trust. Summarized, chunking has the following benefits:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Embeddings are more meaningful\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Precise citations\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Shorter context ‚Üí lower cost\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can now extend our¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†step with chunking:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide12.png\",\"width\":2112,\"height\":959,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG retrieval step with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"chunking\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We have our Basic RAG set up with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vector search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†and¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Chunking\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". We also saw our system can answer questions correctly. But how well actually does it do? Let‚Äôs take the question ‚Äú\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What‚Äôs the talk starting with Maximizing about?\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äù and launch it at our RAG:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide13.png\",\"width\":2112,\"height\":727,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"For this question the incorrect context is retrieved, causing the LLM to give a¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"wrong\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†answer.\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"UI shown is¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LangSmith\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.langchain.com/langsmith\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", a GenAI monitoring tool. Open source alternative is¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LangFuse\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://langfuse.com/\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ouch! This answer is wildly wrong. This is not the talk starting with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Maximizing\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". The talk described has the title¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Scikit-Learn can do THAT?!\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", which clearly does not start with the word¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Maximizing\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"For this reason, we need another method of search, like¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"keyword search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Because we would also still like to keep the benefits of vector search, we can combine the two methods to create a¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hybrid Search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 2: Hybrid Search\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-2-high-res.jpg\",\"width\":1024,\"height\":562,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In Hybrid Search, we aim to combine two ranking methods to get the best of both worlds. We will combine¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Vector search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Keyword search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†to create a Hybrid Search setup. To do so, we must pick a suitable ranking method. Common ranking algorithms are:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"TF-IDF\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": Term Frequency-Inverse Document Frequency\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Okapi BM-25\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://en.wikipedia.org/wiki/Okapi_BM25\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": Okapi Best Matching 25\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Ä¶ of which we can consider BM-25 an improved version of TF-IDF. Now, how do we combine Vector Search with an algorithm like BM-25? We now have two separate rankings we want to¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"fuse\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†together into one. For this, we can use¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Reciprocal Rank Fusion\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide14.png\",\"width\":2112,\"height\":872,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Reciprocal Rank Fusion is used to merge two ranking methods into one (see¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Elastic Search docs\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\").\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Reciprocal Rank Fusion is massively useful to combine two rankings. This way, we can now use both Vector search and Keyword search to retrieve documents. We can now extend again our¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†step to create a Hybrid Search setup:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide15.png\",\"width\":2112,\"height\":959,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The RAG retrieval step with¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"hybrid search\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Most notably, when a user performs a search, a query is made both to our vector database and to our keyword search. Once the results are merged using Reciprocal Rank Fusion, the top results are taken and passed back to our LLM.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs take again the question ‚Äú\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What‚Äôs the talk starting with Maximizing about?\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äù like we did in Level 1 and see how our RAG handles it with Hybrid Search:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide16.png\",\"width\":2112,\"height\":866,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hybrid Search allows us to combine Vector search and Keyword search, allowing us to retrieve the desired document containing the keyword.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is much better! üëèüèª The document we were looking for was now ranked high enough that it shows up in our retrieval step. It did so by boosting the terms used in the search query. Without the document available in our prompt context, the LLM could impossibly give us the correct answer. Let‚Äôs see both the retrieval step and generation step to see what our LLM now answers on this question:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide17.jpg\",\"width\":2112,\"height\":727,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With the correct context, we also got the correct answer!\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is the correct answer ‚úì. With the right context available to the LLM we also get the right answer. Retrieving the right context is the most important feature of our RAG: without it, the LLM can impossibly give us the right answer.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We now learned how to build a Basic RAG system and how to improve it with Hybrid Search. The data we loaded in comes from the PyData Eindhoven 2024 schedule: which was actually conveniently available in JSON format. But what about other data formats? In the real world, we can be asked to ingest other formats into our RAG like HTML, Word, and PDF.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide18.png\",\"width\":2112,\"height\":656,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Having structured data available like JSON is great, but that is not always the case in real life‚Ä¶\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Formats like HTML, Word and especially PDF can be very unpredictable in terms of structure, making it hard for us to parse them consistently. PDF documents can contain images, graphs, tables, text, basically anything. So let us take on this challenge and level up to Level 3:¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Advanced data formats\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 3: Advanced data formats\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-3-high-res.jpg\",\"width\":1024,\"height\":562,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This level revolves around ingesting challenging data formats like HTML, Word or PDF into our RAG. This requires extra considerations to do properly. For now, we will focus on¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PDF‚Äôs\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So, let us take some PDF‚Äôs as an example. I found some documents related to Space X‚Äôs Falcon 9 rocket:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Falcon 9 related documents we are going to build a RAG with.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<table style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse;\\\"><thead style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><th style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-falcon-users-guide-high-res.png\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"></th><th style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-falcon-nasa-document-high-res.png\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"></th><th style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-falcon-capabilities-overview-high-res.png\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"></th></tr></thead><tbody style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">User‚Äôs guide<span class=\\\"Apple-converted-space\\\">&nbsp;</span><a href=\\\"https://www.spacex.com/media/falcon-users-guide-2021-09.pdf\\\" style=\\\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\\\">(pdf)</a></small></td><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">Cost estimates<a href=\\\"https://www.nasa.gov/wp-content/uploads/2015/01/586023main_8-3-11_NAFCOM.pdf\\\" style=\\\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\\\"><span class=\\\"Apple-converted-space\\\">&nbsp;</span>(pdf)</a></small></td><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">Capabilities &amp; Services<span class=\\\"Apple-converted-space\\\">&nbsp;</span><a href=\\\"https://www.spacex.com/media/Capabilities&amp;Services.pdf\\\" style=\\\"box-sizing: border-box; border-width: 0px 0px 2px; border-style: solid; border-color: currentcolor currentcolor rgba(0, 0, 0, 0.05); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; color: rgb(106, 29, 87); text-decoration: inherit; transition: all 0.3s ease 0s;\\\">(pdf)</a></small></td></tr></tbody></table>\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We will now want to first parse those documents into raw text, such that we can then chunk- and embed that text. To do so, we will use a PDF parser for Python like¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"pypdf\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://pypdf.readthedocs.io/en/stable/\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Conveniently, there‚Äôs a LangChain loader available for pypdf:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide20.png\",\"width\":2112,\"height\":792,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PDF to text parsing using¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PyPDFLoader\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html#langchain_community.document_loaders.pdf.PyPDFLoader\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Using pypdf we could convert these PDF‚Äôs into¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"raw text\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Note there‚Äôs many more available, check out the¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LangChain API reference\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://api.python.langchain.com/en/latest/search.html?q=pdf\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". Both offline, local and Cloud solutions are offered like¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GCP Document AI\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://cloud.google.com/document-ai\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\",¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Azure Document Intelligence\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†or¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Amazon Textract\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://aws.amazon.com/textract/\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With such a document parsing step set up, we will need to extend our¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Document retrieval\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†step to cover this new component:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide21.png\",\"width\":2112,\"height\":959,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG retrieval with a¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"document parsing step\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", like converting PDF to raw text.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Time to test out our RAG! We embedded and ingested the raw text into our vector database and can now make queries against it. Let‚Äôs ask about the cost of a Falcon 9 rocket launch:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide22.png\",\"width\":2112,\"height\":735,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Our RAG can now answer questions about the PDF‚Äôs.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Awesome, that is the correct answer. Let‚Äôs try another question: ‚Äú\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"What is the payload I can carry with the Falcon 9 rocket to Mars?\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"‚Äú:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide23.png\",\"width\":2112,\"height\":740,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ouch! Our RAG got the answer wrong. It suggests that we can bring 8,300 kg of payload to Mars with a Falcon 9 rocket whilst in reality, this is 4,020 kg. That‚Äôs no small error.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ow! Our RAG got that answer all wrong. It suggests we can bring double the payload to Mars than what is allowed. That is pretty inconvenient, in case you were preparing for a trip to Mars üòâ.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We need to debug what went wrong. Let‚Äôs look at the context that was passed to the LLM:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide24.png\",\"width\":2112,\"height\":834,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The context provided to the LLM contains jumbled and unformatted text originating from a table. It should therefore come as no surprise that our LLM has difficulties answering questions about this table.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That explains a bunch. The context we‚Äôre passing to the LLM is hard to read and has a table encoded in a jumbled way. Like us the LLM has a hard time making sense of this. Therefore, we need to better encode this information in the prompt so our LLM can understand it.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If we want to support tables we can introduce an extra processing step. One option is to use Computer vision models to detect tables inside our documents, like¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":16,\"mode\":\"normal\",\"style\":\"\",\"text\":\"table-transformer\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/microsoft/table-transformer\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". If a table gets detected we can then give it a special treatment. What we can for example do, is encode tables in our prompt as¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Markdown\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\":\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide25.png\",\"width\":2112,\"height\":420,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"First parsing our table into a native format and then converting it to Markdown allows our LLM to much better understand the table.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Having detected the table and parsed it into a native format in our Python code allows us to then encode it in Markdown. Let‚Äôs pass that to our LLM instead and see what it answers this time:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide26.jpg\",\"width\":2112,\"height\":849,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Our RAG got the answer correct this time.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hurray! We got it correct this time. The LLM we used could easily interpret the Markdown table and pinpoint the correct value to use in answering the question. Note that still, we need to be able to retrieve the table in our retrieval step. This setup assumes we built a retrieval step that is able to retrieve the table given the user question.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"However, I have to admit something. The model we have been using for this task was GPT-3.5 turbo, which is a text-only model. Newer models have been released that can handle more than just text, which are¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Multimodal\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†models. After all, we are dealing with PDF‚Äôs which can also be seen as a series of images. Can we leverage such multimodal models to better answer our questions? Let‚Äôs find out in Level 4:¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Multimodal\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 4: Multimodal\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-4-high-res.jpg\",\"width\":1024,\"height\":562,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In this final level, we will look into leveraging the possibilities of Multimodal models. One of them is GPT-4o, which was announced May 2024:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide27.png\",\"width\":2112,\"height\":531,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GPT-4o is a multimodal model that can reason across audio, vision and text. Launched by OpenAI in May 2024.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This is a very powerful model that can understand audio, vision and text. This means we can feed it images as part of an input prompt. Given that we can in our retrieval step retrieve the right PDF pages to use, we can insert those images in the prompt and ask the LLM our original question. This has the advantage that we can understand content that was previously¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"very\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†challenging to encode in just text. Also, content we interpret and encode as text is exposed to more conversion steps exposing us to risk of information getting lost in translation.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"For example sake we will take the same table we had before but then answer the question using a Multimodal model. We can take the retrieved PDF pages encoded as images and insert them right into the prompt:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide28.png\",\"width\":2112,\"height\":874,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"With a Multimodal model, we can insert an image in the prompt and let the LLM answer questions about it.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Impressive. The LLM got the answer correct. We should be aware though, that inserting images in the prompt comes with a very different token usage than the Markdown table we inserted as text before:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-Slide29.png\",\"width\":2112,\"height\":327,\"title\":\"\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"When using Multimodal models, do be aware of the extra cost that comes with it.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is an immense increase in cost. Multimodal models can be incredibly powerful to interpret content that is otherwise very difficult to encode in text, as long as it is worth the cost ‚úì.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Concluding\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We have explored RAG in 4 levels of complexity. We went from building our first basic RAG to a RAG that leverages Multimodal models to answer questions based on complex documents. Each level introduces new complexities which are justified in each their own way. Summarising, the Levels of RAG are:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<table style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: inherit; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-indent: 0px; border-collapse: collapse;\\\"><caption class=\\\"text-center text-grey-500 text-sm\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center; font-size: 0.875rem; --tw-text-opacity: 1; color: rgb(119 131 148/var(--tw-text-opacity));\\\">The Levels of RAG, summarised.</caption><thead style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-1-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 1</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Basic RAG</th><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-2-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 2</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Hybrid Search</th><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left; padding-right: 15px;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-3-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 3</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Advanced data formats</th><th width=\\\"25%\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: left;\\\"><img decoding=\\\"async\\\" alt=\\\"alt text\\\" src=\\\"https://xebia.com/wp-content/uploads/2024/07/levels-of-rag-level-4-high-res.jpg\\\" style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: auto; border-radius: 0.5rem;\\\"><span style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: normal; font-size: 16.200001px;\\\">Level 4</span><br style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\">Multimodal</th></tr></thead><tbody style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\\\">RAG‚Äôs main steps are 1) retrieval and 2) generation. Important components to do so are Embedding, Vector Search using a Vector database, Chunking and a Large Language Model (LLM).</td><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\\\">Combining vector search and keyword search can improve retrieval performance. Sparse text search can be done using: TF-IDF and BM- 25. Reciprocal Rank Fusion can be used to merge two search engine rankings.</td><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\\\">Support formats like HTML, Word and PDF. PDF can contain images, graphs but also tables. Tables need a separate treatment, for example with Computer Vision, to then expose the table to the LLM as Markdown.</td><td style=\\\"box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246/0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top;\\\">Multimodal models can reason across audio, images and even video. Such models can help process complex data formats, for example by exposing PDF‚Äôs as images to the model. Given that the extra cost is worth the benefit, such models can be incredibly powerful.</td></tr></tbody></table>\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"RAG is a very powerful technique which can open up many new possibilities at companies. The Levels of RAG help you reason about the complexity of your RAG and allow you to understand what is difficult to do with RAG and what is easier. So: what is¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"your\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†level? ü´µ\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We wish you all the best with building your own RAG üëèüèª.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"type\":\"linebreak\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"66f82dfa47530500019e906c","uuid":"e8d197d5-dbd3-4793-93f0-9514592f9b21","title":"(Untitled)","slug":"untitled","mobiledoc":null,"html":null,"comment_id":"66f82dfa47530500019e906c","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-09-28 16:25:30","created_by":"1","updated_at":"2024-09-30 07:21:54","updated_by":"1","published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"671b6c1fe39b3b0001b04b05","uuid":"9232e52e-af67-436b-9576-8a451505e178","title":"Bookshelf","slug":"books","mobiledoc":null,"html":"<p>I like reading. These are some of the books I read.</p>\n<!--kg-card-begin: html-->\n<iframe class=\"kg-width-full\" src=\"https://dunnkers.com/bookshelf/\" width=\"100%\" height=\"7000px\" scrolling=\"no\"></iframe>\n<!--kg-card-end: html-->\n\n<!--kg-card-begin: html-->\n<!--\n      <div id=\"gr_grid_widget_1730651130\">\n  <div class=\"gr_grid_container\">\n    <div class=\"gr_grid_book_container\"><a title=\"Why Nations Fail: The Origins of Power, Prosperity, and Poverty\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/13278458-why-nations-fail\"><img alt=\"Why Nations Fail: The Origins of Power, Prosperity, and Poverty\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1333311586l/13278458._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"How to Overcome Your Childhood\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/53068003-how-to-overcome-your-childhood\"><img alt=\"How to Overcome Your Childhood\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1561811187l/53068003._SX98_SY160_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"The School of Life, An Emotional Education\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/58973006-the-school-of-life-an-emotional-education\"><img alt=\"The School of Life, An Emotional Education\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1631377503l/58973006._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Lying\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/18869177-lying\"><img alt=\"Lying\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1440069862l/18869177._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Stolen Focus: Why You Can't Pay Attention‚Äî and How to Think Deeply Again\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/57933306-stolen-focus\"><img alt=\"Stolen Focus: Why You Can't Pay Attention‚Äî and How to Think Deeply Again\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1626718328l/57933306._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"The Hitchhiker‚Äôs Guide to the Galaxy (The Hitchhiker's Guide to the Galaxy, #1)\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/386162.The_Hitchhiker_s_Guide_to_the_Galaxy\"><img alt=\"The Hitchhiker‚Äôs Guide to the Galaxy\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1559986152l/386162._SX98_SY160_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Four Thousand Weeks: Time and How to Use It\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/49785708-four-thousand-weeks\"><img alt=\"Four Thousand Weeks: Time and How to Use It\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1630118607l/49785708._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"A Little History of Philosophy\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/25230465-a-little-history-of-philosophy\"><img alt=\"A Little History of Philosophy\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1427477550l/25230465._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Why Has Nobody Told Me This Before?\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/60872124-why-has-nobody-told-me-this-before\"><img alt=\"Why Has Nobody Told Me This Before?\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1650807340l/60872124._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Moonwalking with Einstein: The Art and Science of Remembering Everything\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/43810158-moonwalking-with-einstein\"><img alt=\"Moonwalking with Einstein: The Art and Science of Remembering Everything\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1549075101l/43810158._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"River Town: Two Years on the Yangtze (P.S.)\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/9725132-river-town\"><img alt=\"River Town: Two Years on the Yangtze\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1328363688l/9725132._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Anything You Want\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/11878168-anything-you-want\"><img alt=\"Anything You Want\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1608491606l/11878168._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"How We Learn: The New Science of Education and the Brain\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/48708583-how-we-learn\"><img alt=\"How We Learn: The New Science of Education and the Brain\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1572929027l/48708583._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Man's Search for Meaning\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/9700791-man-s-search-for-meaning\"><img alt=\"Man's Search for Meaning\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1589926186l/9700791._SY160_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"1984\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/40961427-1984\"><img alt=\"1984\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1532714506l/40961427._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Has China Won?: The Chinese Challenge to American Primacy\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/48654973-has-china-won\"><img alt=\"Has China Won?: The Chinese Challenge to American Primacy\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1572575720l/48654973._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Ego Is the Enemy\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/29428533-ego-is-the-enemy\"><img alt=\"Ego Is the Enemy\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1457263853l/29428533._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Tribe: On Homecoming and Belonging\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/40940205-tribe\"><img alt=\"Tribe: On Homecoming and Belonging\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1532479315l/40940205._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Enlightenment Now\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/39669805-enlightenment-now\"><img alt=\"Enlightenment Now\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1567606505l/39669805._SX98_.jpg\" /></a></div>\n    <div class=\"gr_grid_book_container\"><a title=\"Daniel Lieberman Exercised: Why Something We Never Evolved to Do Is Healthy and Rewarding\" rel=\"nofollow\" href=\"https://www.goodreads.com/book/show/56155261-daniel-lieberman-exercised\"><img alt=\"Daniel Lieberman Exercised: Why Something We Never Evolved to Do Is Healthy and Rewarding\" border=\"0\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1623093769l/56155261._SX98_.jpg\" /></a></div>  </div>\n\n      </div>\n<script src=\"https://www.goodreads.com/review/grid_widget/88771324.Read%20books?cover_size=medium&hide_link=&hide_title=&num_books=200&order=d&shelf=read&sort=date_read&widget_id=1730651130\" type=\"text/javascript\" charset=\"utf-8\"></script>-->\n<!--kg-card-end: html-->\n<p>Check out my <a href=\"https://www.goodreads.com/review/list/88771324-jeroen?shelf=read&amp;utm_medium=api&amp;utm_source=grid_widget\">Goodreads profile</a>.</p>\n<!--kg-card-begin: html-->\n<div style=\"border: 2px solid #EBE8D5; border-radius:10px; padding: 0px 7px 0px 7px;\"><div id=\"gr_quote_body\"></div><script src=\"https://www.goodreads.com/quotes/widget/88771324-jeroen?v=2\" type=\"text/javascript\"></script><div style=\"text-align: right;\"><a href=\"https://www.goodreads.com/quotes\" style=\"color: #382110; text-decoration: none; font-size: 10px;\" rel=\"nofollow\">Goodreads Quotes</a></div></div>\n\n<!--kg-card-end: html-->\n<p></p><p>Read on! Cheers ‚ô°</p>","comment_id":"671b6c1fe39b3b0001b04b05","plaintext":"I like reading. These are some of the books I read.\n\n\n\n\n\n\n\n\n\nCheck out my Goodreads profile.\n\n\n\nGoodreads Quotes\n\n\n\n\n\n\nRead on! Cheers ‚ô°","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-10-25 09:59:59","created_by":"1","updated_at":"2024-11-24 13:35:37","updated_by":"1","published_at":"2024-11-03 16:38:18","published_by":"1","custom_excerpt":null,"codeinjection_head":"<style>\n  .shelf {\n    padding: 40px;\n    display: grid;\n    grid-gap: 30px;\n    grid-template-columns: repeat(auto-fill, minmax(100px, 1fr));\n    grid-auto-rows: 400px;\n  }\n</style>\n<style type=\"text/css\" media=\"screen\">\n    .gr_grid_container {\n        /* customize grid container div here. eg: width: 500px; */\n        height: 600px;\n    }\n\n    .gr_grid_book_container {\n        /* customize book cover container div here */\n        float: left;\n        width: 150;\n        height: 160px;\n        padding: 20px 20px;\n        overflow: hidden;\n    }\n</style>\n","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I like reading. These are some of the books I read.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<iframe class=\\\"kg-width-full\\\" src=\\\"https://dunnkers.com/bookshelf/\\\" width=\\\"100%\\\" height=\\\"7000px\\\" scrolling=\\\"no\\\"></iframe>\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"type\":\"html\",\"version\":1,\"html\":\"<!--\\n      <div id=\\\"gr_grid_widget_1730651130\\\">\\n  <div class=\\\"gr_grid_container\\\">\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Why Nations Fail: The Origins of Power, Prosperity, and Poverty\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/13278458-why-nations-fail\\\"><img alt=\\\"Why Nations Fail: The Origins of Power, Prosperity, and Poverty\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1333311586l/13278458._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"How to Overcome Your Childhood\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/53068003-how-to-overcome-your-childhood\\\"><img alt=\\\"How to Overcome Your Childhood\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1561811187l/53068003._SX98_SY160_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"The School of Life, An Emotional Education\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/58973006-the-school-of-life-an-emotional-education\\\"><img alt=\\\"The School of Life, An Emotional Education\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1631377503l/58973006._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Lying\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/18869177-lying\\\"><img alt=\\\"Lying\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1440069862l/18869177._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Stolen Focus: Why You Can't Pay Attention‚Äî and How to Think Deeply Again\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/57933306-stolen-focus\\\"><img alt=\\\"Stolen Focus: Why You Can't Pay Attention‚Äî and How to Think Deeply Again\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1626718328l/57933306._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"The Hitchhiker‚Äôs Guide to the Galaxy (The Hitchhiker's Guide to the Galaxy, #1)\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/386162.The_Hitchhiker_s_Guide_to_the_Galaxy\\\"><img alt=\\\"The Hitchhiker‚Äôs Guide to the Galaxy\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1559986152l/386162._SX98_SY160_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Four Thousand Weeks: Time and How to Use It\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/49785708-four-thousand-weeks\\\"><img alt=\\\"Four Thousand Weeks: Time and How to Use It\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1630118607l/49785708._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"A Little History of Philosophy\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/25230465-a-little-history-of-philosophy\\\"><img alt=\\\"A Little History of Philosophy\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1427477550l/25230465._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Why Has Nobody Told Me This Before?\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/60872124-why-has-nobody-told-me-this-before\\\"><img alt=\\\"Why Has Nobody Told Me This Before?\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1650807340l/60872124._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Moonwalking with Einstein: The Art and Science of Remembering Everything\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/43810158-moonwalking-with-einstein\\\"><img alt=\\\"Moonwalking with Einstein: The Art and Science of Remembering Everything\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1549075101l/43810158._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"River Town: Two Years on the Yangtze (P.S.)\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/9725132-river-town\\\"><img alt=\\\"River Town: Two Years on the Yangtze\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1328363688l/9725132._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Anything You Want\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/11878168-anything-you-want\\\"><img alt=\\\"Anything You Want\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1608491606l/11878168._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"How We Learn: The New Science of Education and the Brain\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/48708583-how-we-learn\\\"><img alt=\\\"How We Learn: The New Science of Education and the Brain\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1572929027l/48708583._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Man's Search for Meaning\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/9700791-man-s-search-for-meaning\\\"><img alt=\\\"Man's Search for Meaning\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1589926186l/9700791._SY160_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"1984\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/40961427-1984\\\"><img alt=\\\"1984\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1532714506l/40961427._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Has China Won?: The Chinese Challenge to American Primacy\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/48654973-has-china-won\\\"><img alt=\\\"Has China Won?: The Chinese Challenge to American Primacy\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1572575720l/48654973._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Ego Is the Enemy\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/29428533-ego-is-the-enemy\\\"><img alt=\\\"Ego Is the Enemy\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1457263853l/29428533._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Tribe: On Homecoming and Belonging\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/40940205-tribe\\\"><img alt=\\\"Tribe: On Homecoming and Belonging\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1532479315l/40940205._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Enlightenment Now\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/39669805-enlightenment-now\\\"><img alt=\\\"Enlightenment Now\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1567606505l/39669805._SX98_.jpg\\\" /></a></div>\\n    <div class=\\\"gr_grid_book_container\\\"><a title=\\\"Daniel Lieberman Exercised: Why Something We Never Evolved to Do Is Healthy and Rewarding\\\" rel=\\\"nofollow\\\" href=\\\"https://www.goodreads.com/book/show/56155261-daniel-lieberman-exercised\\\"><img alt=\\\"Daniel Lieberman Exercised: Why Something We Never Evolved to Do Is Healthy and Rewarding\\\" border=\\\"0\\\" src=\\\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1623093769l/56155261._SX98_.jpg\\\" /></a></div>  </div>\\n\\n      </div>\\n<script src=\\\"https://www.goodreads.com/review/grid_widget/88771324.Read%20books?cover_size=medium&hide_link=&hide_title=&num_books=200&order=d&shelf=read&sort=date_read&widget_id=1730651130\\\" type=\\\"text/javascript\\\" charset=\\\"utf-8\\\"></script>-->\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Check out my \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Goodreads profile\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://www.goodreads.com/review/list/88771324-jeroen?shelf=read&utm_medium=api&utm_source=grid_widget\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<div style=\\\"border: 2px solid #EBE8D5; border-radius:10px; padding: 0px 7px 0px 7px;\\\"><div id=\\\"gr_quote_body\\\"></div><script src=\\\"https://www.goodreads.com/quotes/widget/88771324-jeroen?v=2\\\" type=\\\"text/javascript\\\"></script><div style=\\\"text-align: right;\\\"><a href=\\\"https://www.goodreads.com/quotes\\\" style=\\\"color: #382110; text-decoration: none; font-size: 10px;\\\" rel=\\\"nofollow\\\">Goodreads Quotes</a></div></div>\\n\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Read on! Cheers ‚ô°\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"6727a23962baf50001937d5d","uuid":"44be0cd3-b640-40ce-a191-790a4627c168","title":"About me","slug":"about","mobiledoc":null,"html":"<p>Hi üëã!<br>I am Jeroen. I currently work at Xebia Data as a Machine Learning Engineer, helping out companies scale their ML products.</p><p>My background lies in Data Science, AI and Software Engineering. I get excited about technical challenges and also love to <em>tell stories</em>.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/53736787113_96c46a9b94.jpg\" width=\"1920\" height=\"1282\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/53736787113_96c46a9b94.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/53736787113_96c46a9b94.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/53736787113_96c46a9b94.jpg 1600w, __GHOST_URL__/content/images/2024/11/53736787113_96c46a9b94.jpg 1920w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/53736920004_353ddecdd6.jpg\" width=\"1920\" height=\"1282\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/53736920004_353ddecdd6.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/53736920004_353ddecdd6.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/53736920004_353ddecdd6.jpg 1600w, __GHOST_URL__/content/images/2024/11/53736920004_353ddecdd6.jpg 1920w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/Screenshot-2024-11-01-at-14.47.31.png\" width=\"2000\" height=\"1125\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/Screenshot-2024-11-01-at-14.47.31.png 600w, __GHOST_URL__/content/images/size/w1000/2024/11/Screenshot-2024-11-01-at-14.47.31.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/Screenshot-2024-11-01-at-14.47.31.png 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/Screenshot-2024-11-01-at-14.47.31.png 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/Image.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/Image.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/Image.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/Image.jpg 1600w, __GHOST_URL__/content/images/2024/11/Image.jpg 2048w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PHOTO-2024-06-16-16-20-06.jpg\" width=\"1600\" height=\"1200\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PHOTO-2024-06-16-16-20-06.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PHOTO-2024-06-16-16-20-06.jpg 1000w, __GHOST_URL__/content/images/2024/11/PHOTO-2024-06-16-16-20-06.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/Image-2.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/Image-2.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/Image-2.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/Image-2.jpg 1600w, __GHOST_URL__/content/images/2024/11/Image-2.jpg 2048w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Jeroen presenting at </span><a href=\"https://jeroenoverschie.nl/publications/\"><span style=\"white-space: pre-wrap;\">various conferences</span></a><span style=\"white-space: pre-wrap;\">.</span></p></figcaption></figure><p></p><p></p><h2 id=\"contact\">Contact</h2><ul><li>LinkedIn: <a href=\"https://linkedin.com/in/jeroenoverschie\">linkedin.com/in/jeroenoverschie</a></li><li>GitHub: <a href=\"https://github.com/dunnkers\">github.com/dunnkers</a></li></ul><p>Have a nice day!! ‚òº</p>","comment_id":"6727a23962baf50001937d5d","plaintext":"Hi üëã!\nI am Jeroen. I currently work at Xebia Data as a Machine Learning Engineer, helping out companies scale their ML products.\n\nMy background lies in Data Science, AI and Software Engineering. I get excited about technical challenges and also love to tell stories.\n\n\n\n\n\n\nContact\n\n * LinkedIn: linkedin.com/in/jeroenoverschie\n * GitHub: github.com/dunnkers\n\nHave a nice day!! ‚òº","feature_image":"__GHOST_URL__/content/images/2024/11/jeroen-overschie-profile-picture-2.jpeg","featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-11-03 16:18:01","created_by":"1","updated_at":"2024-11-08 15:57:13","updated_by":"1","published_at":"2024-11-03 16:21:07","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Hi üëã!\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"I am Jeroen. I currently work at Xebia Data as a Machine Learning Engineer, helping out companies scale their ML products.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"My background lies in Data Science, AI and Software Engineering. I get excited about technical challenges and also love to \",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"tell stories\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"gallery\",\"version\":1,\"images\":[{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/53736787113_96c46a9b94.jpg\",\"width\":1920,\"height\":1282,\"fileName\":\"53736787113_96c46a9b94_o.jpg\"},{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/53736920004_353ddecdd6.jpg\",\"width\":1920,\"height\":1282,\"alt\":\"\",\"caption\":\"\",\"fileName\":\"53736920004_353ddecdd6.jpg\"},{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/Screenshot-2024-11-01-at-14.47.31.png\",\"width\":5120,\"height\":2880,\"fileName\":\"Screenshot 2024-11-01 at 14.47.31.png\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/Image.jpg\",\"width\":2048,\"height\":1536,\"alt\":\"\",\"caption\":\"\",\"fileName\":\"Image.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/PHOTO-2024-06-16-16-20-06.jpg\",\"width\":1600,\"height\":1200,\"fileName\":\"PHOTO-2024-06-16-16-20-06.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/Image-2.jpg\",\"width\":2048,\"height\":1536,\"fileName\":\"Image 2.jpg\"}],\"caption\":\"<p><span style=\\\"white-space: pre-wrap;\\\">Jeroen presenting at </span><a href=\\\"https://jeroenoverschie.nl/publications/\\\"><span style=\\\"white-space: pre-wrap;\\\">various conferences</span></a><span style=\\\"white-space: pre-wrap;\\\">.</span></p>\"},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Contact\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LinkedIn: \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"linkedin.com/in/jeroenoverschie\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://linkedin.com/in/jeroenoverschie\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GitHub: \",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"github.com/dunnkers\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://github.com/dunnkers\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Have a nice day!! ‚òº\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"672e1aa962baf50001937de9","uuid":"fe844dbc-c090-49e4-ab66-cbc03686fc69","title":"Photos","slug":"photography","mobiledoc":null,"html":"<figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20211015_232150330.jpg\" width=\"2000\" height=\"1600\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20211015_232150330.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20211015_232150330.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20211015_232150330.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20211015_232150330.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210925_164251627.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210925_164251627.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210925_164251627.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210925_164251627.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210925_164251627.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210921_123433221.MP.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210921_123433221.MP.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210921_123433221.MP.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210921_123433221.MP.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210921_123433221.MP.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210910_225152911.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210910_225152911.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210910_225152911.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210910_225152911.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210910_225152911.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210914_222628433.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210914_222628433.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210914_222628433.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210914_222628433.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210914_222628433.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210916_231337366.NIGHT.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210916_231337366.NIGHT.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210916_231337366.NIGHT.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210916_231337366.NIGHT.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210916_231337366.NIGHT.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210921_141750124.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210921_141750124.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210921_141750124.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210921_141750124.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210921_141750124.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/PXL_20210923_142330260.MP.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/PXL_20210923_142330260.MP.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/PXL_20210923_142330260.MP.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/PXL_20210923_142330260.MP.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/PXL_20210923_142330260.MP.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div></div></figure><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0261.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0261.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0261.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0261.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0261.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0264.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0264.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0264.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0264.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0264.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0279.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0279.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0279.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0279.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0279.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0232.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0232.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0232.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0232.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0232.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0307.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0307.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0307.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0307.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0307.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0342.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0342.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0342.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0342.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0342.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0251.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0251.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0251.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0251.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0251.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0325.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0325.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0325.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0325.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0325.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMAG0262.jpg\" width=\"2000\" height=\"1120\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMAG0262.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMAG0262.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMAG0262.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMAG0262.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></div></div></div></figure><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2024/11/IMG_20211110_084601.jpg\" width=\"2000\" height=\"1500\" loading=\"lazy\" alt=\"\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/11/IMG_20211110_084601.jpg 600w, __GHOST_URL__/content/images/size/w1000/2024/11/IMG_20211110_084601.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2024/11/IMG_20211110_084601.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2024/11/IMG_20211110_084601.jpg 2400w\" sizes=\"(min-width: 1200px) 1200px\"></div></div></div></figure>","comment_id":"672e1aa962baf50001937de9","plaintext":null,"feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-11-08 14:05:29","created_by":"1","updated_at":"2024-11-23 21:48:03","updated_by":"1","published_at":"2024-11-08 14:17:57","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"type\":\"gallery\",\"version\":1,\"images\":[{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20211015_232150330.jpg\",\"width\":2819,\"height\":2255,\"fileName\":\"PXL_20211015_232150330.jpg\"},{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210925_164251627.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210925_164251627.jpg\"},{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210921_123433221.MP.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210921_123433221.MP.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210910_225152911.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210910_225152911.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210914_222628433.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210914_222628433.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210916_231337366.NIGHT.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210916_231337366.NIGHT.jpg\"},{\"row\":2,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210921_141750124.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210921_141750124.jpg\"},{\"row\":2,\"src\":\"__GHOST_URL__/content/images/2024/11/PXL_20210923_142330260.MP.jpg\",\"width\":3264,\"height\":2448,\"fileName\":\"PXL_20210923_142330260.MP.jpg\"}],\"caption\":\"\"},{\"type\":\"gallery\",\"version\":1,\"images\":[{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0261.jpg\",\"width\":4000,\"height\":2240,\"fileName\":\"IMAG0261.jpg\"},{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0264.jpg\",\"width\":4000,\"height\":2240,\"fileName\":\"IMAG0264.jpg\"},{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0279.jpg\",\"width\":4000,\"height\":2240,\"fileName\":\"IMAG0279.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0232.jpg\",\"width\":4000,\"height\":3000,\"fileName\":\"IMAG0232.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0307.jpg\",\"width\":3865,\"height\":2164,\"fileName\":\"IMAG0307.jpg\"},{\"row\":1,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0342.jpg\",\"width\":4000,\"height\":2240,\"fileName\":\"IMAG0342.jpg\"},{\"row\":2,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0251.jpg\",\"width\":4000,\"height\":3000,\"fileName\":\"IMAG0251.jpg\"},{\"row\":2,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0325.jpg\",\"width\":4000,\"height\":2240,\"alt\":\"\",\"caption\":\"\",\"fileName\":\"IMAG0325.jpg\"},{\"row\":2,\"src\":\"__GHOST_URL__/content/images/2024/11/IMAG0262.jpg\",\"width\":4000,\"height\":2240,\"fileName\":\"IMAG0262.jpg\"}],\"caption\":\"\"},{\"type\":\"gallery\",\"version\":1,\"images\":[{\"row\":0,\"src\":\"__GHOST_URL__/content/images/2024/11/IMG_20211110_084601.jpg\",\"width\":4000,\"height\":3000,\"alt\":\"\",\"caption\":\"\",\"fileName\":\"IMG_20211110_084601.jpg\"}],\"caption\":\"\"},{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1},{"id":"67f6791adc68180001657297","uuid":"f14bb5de-2db6-46c8-a8c8-fb5fb5024dba","title":"The GenAI automation potential of data extraction (on Xebia.com ‚ßâ)","slug":"the-genai-automation-potential-of-data-extraction-on-xebia-com","mobiledoc":null,"html":"<h2 id=\"introduction\">Introduction</h2><p>GenAI has been around for a little while and its capabilities are expanding quickly. We are experiencing peak-hype levels for AI and expectations are sky-high. But still, many companies fail to generate actual value with GenAI. Why is that? Why are we promised so much yet manage to get so little? What is still fantasy and what concrete potential exists? What should be automated and what should not? In this blogpost, we explore the GenAI automation potential that exists today for&nbsp;<em>data extraction</em>.</p><p>Together, we will learn about:</p><ol><li>Why GenAI data extraction</li><li>The automation levels</li><li>The automation potential</li></ol><p>Let‚Äôs start!</p><h2 id=\"1-why-genai-data-extraction\">1. Why GenAI data extraction</h2><p>Why exactly should we care about GenAI data extraction? Let us motivate this by looking at 4 example usecases in different domains and with various data types like text-, images-, documents- or audio.</p><ul><li>Insurance claims (text)</li><li>Menu cards (images)</li><li>Annual reports (PDF documents)</li><li>Customer service calls (audio)</li></ul><h3 id=\"insurance-claims-text\">Insurance claims (text)</h3><p>Imagine you are running an insurance company and your employees are tasked with processing insurance claims. Claims are often provided in a free-form format: email, phone call, chat, in short: unstructured data. Your employees will then need to process this information to handle the claim: update the internal databases, find similar claim cases, etc. Let‚Äôs take the case where we receive this information in free-form&nbsp;<strong>text</strong>&nbsp;like emails.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Victim_data_extraction_-_entire_email_box.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 1\" width=\"1908\" height=\"872\"><figcaption><span style=\"white-space: pre-wrap;\">LLMs can extract structured data from free-form text like an insurance claim, saving employees time doing it manually.&nbsp;See&nbsp;</span><a href=\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/insurance_claims_reports.ipynb\" rel=\"noopener\"><span style=\"white-space: pre-wrap;\">Code</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>We instruct the LLM to give its answers in&nbsp;<em>structured</em>&nbsp;format&nbsp;<a href=\"https://ai.google.dev/gemini-api/docs/structured-output?lang=python\" rel=\"noopener\"><sup>[ref]</sup></a>, so we can easily work with- and transform its output. To clear this entire email box with some 25 emails Gemini 2.0 Flash took some 15 seconds. That is if ran&nbsp;<em>sequentially</em>: if ran in parallel this amount of data process can be processed in a matter of seconds. If this were scaled to process a mailbox of 100k emails we would spend about $1.52. Not an awful lot given we can save some serious time here. We can also half this cost if we use Gemini‚Äôs&nbsp;<a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini\" rel=\"noopener\">Batch API</a>&nbsp;instead. How long would it take a human to process 100k emails? What is their time worth?</p><p>Extracting client information from this email message took Gemini 2.0 Flash about 0.5 seconds and cost $0.000015.</p><h3 id=\"menu-cards-images\">Menu cards (images)</h3><p>Different usecase. Imagine now you are running an online platform for food delivery. You want to digitise menu cards so the offering can be ingested in your own platform in standardised manner. You are staffing a number of employees to help you do this task. The menu cards arrive in&nbsp;<strong>image</strong>&nbsp;format. Can we speed up the menu conversion process?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Menu_card.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 2\" width=\"1893\" height=\"1108\"><figcaption><span style=\"white-space: pre-wrap;\">LLMs can read menu cards and convert them directly into a desired structured format, cutting development cost for custom- OCR and extraction solutions and potentially providing superior extraction performance.&nbsp;See&nbsp;</span><a href=\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/menu_card.ipynb\" rel=\"noopener\"><span style=\"white-space: pre-wrap;\">Code</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Yes: we can speed this up. LLMs consider the image as a whole and are also able to accurately process text details in the image. Because we are using a generic model images in many contexts can be processed by the same model without having to fine-tune over specific situations.</p><p>Converting this menu card took Gemini 2.0 Flash about 10 seconds and cost $0.0005. How long would a human take?</p><h3 id=\"annual-reports-pdf-documents\">Annual reports (PDF documents)</h3><p>Imagine you are tasked with answering questions based on financial documents. You are to go through each question and reason on the answer based on information provided in the documents. Take annual statements, for example. Companies worldwide are obliged to publish these statements and company finances are to be audited. Going through such documents is time-consuming. Take an annual statement in&nbsp;<strong>PDF document</strong>&nbsp;format. Can we help employees formulate answers based on this document faster?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Ahold_report.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 3\" width=\"1914\" height=\"945\"><figcaption><span style=\"white-space: pre-wrap;\">An entire annual report document of 382 pages fits in the model context at once, allowing the model to answer questions considering the&nbsp;</span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">whole</em></i><span style=\"white-space: pre-wrap;\">&nbsp;document. Reasoning-type of questions like audit questionnaires can now be answered by LLMs, citing where the answer is located. The human gets a head start and then only has to check and extend the answers, saving time.&nbsp;See&nbsp;</span><a href=\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/annual_report_audit.ipynb\" rel=\"noopener\"><span style=\"white-space: pre-wrap;\">Code</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Yes. The document fits in the model context&nbsp;<em>all at once</em>, allowing the model to take a comprehensive view of the document and formulate sensible answers. In fact, LLMs like Google‚Äôs Gemini models take up to 2 million tokens in context at once&nbsp;<a href=\"https://ai.google.dev/gemini-api/docs/long-context\" rel=\"noopener\"><sup>[ref]</sup></a>. This allows for up to 3,000 PDF pages to be inserted into the prompt. PDF pages are processed as images by Gemini&nbsp;<a href=\"https://ai.google.dev/gemini-api/docs/vision?lang=python\" rel=\"noopener\"><sup>[ref]</sup></a>.</p><p>Answering these questions took Gemini 2.0 Flash just under 1 minute and cost $0.015 . How long would a human take?</p><h3 id=\"customer-service-calls-audio\">Customer service calls (audio)</h3><p>Imagine you have a customer service department and are processing many calls on the daily. The customer service agents do their best to document and record useful information during the call but cannot possibly document all valuable information. Calls are temporarily recorded for analytical purposes. Calls could be listened back to figure out the customer problem, the root cause, the solution and the customer sentiment. This information is valuable to have because this is necessary feedback information to improve customer service in a targeted way. What issues were we most often unable to resolve? In what situations are our customers left unhappy?</p><p>The insights are valuable, but hard to get to. The data is hidden in an&nbsp;<strong>audio format</strong>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Customer_support_call.jpg\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 4\" width=\"1910\" height=\"1124\"><figcaption><span style=\"white-space: pre-wrap;\">LLMs can listen to audio quicker than any human. Customer service calls can be analysed and previously-inaccessible but valuable information can be extracted and taken advantage of. Insights can be gathered in automated fashion, effectively improving business processes in targeted fashion.&nbsp;See&nbsp;</span><a href=\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/annual_report_audit.ipynb\" rel=\"noopener\"><span style=\"white-space: pre-wrap;\">Code</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Gemini 2.0 Flash processed this 5-minute customer service call in 3 seconds and spent $0.0083. That is faster than any human takes to listen- and process the call. LLMs can also accurately process audio, without the need for a transcription step first. Because the model is exposed to the raw audio file, more than just the words that are spoken can be taken into account. In what&nbsp;<em>way</em>&nbsp;is this customer talking? Is this customer happy or unhappy? Off-the-shelf LLMs can be used without modification and just by prompting to extract structured data from audio&nbsp;<a href=\"https://ai.google.dev/gemini-api/docs/audio?lang=python\" rel=\"noopener\"><sup>[ref]</sup></a>.</p><p>Concluding on the 4 usecase examples, we learned the following.</p><ul><li><strong>LLMs for data extraction</strong>: LLMs can be used to extract structured data from free-form formats like text, images, PDFs, and audio.</li><li><strong>Larger contexts</strong>: Growing LLM context windows allow processing of larger documents at once. This strengthens the applicability for data extraction using LLMs without the need for extra retrieval steps.</li><li><strong>Competitive pricing</strong>: Lower inference costs make more data extraction use cases feasible.</li><li><strong>GenAI data extraction ROI</strong>: LLMs can complete data extraction tasks faster than humans and at a lower cost. Albeit not at perfection, the goal is to create a system that is good enough to assist humans in their work and provide business value.</li></ul><p>That is really cool. LLMs are becoming more capable and cheaper, making possible more usecases than before. So now what exactly is this data extraction? How can we use this to our advantage to automate business processes? What are&nbsp;<strong>the automation levels</strong>&nbsp;for data extraction?</p><h2 id=\"2-the-automation-levels\">2. The automation levels</h2><p>To discover what is possible with GenAI data extraction, we will divide into 4 levels of increasing automation.</p><ul><li><strong>Level 0: Manual</strong></li><li><strong>Level 1: Assisted</strong></li><li><strong>Level 2: Autopilot</strong></li><li><strong>Level 3: Autonomous</strong></li></ul><h3 id=\"level-0-manual-work\">Level 0: Manual work</h3><p>No automation is applied. Human labor is required to extract data in the desired format. The extracted data is then consumed by a human user. All of the data extraction-, evaluation of the data quality and any actions to be done with the data are manual human processes.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_manual_work.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 5\" width=\"1623\" height=\"427\"></figure><p>Without automation, manual work is required to extract data in structured format. After extraction humans decide on what to do with the data.</p><p>We can do better. LLMs can be used to automate part of this process, helping the human in the&nbsp;<strong>Assisted</strong>automation level.</p><h3 id=\"level-1-assisted\">Level 1: Assisted</h3><p>In the assisted workflow, a LLM is used to extract useful data. This process we like to call&nbsp;<strong>Structured Data Extraction</strong>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_Assisted.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 6\" width=\"1643\" height=\"471\"></figure><p>LLMs can be used for structured data extraction: extracting useful data from free-form documents in structured format.</p><p>That can already save a lot of time. Previously we needed to manually perform the tedious process of processing the document to formulate answers in the target format. In this level of automation, though, we are still targeting a human user as output. The human user is responsible for all of evaluating the LLM output and deciding what to do with the data. This gives the human control but also costs extra time. We can do better in&nbsp;<strong>Autopilot</strong>.</p><h3 id=\"level-2-autopilot\">Level 2: Autopilot</h3><p>In the autopilot workflow we go a step further. The extracted data is directly ingested in a&nbsp;<strong>Data Warehouse</strong>, opening up new automation possibilities. With the data in the data warehouse the data can be more efficiently taken advantage of. We can now use the data for dashboards and insights as well as use the data for Machine Learning usecases.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_The_SDE_integration_benefit.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 7\" width=\"1913\" height=\"695\"><figcaption><span style=\"white-space: pre-wrap;\">When extracted data is ingested into a Data Warehouse the data can be more efficiently taken advantage of. Dashboards can reveal new insights and ML usecases can benefit from richer available data.</span></figcaption></figure><p>Ingesting this data directly into a Data Warehouse can be beneficial indeed. But with the introduction of this automated ingestion we also need to be careful. Data Warehouse consumers are farther away from the extraction process and are not aware of the data quality. We need to always ask ourselves: Is this data reliable?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Is_this_data_reliable.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 8\" width=\"1418\" height=\"474\"><figcaption><span style=\"white-space: pre-wrap;\">Data Warehouse consumers need to always be aware of data- quality and reliability.</span></figcaption></figure><p>Bad data quality can lead to misleading insights and bad decisions later down. This is not what we want! Quantified metrics informing us on the data reliability are required. Those with expertise in the dataset and those involved in the extraction process will need to help out. These are typically an AI Engineer and a Subject Matter Expert. We need to introduce an&nbsp;<strong>evaluation</strong>&nbsp;step.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_Autopilot.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 9\" width=\"1913\" height=\"1084\"><figcaption><span style=\"white-space: pre-wrap;\">A Subject Matter Expert is required to label samples of data, informing Data Warehouse consumers and engineers on the data reliability. The Subject Matter Expert is to be enabled to also experiment with the data extraction process, lowering iteration times.</span></figcaption></figure><p>More steps are introduced, keeping the human-in-the-loop. The steps are necessary, though. The Subject Matter Expert plays a key role in ensuring the quality- and reliability of the data. This gives Data Warehouse consumers the trust they are looking for and at the same time enables the AI Engineer to more systematically improve the Structured Data Extraction system. Additionally, enabling the Subject Matter Expert themselves to be part of the prompting process lowers iteration times and reduces context being lost in translation between AI Engineer and Subject Matter Expert. Win-win.</p><p>We can go more automated, even. Let‚Äôs continue to the last level:&nbsp;<strong>Autonomous</strong>.</p><h3 id=\"level-3-autonomous\">Level 3: Autonomous</h3><p>In this level, the last human interactions are ought to be automated. Evaluation previously manually done by the Subject Matter Expert is now done by&nbsp;<strong>Quality Assurance- and evaluation</strong>&nbsp;tooling.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_Autonomous_mode.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 10\" width=\"1913\" height=\"1065\"><figcaption><span style=\"white-space: pre-wrap;\">Introducing Quality Assurance- and evaluation tooling allows the Structured Data Extraction system to run fully autonomous.</span></figcaption></figure><p>So what do we mean with such tooling? We want tooling to help us guarantee the outcome and quality of our data without minimal human intervention. This can be&nbsp;<a href=\"https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge\" rel=\"noopener\">LLM-as-a-judge systems</a>, for example. The required effort, however, to successfully implement such systems and run them safely is high. One gets value in return, from extra automation, but whether this is worth the effort- and cost is the question. Let‚Äôs compare the automation levels and summarise its potential.</p><h2 id=\"3-the-automation-potential\">3. The automation potential</h2><p>We have learned about each of the four different levels of automation for data extraction using GenAI. Also, for each level, we have explored how architecturally a Structured Data Extraction system would look like. That is great, but now where is most potential? Let‚Äôs first summarise the automation levels as follows:</p>\n<!--kg-card-begin: html-->\n<table style=\"font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: start; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; box-sizing: border-box; border: 1px solid rgb(204, 204, 204); text-indent: 0px; border-collapse: collapse; caret-color: rgb(53, 68, 90); color: rgb(53, 68, 90); font-family: Proxima, sans-serif; font-size: 18px;\"><caption class=\"text-center text-grey-500 text-sm\" style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center; font-size: 0.875rem; --tw-text-opacity: 1; color: rgb(119 131 148/var(--tw-text-opacity));\">The Automation Levels for GenAI data extraction.</caption><thead style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><th style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\">Level</th><th style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\">Automation</th><th style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\">Description</th><th style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\">Human-in-the-loop</th></tr></thead><tbody style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><tr style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">0</td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\"><b style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\">Manual</b></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Human performs all work.<br style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">Human responsible for data extraction, quality assurance and consumption of data.</small></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Yes</td></tr><tr style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">1</td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\"><b style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\">Assisted</b></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Human is assisted with tasks but still in control and responsible.<br style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">Data is extracted using a LLM. Extracted data is used to assist human users.</small></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Yes</td></tr><tr style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">2</td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\"><b style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\">Autopilot</b></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Tasks largely automated ‚Äì human supervises.<br style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">Extracted data is upserted directly into a Data Warehouse. Systematic evaluation is necessary and Subject Matter Expert involvement is key.</small></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Yes</td></tr><tr style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">3</td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\"><b style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\">Autonomous</b></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">Tasks fully automated.<br style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\"><small style=\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\">Evaluation step is to be automated using Quality Assurance- and evaluation tooling. Fully automated pipelines: no human intervention or supervision.</small></td><td style=\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\">No</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>So say you are thinking about implementing a data extraction usecase. What is the ultimate goal? The ultimate goal need not always be to automate as much as possible: it should be to create value. Because perhaps, you can benefit largely enough from your usecase if Assisted- or Autopilot automation is applied and the extra investment to full automation is not worth it. If we were to take the menu card example from earlier, how would potential time savings look like?</p><p><strong>Time savings for automating data extraction</strong>&nbsp;(example):&nbsp;<br>30 minutes (manual) ‚Üí 5 minutes (assisted) ‚Üí 1 minute (autopilot) ‚Üí 0 minutes (autonomous).</p><p>We can see that the time savings are not linear. The more automation we apply, the less time we save. Even though, the last step is the hardest and most expensive to implement. It is not always worth the effort and cost to implement this last step. These are the&nbsp;<strong>Diminishing Returns</strong>&nbsp;of automation, which can be plotted as follows:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Automation_is_nice_but_value_is_the_goal.png\" class=\"kg-image\" alt=\"alt text\" loading=\"lazy\" title=\"The GenAI automation potential of data extraction 11\" width=\"1730\" height=\"922\"></figure><p>Automation is nice but value is the goal. Take automation step-by-step. Partial automation is also valuable.</p><p></p><p>Let‚Äôs sum things up. To conclude, GenAI for data extraction is:</p><ul><li>Useful for a wide variety of usecases with various data types including text, images, PDFs and audio.</li><li>More likely to provide ROI due to 1) cheaper models, 2) larger context windows and 3) more capable models able to process multi-modal data.</li><li>Very well suited for&nbsp;<em>partial automation</em>: which can bring business value without needing extra investment for full automation.</li></ul><p>Good luck with your own GenAI data extraction usecases üçÄ‚ô°.</p>","comment_id":"67f6791adc68180001657297","plaintext":"Introduction\n\nGenAI has been around for a little while and its capabilities are expanding quickly. We are experiencing peak-hype levels for AI and expectations are sky-high. But still, many companies fail to generate actual value with GenAI. Why is that? Why are we promised so much yet manage to get so little? What is still fantasy and what concrete potential exists? What should be automated and what should not? In this blogpost, we explore the GenAI automation potential that exists today for¬†data extraction.\n\nTogether, we will learn about:\n\n 1. Why GenAI data extraction\n 2. The automation levels\n 3. The automation potential\n\nLet‚Äôs start!\n\n\n1. Why GenAI data extraction\n\nWhy exactly should we care about GenAI data extraction? Let us motivate this by looking at 4 example usecases in different domains and with various data types like text-, images-, documents- or audio.\n\n * Insurance claims (text)\n * Menu cards (images)\n * Annual reports (PDF documents)\n * Customer service calls (audio)\n\n\nInsurance claims (text)\n\nImagine you are running an insurance company and your employees are tasked with processing insurance claims. Claims are often provided in a free-form format: email, phone call, chat, in short: unstructured data. Your employees will then need to process this information to handle the claim: update the internal databases, find similar claim cases, etc. Let‚Äôs take the case where we receive this information in free-form¬†text¬†like emails.\n\nWe instruct the LLM to give its answers in¬†structured¬†format¬†[ref], so we can easily work with- and transform its output. To clear this entire email box with some 25 emails Gemini 2.0 Flash took some 15 seconds. That is if ran¬†sequentially: if ran in parallel this amount of data process can be processed in a matter of seconds. If this were scaled to process a mailbox of 100k emails we would spend about $1.52. Not an awful lot given we can save some serious time here. We can also half this cost if we use Gemini‚Äôs¬†Batch API¬†instead. How long would it take a human to process 100k emails? What is their time worth?\n\nExtracting client information from this email message took Gemini 2.0 Flash about 0.5 seconds and cost $0.000015.\n\n\nMenu cards (images)\n\nDifferent usecase. Imagine now you are running an online platform for food delivery. You want to digitise menu cards so the offering can be ingested in your own platform in standardised manner. You are staffing a number of employees to help you do this task. The menu cards arrive in¬†image¬†format. Can we speed up the menu conversion process?\n\nYes: we can speed this up. LLMs consider the image as a whole and are also able to accurately process text details in the image. Because we are using a generic model images in many contexts can be processed by the same model without having to fine-tune over specific situations.\n\nConverting this menu card took Gemini 2.0 Flash about 10 seconds and cost $0.0005. How long would a human take?\n\n\nAnnual reports (PDF documents)\n\nImagine you are tasked with answering questions based on financial documents. You are to go through each question and reason on the answer based on information provided in the documents. Take annual statements, for example. Companies worldwide are obliged to publish these statements and company finances are to be audited. Going through such documents is time-consuming. Take an annual statement in¬†PDF document¬†format. Can we help employees formulate answers based on this document faster?\n\nYes. The document fits in the model context¬†all at once, allowing the model to take a comprehensive view of the document and formulate sensible answers. In fact, LLMs like Google‚Äôs Gemini models take up to 2 million tokens in context at once¬†[ref]. This allows for up to 3,000 PDF pages to be inserted into the prompt. PDF pages are processed as images by Gemini¬†[ref].\n\nAnswering these questions took Gemini 2.0 Flash just under 1 minute and cost $0.015 . How long would a human take?\n\n\nCustomer service calls (audio)\n\nImagine you have a customer service department and are processing many calls on the daily. The customer service agents do their best to document and record useful information during the call but cannot possibly document all valuable information. Calls are temporarily recorded for analytical purposes. Calls could be listened back to figure out the customer problem, the root cause, the solution and the customer sentiment. This information is valuable to have because this is necessary feedback information to improve customer service in a targeted way. What issues were we most often unable to resolve? In what situations are our customers left unhappy?\n\nThe insights are valuable, but hard to get to. The data is hidden in an¬†audio format.\n\nGemini 2.0 Flash processed this 5-minute customer service call in 3 seconds and spent $0.0083. That is faster than any human takes to listen- and process the call. LLMs can also accurately process audio, without the need for a transcription step first. Because the model is exposed to the raw audio file, more than just the words that are spoken can be taken into account. In what¬†way¬†is this customer talking? Is this customer happy or unhappy? Off-the-shelf LLMs can be used without modification and just by prompting to extract structured data from audio¬†[ref].\n\nConcluding on the 4 usecase examples, we learned the following.\n\n * LLMs for data extraction: LLMs can be used to extract structured data from free-form formats like text, images, PDFs, and audio.\n * Larger contexts: Growing LLM context windows allow processing of larger documents at once. This strengthens the applicability for data extraction using LLMs without the need for extra retrieval steps.\n * Competitive pricing: Lower inference costs make more data extraction use cases feasible.\n * GenAI data extraction ROI: LLMs can complete data extraction tasks faster than humans and at a lower cost. Albeit not at perfection, the goal is to create a system that is good enough to assist humans in their work and provide business value.\n\nThat is really cool. LLMs are becoming more capable and cheaper, making possible more usecases than before. So now what exactly is this data extraction? How can we use this to our advantage to automate business processes? What are¬†the automation levels¬†for data extraction?\n\n\n2. The automation levels\n\nTo discover what is possible with GenAI data extraction, we will divide into 4 levels of increasing automation.\n\n * Level 0: Manual\n * Level 1: Assisted\n * Level 2: Autopilot\n * Level 3: Autonomous\n\n\nLevel 0: Manual work\n\nNo automation is applied. Human labor is required to extract data in the desired format. The extracted data is then consumed by a human user. All of the data extraction-, evaluation of the data quality and any actions to be done with the data are manual human processes.\n\nWithout automation, manual work is required to extract data in structured format. After extraction humans decide on what to do with the data.\n\nWe can do better. LLMs can be used to automate part of this process, helping the human in the¬†Assistedautomation level.\n\n\nLevel 1: Assisted\n\nIn the assisted workflow, a LLM is used to extract useful data. This process we like to call¬†Structured Data Extraction.\n\nLLMs can be used for structured data extraction: extracting useful data from free-form documents in structured format.\n\nThat can already save a lot of time. Previously we needed to manually perform the tedious process of processing the document to formulate answers in the target format. In this level of automation, though, we are still targeting a human user as output. The human user is responsible for all of evaluating the LLM output and deciding what to do with the data. This gives the human control but also costs extra time. We can do better in¬†Autopilot.\n\n\nLevel 2: Autopilot\n\nIn the autopilot workflow we go a step further. The extracted data is directly ingested in a¬†Data Warehouse, opening up new automation possibilities. With the data in the data warehouse the data can be more efficiently taken advantage of. We can now use the data for dashboards and insights as well as use the data for Machine Learning usecases.\n\nIngesting this data directly into a Data Warehouse can be beneficial indeed. But with the introduction of this automated ingestion we also need to be careful. Data Warehouse consumers are farther away from the extraction process and are not aware of the data quality. We need to always ask ourselves: Is this data reliable?\n\nBad data quality can lead to misleading insights and bad decisions later down. This is not what we want! Quantified metrics informing us on the data reliability are required. Those with expertise in the dataset and those involved in the extraction process will need to help out. These are typically an AI Engineer and a Subject Matter Expert. We need to introduce an¬†evaluation¬†step.\n\nMore steps are introduced, keeping the human-in-the-loop. The steps are necessary, though. The Subject Matter Expert plays a key role in ensuring the quality- and reliability of the data. This gives Data Warehouse consumers the trust they are looking for and at the same time enables the AI Engineer to more systematically improve the Structured Data Extraction system. Additionally, enabling the Subject Matter Expert themselves to be part of the prompting process lowers iteration times and reduces context being lost in translation between AI Engineer and Subject Matter Expert. Win-win.\n\nWe can go more automated, even. Let‚Äôs continue to the last level:¬†Autonomous.\n\n\nLevel 3: Autonomous\n\nIn this level, the last human interactions are ought to be automated. Evaluation previously manually done by the Subject Matter Expert is now done by¬†Quality Assurance- and evaluation¬†tooling.\n\nSo what do we mean with such tooling? We want tooling to help us guarantee the outcome and quality of our data without minimal human intervention. This can be¬†LLM-as-a-judge systems, for example. The required effort, however, to successfully implement such systems and run them safely is high. One gets value in return, from extra automation, but whether this is worth the effort- and cost is the question. Let‚Äôs compare the automation levels and summarise its potential.\n\n\n3. The automation potential\n\nWe have learned about each of the four different levels of automation for data extraction using GenAI. Also, for each level, we have explored how architecturally a Structured Data Extraction system would look like. That is great, but now where is most potential? Let‚Äôs first summarise the automation levels as follows:\n\n\n\nThe Automation Levels for GenAI data extraction.LevelAutomationDescriptionHuman-in-the-loop0ManualHuman performs all work.\nHuman responsible for data extraction, quality assurance and consumption of data.Yes1AssistedHuman is assisted with tasks but still in control and responsible.\nData is extracted using a LLM. Extracted data is used to assist human users.Yes2AutopilotTasks largely automated ‚Äì human supervises.\nExtracted data is upserted directly into a Data Warehouse. Systematic evaluation is necessary and Subject Matter Expert involvement is key.Yes3AutonomousTasks fully automated.\nEvaluation step is to be automated using Quality Assurance- and evaluation tooling. Fully automated pipelines: no human intervention or supervision.No\n\n\n\nSo say you are thinking about implementing a data extraction usecase. What is the ultimate goal? The ultimate goal need not always be to automate as much as possible: it should be to create value. Because perhaps, you can benefit largely enough from your usecase if Assisted- or Autopilot automation is applied and the extra investment to full automation is not worth it. If we were to take the menu card example from earlier, how would potential time savings look like?\n\nTime savings for automating data extraction¬†(example):¬†\n30 minutes (manual) ‚Üí 5 minutes (assisted) ‚Üí 1 minute (autopilot) ‚Üí 0 minutes (autonomous).\n\nWe can see that the time savings are not linear. The more automation we apply, the less time we save. Even though, the last step is the hardest and most expensive to implement. It is not always worth the effort and cost to implement this last step. These are the¬†Diminishing Returns¬†of automation, which can be plotted as follows:\n\nAutomation is nice but value is the goal. Take automation step-by-step. Partial automation is also valuable.\n\n\n\nLet‚Äôs sum things up. To conclude, GenAI for data extraction is:\n\n * Useful for a wide variety of usecases with various data types including text, images, PDFs and audio.\n * More likely to provide ROI due to 1) cheaper models, 2) larger context windows and 3) more capable models able to process multi-modal data.\n * Very well suited for¬†partial automation: which can bring business value without needing extra investment for full automation.\n\nGood luck with your own GenAI data extraction usecases üçÄ‚ô°.","feature_image":"__GHOST_URL__/content/images/2025/04/banner.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2025-04-09 13:41:46","created_by":"1","updated_at":"2025-04-09 13:45:49","updated_by":"1","published_at":"2025-04-09 13:45:49","published_by":"1","custom_excerpt":null,"codeinjection_head":"<meta http-equiv=\"refresh\" content=\"0; URL=https://xebia.com/blog/genai-automation-opportunity-sde/\" />","codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null,"lexical":"{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Introduction\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GenAI has been around for a little while and its capabilities are expanding quickly. We are experiencing peak-hype levels for AI and expectations are sky-high. But still, many companies fail to generate actual value with GenAI. Why is that? Why are we promised so much yet manage to get so little? What is still fantasy and what concrete potential exists? What should be automated and what should not? In this blogpost, we explore the GenAI automation potential that exists today for¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"data extraction\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Together, we will learn about:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Why GenAI data extraction\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The automation levels\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The automation potential\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"number\",\"start\":1,\"tag\":\"ol\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs start!\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"1. Why GenAI data extraction\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Why exactly should we care about GenAI data extraction? Let us motivate this by looking at 4 example usecases in different domains and with various data types like text-, images-, documents- or audio.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Insurance claims (text)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Menu cards (images)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Annual reports (PDF documents)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Customer service calls (audio)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":4}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Insurance claims (text)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Imagine you are running an insurance company and your employees are tasked with processing insurance claims. Claims are often provided in a free-form format: email, phone call, chat, in short: unstructured data. Your employees will then need to process this information to handle the claim: update the internal databases, find similar claim cases, etc. Let‚Äôs take the case where we receive this information in free-form¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"text\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†like emails.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Victim_data_extraction_-_entire_email_box.jpg\",\"width\":1908,\"height\":872,\"title\":\"The GenAI automation potential of data extraction 1\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">LLMs can extract structured data from free-form text like an insurance claim, saving employees time doing it manually.&nbsp;See&nbsp;</span><a href=\\\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/insurance_claims_reports.ipynb\\\" rel=\\\"noopener\\\"><span style=\\\"white-space: pre-wrap;\\\">Code</span></a><span style=\\\"white-space: pre-wrap;\\\">.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We instruct the LLM to give its answers in¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"structured\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†format¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":64,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[ref]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://ai.google.dev/gemini-api/docs/structured-output?lang=python\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", so we can easily work with- and transform its output. To clear this entire email box with some 25 emails Gemini 2.0 Flash took some 15 seconds. That is if ran¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"sequentially\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": if ran in parallel this amount of data process can be processed in a matter of seconds. If this were scaled to process a mailbox of 100k emails we would spend about $1.52. Not an awful lot given we can save some serious time here. We can also half this cost if we use Gemini‚Äôs¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Batch API\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†instead. How long would it take a human to process 100k emails? What is their time worth?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Extracting client information from this email message took Gemini 2.0 Flash about 0.5 seconds and cost $0.000015.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Menu cards (images)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Different usecase. Imagine now you are running an online platform for food delivery. You want to digitise menu cards so the offering can be ingested in your own platform in standardised manner. You are staffing a number of employees to help you do this task. The menu cards arrive in¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"image\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†format. Can we speed up the menu conversion process?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Menu_card.jpg\",\"width\":1893,\"height\":1108,\"title\":\"The GenAI automation potential of data extraction 2\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">LLMs can read menu cards and convert them directly into a desired structured format, cutting development cost for custom- OCR and extraction solutions and potentially providing superior extraction performance.&nbsp;See&nbsp;</span><a href=\\\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/menu_card.ipynb\\\" rel=\\\"noopener\\\"><span style=\\\"white-space: pre-wrap;\\\">Code</span></a><span style=\\\"white-space: pre-wrap;\\\">.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Yes: we can speed this up. LLMs consider the image as a whole and are also able to accurately process text details in the image. Because we are using a generic model images in many contexts can be processed by the same model without having to fine-tune over specific situations.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Converting this menu card took Gemini 2.0 Flash about 10 seconds and cost $0.0005. How long would a human take?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Annual reports (PDF documents)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Imagine you are tasked with answering questions based on financial documents. You are to go through each question and reason on the answer based on information provided in the documents. Take annual statements, for example. Companies worldwide are obliged to publish these statements and company finances are to be audited. Going through such documents is time-consuming. Take an annual statement in¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"PDF document\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†format. Can we help employees formulate answers based on this document faster?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Ahold_report.jpg\",\"width\":1914,\"height\":945,\"title\":\"The GenAI automation potential of data extraction 3\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">An entire annual report document of 382 pages fits in the model context at once, allowing the model to answer questions considering the&nbsp;</span><i><em class=\\\"italic\\\" style=\\\"white-space: pre-wrap;\\\">whole</em></i><span style=\\\"white-space: pre-wrap;\\\">&nbsp;document. Reasoning-type of questions like audit questionnaires can now be answered by LLMs, citing where the answer is located. The human gets a head start and then only has to check and extend the answers, saving time.&nbsp;See&nbsp;</span><a href=\\\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/annual_report_audit.ipynb\\\" rel=\\\"noopener\\\"><span style=\\\"white-space: pre-wrap;\\\">Code</span></a><span style=\\\"white-space: pre-wrap;\\\">.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Yes. The document fits in the model context¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"all at once\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", allowing the model to take a comprehensive view of the document and formulate sensible answers. In fact, LLMs like Google‚Äôs Gemini models take up to 2 million tokens in context at once¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":64,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[ref]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://ai.google.dev/gemini-api/docs/long-context\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\". This allows for up to 3,000 PDF pages to be inserted into the prompt. PDF pages are processed as images by Gemini¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":64,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[ref]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://ai.google.dev/gemini-api/docs/vision?lang=python\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Answering these questions took Gemini 2.0 Flash just under 1 minute and cost $0.015 . How long would a human take?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Customer service calls (audio)\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Imagine you have a customer service department and are processing many calls on the daily. The customer service agents do their best to document and record useful information during the call but cannot possibly document all valuable information. Calls are temporarily recorded for analytical purposes. Calls could be listened back to figure out the customer problem, the root cause, the solution and the customer sentiment. This information is valuable to have because this is necessary feedback information to improve customer service in a targeted way. What issues were we most often unable to resolve? In what situations are our customers left unhappy?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The insights are valuable, but hard to get to. The data is hidden in an¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"audio format\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Customer_support_call.jpg\",\"width\":1910,\"height\":1124,\"title\":\"The GenAI automation potential of data extraction 4\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">LLMs can listen to audio quicker than any human. Customer service calls can be analysed and previously-inaccessible but valuable information can be extracted and taken advantage of. Insights can be gathered in automated fashion, effectively improving business processes in targeted fashion.&nbsp;See&nbsp;</span><a href=\\\"https://github.com/xebia/genai-data-extraction/blob/main/notebooks/annual_report_audit.ipynb\\\" rel=\\\"noopener\\\"><span style=\\\"white-space: pre-wrap;\\\">Code</span></a><span style=\\\"white-space: pre-wrap;\\\">.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Gemini 2.0 Flash processed this 5-minute customer service call in 3 seconds and spent $0.0083. That is faster than any human takes to listen- and process the call. LLMs can also accurately process audio, without the need for a transcription step first. Because the model is exposed to the raw audio file, more than just the words that are spoken can be taken into account. In what¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"way\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†is this customer talking? Is this customer happy or unhappy? Off-the-shelf LLMs can be used without modification and just by prompting to extract structured data from audio¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":64,\"mode\":\"normal\",\"style\":\"\",\"text\":\"[ref]\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://ai.google.dev/gemini-api/docs/audio?lang=python\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Concluding on the 4 usecase examples, we learned the following.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LLMs for data extraction\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": LLMs can be used to extract structured data from free-form formats like text, images, PDFs, and audio.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Larger contexts\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": Growing LLM context windows allow processing of larger documents at once. This strengthens the applicability for data extraction using LLMs without the need for extra retrieval steps.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Competitive pricing\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": Lower inference costs make more data extraction use cases feasible.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"GenAI data extraction ROI\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": LLMs can complete data extraction tasks faster than humans and at a lower cost. Albeit not at perfection, the goal is to create a system that is good enough to assist humans in their work and provide business value.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":4}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That is really cool. LLMs are becoming more capable and cheaper, making possible more usecases than before. So now what exactly is this data extraction? How can we use this to our advantage to automate business processes? What are¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"the automation levels\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†for data extraction?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"2. The automation levels\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"To discover what is possible with GenAI data extraction, we will divide into 4 levels of increasing automation.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 0: Manual\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 1: Assisted\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 2: Autopilot\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 3: Autonomous\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":4}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 0: Manual work\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"No automation is applied. Human labor is required to extract data in the desired format. The extracted data is then consumed by a human user. All of the data extraction-, evaluation of the data quality and any actions to be done with the data are manual human processes.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_manual_work.png\",\"width\":1623,\"height\":427,\"title\":\"The GenAI automation potential of data extraction 5\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Without automation, manual work is required to extract data in structured format. After extraction humans decide on what to do with the data.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can do better. LLMs can be used to automate part of this process, helping the human in the¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Assisted\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"automation level.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 1: Assisted\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In the assisted workflow, a LLM is used to extract useful data. This process we like to call¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Structured Data Extraction\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_Assisted.png\",\"width\":1643,\"height\":471,\"title\":\"The GenAI automation potential of data extraction 6\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LLMs can be used for structured data extraction: extracting useful data from free-form documents in structured format.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"That can already save a lot of time. Previously we needed to manually perform the tedious process of processing the document to formulate answers in the target format. In this level of automation, though, we are still targeting a human user as output. The human user is responsible for all of evaluating the LLM output and deciding what to do with the data. This gives the human control but also costs extra time. We can do better in¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Autopilot\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 2: Autopilot\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In the autopilot workflow we go a step further. The extracted data is directly ingested in a¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Data Warehouse\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", opening up new automation possibilities. With the data in the data warehouse the data can be more efficiently taken advantage of. We can now use the data for dashboards and insights as well as use the data for Machine Learning usecases.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_The_SDE_integration_benefit.png\",\"width\":1913,\"height\":695,\"title\":\"The GenAI automation potential of data extraction 7\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">When extracted data is ingested into a Data Warehouse the data can be more efficiently taken advantage of. Dashboards can reveal new insights and ML usecases can benefit from richer available data.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Ingesting this data directly into a Data Warehouse can be beneficial indeed. But with the introduction of this automated ingestion we also need to be careful. Data Warehouse consumers are farther away from the extraction process and are not aware of the data quality. We need to always ask ourselves: Is this data reliable?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Is_this_data_reliable.png\",\"width\":1418,\"height\":474,\"title\":\"The GenAI automation potential of data extraction 8\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">Data Warehouse consumers need to always be aware of data- quality and reliability.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Bad data quality can lead to misleading insights and bad decisions later down. This is not what we want! Quantified metrics informing us on the data reliability are required. Those with expertise in the dataset and those involved in the extraction process will need to help out. These are typically an AI Engineer and a Subject Matter Expert. We need to introduce an¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"evaluation\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†step.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_Autopilot.png\",\"width\":1913,\"height\":1084,\"title\":\"The GenAI automation potential of data extraction 9\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">A Subject Matter Expert is required to label samples of data, informing Data Warehouse consumers and engineers on the data reliability. The Subject Matter Expert is to be enabled to also experiment with the data extraction process, lowering iteration times.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"More steps are introduced, keeping the human-in-the-loop. The steps are necessary, though. The Subject Matter Expert plays a key role in ensuring the quality- and reliability of the data. This gives Data Warehouse consumers the trust they are looking for and at the same time enables the AI Engineer to more systematically improve the Structured Data Extraction system. Additionally, enabling the Subject Matter Expert themselves to be part of the prompting process lowers iteration times and reduces context being lost in translation between AI Engineer and Subject Matter Expert. Win-win.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can go more automated, even. Let‚Äôs continue to the last level:¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Autonomous\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\".\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Level 3: Autonomous\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h3\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"In this level, the last human interactions are ought to be automated. Evaluation previously manually done by the Subject Matter Expert is now done by¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Quality Assurance- and evaluation\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†tooling.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_SDE_Autonomous_mode.png\",\"width\":1913,\"height\":1065,\"title\":\"The GenAI automation potential of data extraction 10\",\"alt\":\"alt text\",\"caption\":\"<span style=\\\"white-space: pre-wrap;\\\">Introducing Quality Assurance- and evaluation tooling allows the Structured Data Extraction system to run fully autonomous.</span>\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So what do we mean with such tooling? We want tooling to help us guarantee the outcome and quality of our data without minimal human intervention. This can be¬†\",\"type\":\"extended-text\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"LLM-as-a-judge systems\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":\"noopener\",\"target\":null,\"title\":null,\"url\":\"https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge\"},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\", for example. The required effort, however, to successfully implement such systems and run them safely is high. One gets value in return, from extra automation, but whether this is worth the effort- and cost is the question. Let‚Äôs compare the automation levels and summarise its potential.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"3. The automation potential\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h2\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We have learned about each of the four different levels of automation for data extraction using GenAI. Also, for each level, we have explored how architecturally a Structured Data Extraction system would look like. That is great, but now where is most potential? Let‚Äôs first summarise the automation levels as follows:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"html\",\"version\":1,\"html\":\"<table style=\\\"font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: start; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; box-sizing: border-box; border: 1px solid rgb(204, 204, 204); text-indent: 0px; border-collapse: collapse; caret-color: rgb(53, 68, 90); color: rgb(53, 68, 90); font-family: Proxima, sans-serif; font-size: 18px;\\\"><caption class=\\\"text-center text-grey-500 text-sm\\\" style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; text-align: center; font-size: 0.875rem; --tw-text-opacity: 1; color: rgb(119 131 148/var(--tw-text-opacity));\\\">The Automation Levels for GenAI data extraction.</caption><thead style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><th style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\\\">Level</th><th style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\\\">Automation</th><th style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\\\">Description</th><th style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; padding: 3px;\\\">Human-in-the-loop</th></tr></thead><tbody style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><tr style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">0</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\"><b style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\\\">Manual</b></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Human performs all work.<br style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">Human responsible for data extraction, quality assurance and consumption of data.</small></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Yes</td></tr><tr style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">1</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\"><b style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\\\">Assisted</b></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Human is assisted with tasks but still in control and responsible.<br style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">Data is extracted using a LLM. Extracted data is used to assist human users.</small></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Yes</td></tr><tr style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">2</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\"><b style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\\\">Autopilot</b></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Tasks largely automated ‚Äì human supervises.<br style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">Extracted data is upserted directly into a Data Warehouse. Systematic evaluation is necessary and Subject Matter Expert involvement is key.</small></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Yes</td></tr><tr style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">3</td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\"><b style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-weight: bolder;\\\">Autonomous</b></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">Tasks fully automated.<br style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ;\\\"><small style=\\\"box-sizing: border-box; border: 0px solid; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; font-size: 14.4px;\\\">Evaluation step is to be automated using Quality Assurance- and evaluation tooling. Fully automated pipelines: no human intervention or supervision.</small></td><td style=\\\"box-sizing: border-box; border: 1px solid rgb(204, 204, 204); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #3b82f680; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; vertical-align: text-top; padding: 3px;\\\">No</td></tr></tbody></table>\",\"visibility\":{\"showOnEmail\":true,\"showOnWeb\":true,\"segment\":\"\"}},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"So say you are thinking about implementing a data extraction usecase. What is the ultimate goal? The ultimate goal need not always be to automate as much as possible: it should be to create value. Because perhaps, you can benefit largely enough from your usecase if Assisted- or Autopilot automation is applied and the extra investment to full automation is not worth it. If we were to take the menu card example from earlier, how would potential time savings look like?\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Time savings for automating data extraction\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†(example):¬†\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"30 minutes (manual) ‚Üí 5 minutes (assisted) ‚Üí 1 minute (autopilot) ‚Üí 0 minutes (autonomous).\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We can see that the time savings are not linear. The more automation we apply, the less time we save. Even though, the last step is the hardest and most expensive to implement. It is not always worth the effort and cost to implement this last step. These are the¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Diminishing Returns\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"¬†of automation, which can be plotted as follows:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"type\":\"image\",\"version\":1,\"src\":\"https://xebia.com/wp-content/uploads/2025/04/genai-automation-opportunity-sde-Slide202504041048_Automation_is_nice_but_value_is_the_goal.png\",\"width\":1730,\"height\":922,\"title\":\"The GenAI automation potential of data extraction 11\",\"alt\":\"alt text\",\"caption\":\"\",\"cardWidth\":\"regular\",\"href\":\"\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Automation is nice but value is the goal. Take automation step-by-step. Partial automation is also valuable.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Let‚Äôs sum things up. To conclude, GenAI for data extraction is:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Useful for a wide variety of usecases with various data types including text, images, PDFs and audio.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"More likely to provide ROI due to 1) cheaper models, 2) larger context windows and 3) more capable models able to process multi-modal data.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":2},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Very well suited for¬†\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":2,\"mode\":\"normal\",\"style\":\"\",\"text\":\"partial automation\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\": which can bring business value without needing extra investment for full automation.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"listitem\",\"version\":1,\"value\":3}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"list\",\"version\":1,\"listType\":\"bullet\",\"start\":1,\"tag\":\"ul\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Good luck with your own GenAI data extraction usecases üçÄ‚ô°.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}","show_title_and_feature_image":1}],"posts_meta":[{"id":"62a48f659cd77522dc21c287","post_id":"62a48f659cd77522dc21c275","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"<span style=\"white-space: pre-wrap;\">This is how I can currently control my curtains: using a little remote.</span>","email_only":0},{"id":"62a48f659cd77522dc21c28a","post_id":"62a48f659cd77522dc21c276","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"The break time friend finder app. By clicking a break-hour, like 'tussen', students could see with whom they could spend their break time :).","email_only":0},{"id":"62a48f659cd77522dc21c28f","post_id":"62a48f659cd77522dc21c278","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"A <a href=\"https://dunnkers.com/disease-spread\">dashboard</a> showing potential Corona hotspots based on population data.","email_only":0},{"id":"62a48f669cd77522dc21c292","post_id":"62a48f659cd77522dc21c279","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"<span style=\"white-space: pre-wrap;\">The idea of re-training a Neural Network to insert a backdoor, causing certain predictions to be wrong.</span>","email_only":0},{"id":"62a48f669cd77522dc21c297","post_id":"62a48f659cd77522dc21c27b","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"Photo by <a href=\"https://unsplash.com/@possessedphotography?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Possessed Photography</a> / <a href=\"https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Unsplash</a>","email_only":0},{"id":"62a48f669cd77522dc21c29a","post_id":"62a48f659cd77522dc21c27c","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"Note: the people on the left are <strong>not</strong> real people. Nor is the art next to it. They were created by an AI ‚Äì to be specific, by a Generative Adversarial Network.","email_only":0},{"id":"6394beea2bf4040001d22226","post_id":"6394bbd42bf4040001d221b6","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"64989bf366b0690001afecba","post_id":"634aca22d0d59f00011b7ba1","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"Photo by <a href=\"https://unsplash.com/@v2osk?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">v2osk</a> / <a href=\"https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Unsplash</a>","email_only":0},{"id":"661bb37831b02500019906a4","post_id":"661bb31631b0250001990697","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"<span style=\"white-space: pre-wrap;\">Photo by Ilo Frey: </span><a href=\"https://www.pexels.com/photo/photo-of-yellow-and-blue-macaw-with-one-wing-open-perched-on-a-wooden-stick-2317904/\"><span style=\"white-space: pre-wrap;\">https://www.pexels.com/photo/photo-of-yellow-and-blue-macaw-with-one-wing-open-perched-on-a-wooden-stick-2317904/</span></a>","email_only":0},{"id":"6727a8a062baf50001937db6","post_id":"6727a23962baf50001937d5d","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"<span style=\"white-space: pre-wrap;\">Jeroen Overschie</span>","email_only":0},{"id":"67408a404ffcb50001b16e38","post_id":"62a48f659cd77522dc21c27f","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"<span style=\"white-space: pre-wrap;\">Photo by </span><a href=\"https://unsplash.com/@nicoiseli?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit\"><span style=\"white-space: pre-wrap;\">Nico</span></a><span style=\"white-space: pre-wrap;\"> / </span><a href=\"https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit\"><span style=\"white-space: pre-wrap;\">Unsplash</span></a>","email_only":0},{"id":"6740949d4ffcb50001b16e98","post_id":"66f6ae120aba3300014c3c45","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"<span style=\"white-space: pre-wrap;\">Photo by </span><a href=\"https://unsplash.com/@voznenko_artur?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit\"><span style=\"white-space: pre-wrap;\">Artur Voznenko</span></a><span style=\"white-space: pre-wrap;\"> / </span><a href=\"https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit\"><span style=\"white-space: pre-wrap;\">Unsplash</span></a>","email_only":0}],"users":[{"id":"1","name":"Jeroen Overschie","slug":"jeroen","password":"$2a$10$1Me.YVV/yWm/bSp.MYctbOvMiyNaj64luh8sFp7qEC0RFfuvIXYfq","email":"jeroenoverschie@gmail.com","profile_image":"https://www.gravatar.com/avatar/b37b136916ade32aada1f345482aafa4?s=250&r=x&d=mp","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"whatsNew\":{\"lastSeenDate\":\"2025-04-08T13:54:15.000+00:00\"},\"nightShift\":false}","status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-04-09 13:40:46","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2025-04-09 13:40:53","updated_by":"1","comment_notifications":1,"free_member_signup_notification":1,"paid_subscription_canceled_notification":0,"paid_subscription_started_notification":1,"mention_notifications":1,"milestone_notifications":1,"donation_notifications":1,"recommendation_notifications":1}],"posts_authors":[{"id":"62a48f659cd77522dc21c286","post_id":"62a48f659cd77522dc21c275","author_id":"1","sort_order":0},{"id":"62a48f659cd77522dc21c289","post_id":"62a48f659cd77522dc21c276","author_id":"1","sort_order":0},{"id":"62a48f659cd77522dc21c28b","post_id":"62a48f659cd77522dc21c277","author_id":"1","sort_order":0},{"id":"62a48f659cd77522dc21c28e","post_id":"62a48f659cd77522dc21c278","author_id":"1","sort_order":0},{"id":"62a48f669cd77522dc21c291","post_id":"62a48f659cd77522dc21c279","author_id":"1","sort_order":0},{"id":"62a48f669cd77522dc21c294","post_id":"62a48f659cd77522dc21c27a","author_id":"1","sort_order":0},{"id":"62a48f669cd77522dc21c296","post_id":"62a48f659cd77522dc21c27b","author_id":"1","sort_order":0},{"id":"62a48f669cd77522dc21c299","post_id":"62a48f659cd77522dc21c27c","author_id":"1","sort_order":0},{"id":"62a48f669cd77522dc21c29c","post_id":"62a48f659cd77522dc21c27d","author_id":"1","sort_order":0},{"id":"62a48f669cd77522dc21c29e","post_id":"62a48f659cd77522dc21c27f","author_id":"1","sort_order":0},{"id":"634aca22d0d59f00011b7ba2","post_id":"634aca22d0d59f00011b7ba1","author_id":"1","sort_order":0},{"id":"636fdea4f90fec00015b93df","post_id":"636fdea4f90fec00015b93de","author_id":"1","sort_order":0},{"id":"6394bbd42bf4040001d221b7","post_id":"6394bbd42bf4040001d221b6","author_id":"1","sort_order":0},{"id":"6394c3a02bf4040001d22236","post_id":"6394c3a02bf4040001d22235","author_id":"1","sort_order":0},{"id":"65993233cdf9710001822254","post_id":"65993233cdf9710001822253","author_id":"1","sort_order":0},{"id":"65993347cdf971000182225f","post_id":"65993347cdf971000182225e","author_id":"1","sort_order":0},{"id":"65993426cdf971000182226a","post_id":"65993426cdf9710001822269","author_id":"1","sort_order":0},{"id":"661bb31731b0250001990698","post_id":"661bb31631b0250001990697","author_id":"1","sort_order":0},{"id":"66f6ae120aba3300014c3c46","post_id":"66f6ae120aba3300014c3c45","author_id":"1","sort_order":0},{"id":"66f6b3910aba3300014c3c68","post_id":"66f6b3910aba3300014c3c67","author_id":"1","sort_order":0},{"id":"66f82dfa47530500019e906d","post_id":"66f82dfa47530500019e906c","author_id":"1","sort_order":0},{"id":"671b6c1fe39b3b0001b04b06","post_id":"671b6c1fe39b3b0001b04b05","author_id":"1","sort_order":0},{"id":"6727a23a62baf50001937d5e","post_id":"6727a23962baf50001937d5d","author_id":"1","sort_order":0},{"id":"672e1aa962baf50001937dea","post_id":"672e1aa962baf50001937de9","author_id":"1","sort_order":0},{"id":"67f6791adc68180001657298","post_id":"67f6791adc68180001657297","author_id":"1","sort_order":0}],"roles":[{"id":"62a48eba9cd77522dc21c089","name":"Administrator","description":"Administrators","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c08a","name":"Editor","description":"Editors","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c08b","name":"Author","description":"Authors","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c08c","name":"Contributor","description":"Contributors","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c08d","name":"Owner","description":"Blog Owner","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c08e","name":"Admin Integration","description":"External Apps","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c08f","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"62a48eba9cd77522dc21c090","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2022-06-11 12:46:50","created_by":"1","updated_at":"2022-06-11 12:46:50","updated_by":"1"},{"id":"634ab18836bd9373d3d52c19","name":"Ghost Explore Integration","description":"Internal Integration for the Ghost Explore directory","created_at":"2022-10-15 13:11:36","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bce","name":"Self-Serve Migration Integration","description":"Core Integration for the Ghost Explore directory","created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null}],"roles_users":[{"id":"62a48eba9cd77522dc21c091","role_id":"62a48eba9cd77522dc21c08d","user_id":"1"}],"settings":[{"id":"62a48ebb9cd77522dc21c207","group":"core","key":"db_hash","value":"90691f48-39c3-4adb-a3af-c542473f4a08","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c208","group":"core","key":"routes_hash","value":"3d180d52c663d173a6be791ef411ed01","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c209","group":"core","key":"next_update_check","value":"1744292446","type":"number","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2025-04-09 13:40:46","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c20a","group":"core","key":"notifications","value":"[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"64989cbd66b0690001afecbc\",\"createdAtVersion\":\"5.19.3\",\"type\":\"warn\",\"message\":\"Ghost is currently unable to send email. See https://ghost.org/docs/concepts/config/#mail for instructions.\",\"seen\":false,\"addedAt\":\"2023-06-25T19:59:57.981Z\"},{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"0d88bf51-ab48-4a7f-a867-e5096e0026ad\",\"createdAtVersion\":\"5.19.3\",\"custom\":true,\"createdAt\":\"2024-08-21T12:01:19.000Z\",\"type\":\"alert\",\"top\":false,\"message\":\"Critical security update available ‚Äî please update Ghost as soon as possible. <a href=\\\"https://github.com/TryGhost/Ghost/security/advisories/GHSA-78x2-cwp9-5j42\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Details here.</a>\",\"seen\":false,\"addedAt\":\"2024-09-27T12:11:54.465Z\"}]","type":"array","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2024-09-27 12:11:54","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c20b","group":"core","key":"version_notifications","value":"[]","type":"array","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c20c","group":"core","key":"admin_session_secret","value":"d18f1ca0021f20696eab6c5cd4f172c548d233cf8196d31ebdc6a3924abe6da4","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c20d","group":"core","key":"theme_session_secret","value":"913f28f4911a4b8409b7c5dbf2fc6739c5238e9e64ef3dd42fa044b24a01cb1c","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c20e","group":"core","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAKO48nyBnRvj+NtWe12UXeNBvSNY+vbky2Z5Aefu8uriQC9XBzYdGfhgW/V+re+B\nd02tvs0fxpxR6o/JQJ577JI7YZNtwZJWjZ1RsAMzNohyEg3+vQE2VrBGTEoleziy+wsZd+lO\nttuDy3fQkEiz0KOQkRO5JLcSqdiZUFjuMoNDAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c20f","group":"core","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXgIBAAKBgQCjuPJ8gZ0b4/jbVntdlF3jQb0jWPr25MtmeQHn7vLq4kAvVwc2HRn4YFv1\nfq3vgXdNrb7NH8acUeqPyUCee+ySO2GTbcGSVo2dUbADMzaIchIN/r0BNlawRkxKJXs4svsL\nGXfpTrbbg8t30JBIs9CjkJETuSS3EqnYmVBY7jKDQwIDAQABAoGAa3qLfV7dU6TqBpuUaw9u\nPzU1xBGy1wfF22SO8sJzp+yVdD5uloCuPtaLJ/NcngFg35ayzhgRGyfPk0rr5960oxBt1d6x\npj7hwir04wg1awLkx1rREyUorA0cVfSZazrjah5M1ASqJqmFghAqkfEpJBLT2y2GcbQwH1GG\nC317BjkCQQDk4mplamMBsWq6CWbue41Ubdjbfvoy7njVUZq7mIqcldYjSTFgk6ev60R4wmTX\ncMUvcVfT6en7KbcVGgeqs/nfAkEAtx5OLVw7Kdnvq8ZBFFiFGHdGfDbz5MIWt0eg0uf4irVE\nqQZza+JiQoPG5QfvRm6nXnbvh6hNoerk7TWIK4FrHQJBAAvv6xCi/crmz+Qn/WBOvU479GVu\nN+pUGaU2flVuXTxRbDum45Zf3Q0Fvip2KQA7d21EAgqhVnys7kmBdAjpHg0CQQCiB0G7Z0G+\nbWVhw+Gv5AeYt2l53ZH/FzHMaKfIFpPYAD7JpLiafEzfeASUgWnaE20q6+hUS7qti8+WiOh9\nPl2NAkEAk/lGxlpXADs0jVKJvDB9Gj+0V1GN2RznNtQsUHF8M3sHPDR8wapP1aeGpq0Jme2B\nlGSUkHTYBVSqO6Spjsa/QQ==\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c210","group":"core","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBANXwRLRMprn6r1TZqvr4A+h7aun9tNzVNN/0NM0eM8AdVpfiJD60NQKtTF8mD1/N\ngV71aEZw0MqUWXugN/sCyDOQboy655lfu0z48oV910Hwtc2XzUfrKPTxqyjWNNpS+282qQii\nV09gZgcWzHNNUXh50/YzJvk8sam1/Qcl125VAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c211","group":"core","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQDV8ES0TKa5+q9U2ar6+APoe2rp/bTc1TTf9DTNHjPAHVaX4iQ+tDUCrUxf\nJg9fzYFe9WhGcNDKlFl7oDf7AsgzkG6MuueZX7tM+PKFfddB8LXNl81H6yj08aso1jTaUvtv\nNqkIoldPYGYHFsxzTVF4edP2Myb5PLGptf0HJdduVQIDAQABAoGBAJucQQz3+Amsmp1YGfKk\nNYuDQbfjDwvVlLkVEtbjkfa6IEMnfP+S6kABN1y5/VLM0r30OJ2L74J6N0AhwLY2RtFNJCoM\nwmEwNC9iy17NeyqC13kONeLkka23YHnQhALpdjmUizhVvwwScQiUkR2OBTBtuJB1pQZb9Mn8\nvl4NT0NdAkEA/pVt5jXUdSa6mVahQK0MC/VV3MKp0qL5iNPNCvpYZiwU50aOdkFuGl/n6HgN\nXPgOlD3FJb35W4/4i4gZLy2qLwJBANcg9BPoHLq715N87Z3LR1IDE7AR8/MLvuE9C6Fk8n2h\nxNUifW6kzBGWS2cYmThChdK9+szA8zM45HByY/UkQrsCQQAJQce4Ojbad6kLUFIWtvQcLzSL\nDWz9Yr2uEv1+q7GxLWMpMbCWbjShsuEM2+ioe8CT9VcI00qQ4MBJ2o4H4CIzAj8daWP1VMaY\nwRW4FFxoNmKJ0+HdMJcpo3F1WeM9LY/5nSRL/2smtWExBltIvRQ1nOKu7UpctASL/Ds/JGSG\nH28CQQCeI0/hBiG3CuvkScRMperFy9HlpGoDZ14B9ejaYSdES3CCS3meOmpnaBzzbtFmNFjs\nLwrrx4qJY6bSrf5LWnun\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c212","group":"core","key":"members_email_auth_secret","value":"b38e4c2c7c32769ff1cc9690f502bd09c297bb9e50df985420ce63ea6eb4ed832f644d25c23e3698a40d04ddfb413e335d949eccc5bb7174c19acf2f7d998619","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c215","group":"site","key":"title","value":"Jeroen Overschie","type":"string","flags":"PUBLIC","created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:37:26","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c216","group":"site","key":"description","value":"This is Jeroen's personal website.","type":"string","flags":"PUBLIC","created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-11-08 14:02:22","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c217","group":"site","key":"logo","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c218","group":"site","key":"cover_image","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-28 13:46:24","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c219","group":"site","key":"icon","value":"__GHOST_URL__/content/images/2022/10/cartoon-head-jeroen-1-2.svg","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2022-10-22 11:08:28","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c21a","group":"site","key":"accent_color","value":"#FF1A75","type":"string","flags":"PUBLIC","created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c21b","group":"site","key":"locale","value":"en","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c21c","group":"site","key":"timezone","value":"Europe/Amsterdam","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 13:38:15","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c21d","group":"site","key":"codeinjection_head","value":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-PQB9198LQ6\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-PQB9198LQ6');\n</script>","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-01-19 10:18:49","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c21e","group":"site","key":"codeinjection_foot","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c21f","group":"site","key":"facebook","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 13:38:15","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c220","group":"site","key":"twitter","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 13:38:15","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c221","group":"site","key":"navigation","value":"[{\"url\":\"/\",\"label\":\"Home\"},{\"url\":\"/publications/\",\"label\":\"Publications\"},{\"url\":\"/about/\",\"label\":\"About me\"}]","type":"array","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-11-08 15:56:34","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c222","group":"site","key":"secondary_navigation","value":"[{\"url\":\"/books/\",\"label\":\"Bookshelf\"},{\"url\":\"/photography/\",\"label\":\"Photography\"},{\"url\":\"/about/#contact\",\"label\":\"Contact\"}]","type":"array","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-11-08 15:56:16","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c223","group":"site","key":"meta_title","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c224","group":"site","key":"meta_description","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c225","group":"site","key":"og_image","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c226","group":"site","key":"og_title","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c227","group":"site","key":"og_description","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c228","group":"site","key":"twitter_image","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c229","group":"site","key":"twitter_title","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c22a","group":"site","key":"twitter_description","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c22b","group":"theme","key":"active_theme","value":"dunnkers-theme-edition","type":"string","flags":"RO","created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2024-11-08 13:57:34","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c22c","group":"private","key":"is_private","value":"false","type":"boolean","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c22d","group":"private","key":"password","value":"","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c22e","group":"private","key":"public_hash","value":"30a95d83845c7d4b9115d829afb88a","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c22f","group":"members","key":"default_content_visibility","value":"public","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c230","group":"members","key":"default_content_visibility_tiers","value":"[]","type":"array","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c231","group":"members","key":"members_signup_access","value":"none","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-28 13:47:24","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c232","group":"members","key":"members_support_address","value":"noreply","type":"string","flags":"PUBLIC,RO","created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c235","group":"members","key":"stripe_plans","value":"[]","type":"array","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c238","group":"members","key":"stripe_connect_livemode","value":null,"type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c239","group":"members","key":"stripe_connect_display_name","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c23b","group":"members","key":"members_monthly_price_id","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c23c","group":"members","key":"members_yearly_price_id","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c23d","group":"portal","key":"portal_name","value":"true","type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c23e","group":"portal","key":"portal_button","value":"false","type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 12:03:54","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c23f","group":"portal","key":"portal_plans","value":"[\"free\"]","type":"array","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c240","group":"portal","key":"portal_products","value":"[]","type":"array","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c241","group":"portal","key":"portal_button_style","value":"icon-and-text","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c242","group":"portal","key":"portal_button_icon","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c243","group":"portal","key":"portal_button_signup_text","value":"Subscribe","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c244","group":"email","key":"mailgun_domain","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c245","group":"email","key":"mailgun_api_key","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c246","group":"email","key":"mailgun_base_url","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c247","group":"email","key":"email_track_opens","value":"false","type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-09-27 12:56:12","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c249","group":"amp","key":"amp","value":"true","type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c24a","group":"amp","key":"amp_gtag_id","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c24b","group":"firstpromoter","key":"firstpromoter","value":"false","type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c24c","group":"firstpromoter","key":"firstpromoter_id","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c24d","group":"labs","key":"labs","value":"{}","type":"object","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c24e","group":"slack","key":"slack_url","value":"","type":"string","flags":null,"created_at":"2022-06-11 12:46:51","created_by":"1","updated_at":"2022-06-11 12:46:51","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c24f","group":"slack","key":"slack_username","value":"Ghost","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2022-01-06 13:37:30","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c250","group":"unsplash","key":"unsplash","value":"true","type":"boolean","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c251","group":"views","key":"shared_views","value":"[]","type":"array","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2021-11-26 11:15:42","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c252","group":"editor","key":"editor_default_email_recipients","value":"disabled","type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-09-27 12:56:24","updated_by":"1"},{"id":"62a48ebb9cd77522dc21c253","group":"editor","key":"editor_default_email_recipients_filter","value":null,"type":"string","flags":null,"created_at":"2021-11-26 11:15:42","created_by":"1","updated_at":"2024-09-27 12:56:24","updated_by":"1"},{"id":"634ab18836bd9373d3d52c18","group":"comments","key":"comments_enabled","value":"off","type":"string","flags":null,"created_at":"2022-10-15 13:11:36","created_by":"1","updated_at":null,"updated_by":null},{"id":"634ab18c36bd9373d3d52c23","group":"email","key":"email_track_clicks","value":"false","type":"boolean","flags":null,"created_at":"2022-10-15 13:11:40","created_by":"1","updated_at":"2024-09-27 12:56:12","updated_by":"1"},{"id":"66f6a2160aba3300014c3bc3","group":"members","key":"members_track_sources","value":"false","type":"boolean","flags":null,"created_at":"2024-09-27 12:16:22","created_by":"1","updated_at":"2024-09-27 12:56:12","updated_by":"1"},{"id":"66f6a2170aba3300014c3bc9","group":"analytics","key":"outbound_link_tagging","value":"false","type":"boolean","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":"2024-09-27 12:56:12","updated_by":"1"},{"id":"66f6a2170aba3300014c3bcd","group":"core","key":"last_mentions_report_email_timestamp","value":null,"type":"number","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bd4","group":"portal","key":"portal_signup_terms_html","value":null,"type":"string","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bd5","group":"portal","key":"portal_signup_checkbox_required","value":"false","type":"boolean","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bd6","group":"announcement","key":"announcement_content","value":null,"type":"string","flags":"PUBLIC","created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bd7","group":"announcement","key":"announcement_background","value":"dark","type":"string","flags":"PUBLIC","created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bd8","group":"pintura","key":"pintura","value":"true","type":"boolean","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bd9","group":"pintura","key":"pintura_js_url","value":null,"type":"string","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bda","group":"pintura","key":"pintura_css_url","value":null,"type":"string","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2170aba3300014c3bdb","group":"announcement","key":"announcement_visibility","value":"[]","type":"array","flags":null,"created_at":"2024-09-27 12:16:23","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2180aba3300014c3c0b","group":"donations","key":"donations_currency","value":"USD","type":"string","flags":null,"created_at":"2024-09-27 12:16:24","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2180aba3300014c3c0c","group":"donations","key":"donations_suggested_amount","value":"500","type":"number","flags":null,"created_at":"2024-09-27 12:16:24","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2180aba3300014c3c22","group":"recommendations","key":"recommendations_enabled","value":"false","type":"boolean","flags":null,"created_at":"2024-09-27 12:16:24","created_by":"1","updated_at":null,"updated_by":null},{"id":"66f6a2190aba3300014c3c29","group":"portal","key":"portal_default_plan","value":"yearly","type":"string","flags":null,"created_at":"2024-09-27 12:16:25","created_by":"1","updated_at":null,"updated_by":null}],"tags":[{"id":"62a48f659cd77522dc21c270","name":"Data Science","slug":"data-science","description":"All things Data Science. Machine Learning, Deep Learning and other cool stuff üôèüèª.","feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-11-29 19:30:44","created_by":"1","updated_at":"2021-12-01 14:55:16","updated_by":null},{"id":"62a48f659cd77522dc21c271","name":"Software Engineering","slug":"software-engineering","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-01 14:55:40","created_by":"1","updated_at":"2021-12-01 14:55:40","updated_by":null},{"id":"62a48f659cd77522dc21c272","name":"Internet of Things","slug":"internet-of-things","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-01 14:57:07","created_by":"1","updated_at":"2021-12-01 14:57:07","updated_by":null},{"id":"6353bc2598da0600016c9d95","name":"Code snippets","slug":"code-snippets","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-10-22 09:47:17","created_by":"1","updated_at":"2022-10-22 10:07:02","updated_by":"1"},{"id":"6353c3be98da0600016c9dc9","name":"#noindex","slug":"hash-noindex","description":"Post is not visible on the main page.","feature_image":null,"parent_id":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-10-22 10:19:42","created_by":"1","updated_at":"2022-10-22 10:19:42","updated_by":"1"},{"id":"6394c4072bf4040001d2223c","name":"Publications","slug":"publications","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-12-10 17:38:15","created_by":"1","updated_at":"2022-12-10 17:38:15","updated_by":"1"}],"posts_tags":[{"id":"62a48f659cd77522dc21c285","post_id":"62a48f659cd77522dc21c275","tag_id":"62a48f659cd77522dc21c272","sort_order":0},{"id":"62a48f659cd77522dc21c288","post_id":"62a48f659cd77522dc21c276","tag_id":"62a48f659cd77522dc21c271","sort_order":0},{"id":"62a48f659cd77522dc21c28c","post_id":"62a48f659cd77522dc21c278","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"62a48f659cd77522dc21c28d","post_id":"62a48f659cd77522dc21c278","tag_id":"62a48f659cd77522dc21c271","sort_order":1},{"id":"62a48f669cd77522dc21c290","post_id":"62a48f659cd77522dc21c279","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"62a48f669cd77522dc21c293","post_id":"62a48f659cd77522dc21c27a","tag_id":"62a48f659cd77522dc21c271","sort_order":0},{"id":"62a48f669cd77522dc21c295","post_id":"62a48f659cd77522dc21c27b","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"62a48f669cd77522dc21c298","post_id":"62a48f659cd77522dc21c27c","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"62a48f669cd77522dc21c29b","post_id":"62a48f659cd77522dc21c27d","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"6353bc2598da0600016c9d96","post_id":"634aca22d0d59f00011b7ba1","tag_id":"6353bc2598da0600016c9d95","sort_order":0},{"id":"6394c4072bf4040001d2223e","post_id":"6394c3a02bf4040001d22235","tag_id":"6394c4072bf4040001d2223c","sort_order":0},{"id":"659937b3cdf971000182227b","post_id":"65993347cdf971000182225e","tag_id":"62a48f659cd77522dc21c271","sort_order":0},{"id":"659937b9cdf971000182227d","post_id":"65993426cdf9710001822269","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"659937c7cdf971000182227f","post_id":"6394c3a02bf4040001d22235","tag_id":"62a48f659cd77522dc21c271","sort_order":1},{"id":"661bb49131b02500019906c9","post_id":"661bb31631b0250001990697","tag_id":"62a48f659cd77522dc21c270","sort_order":0},{"id":"66f6b45f0aba3300014c3c71","post_id":"66f6b3910aba3300014c3c67","tag_id":"62a48f659cd77522dc21c270","sort_order":0}],"products":[{"id":"62a48eba9cd77522dc21c092","name":"Free","slug":"free","active":1,"welcome_page_url":null,"visibility":"public","monthly_price_id":null,"yearly_price_id":null,"description":null,"type":"free","created_at":"2022-06-11 12:46:50","updated_at":"2022-06-11 12:46:50","trial_days":0,"monthly_price":null,"yearly_price":null,"currency":null},{"id":"62a48eba9cd77522dc21c093","name":"Jeroen Overschie","slug":"default-product","active":1,"welcome_page_url":null,"visibility":"public","monthly_price_id":null,"yearly_price_id":null,"description":null,"type":"paid","created_at":"2022-06-11 12:46:50","updated_at":"2022-06-11 12:47:41","trial_days":0,"monthly_price":500,"yearly_price":5000,"currency":"usd"}],"offers":[],"benefits":[],"products_benefits":[],"posts_products":[{"id":"6353b96b98da0600016c9d8a","post_id":"634aca22d0d59f00011b7ba1","product_id":"62a48eba9cd77522dc21c092","sort_order":0},{"id":"6353b96b98da0600016c9d8b","post_id":"634aca22d0d59f00011b7ba1","product_id":"62a48eba9cd77522dc21c093","sort_order":1},{"id":"636fdee2f90fec00015b93e3","post_id":"636fdea4f90fec00015b93de","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"6394bbdb2bf4040001d221bb","post_id":"6394bbd42bf4040001d221b6","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"6394c3b02bf4040001d2223a","post_id":"6394c3a02bf4040001d22235","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"65993240cdf9710001822258","post_id":"65993233cdf9710001822253","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"6599336fcdf9710001822263","post_id":"65993347cdf971000182225e","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"65993433cdf971000182226d","post_id":"65993426cdf9710001822269","product_id":"62a48eba9cd77522dc21c092","sort_order":0},{"id":"65993433cdf971000182226e","post_id":"65993426cdf9710001822269","product_id":"62a48eba9cd77522dc21c093","sort_order":1},{"id":"65aa4e93ed854c00018fc643","post_id":"62a48f659cd77522dc21c277","product_id":"62a48eba9cd77522dc21c092","sort_order":0},{"id":"65aa4e93ed854c00018fc644","post_id":"62a48f659cd77522dc21c277","product_id":"62a48eba9cd77522dc21c093","sort_order":1},{"id":"661bb32631b025000199069c","post_id":"661bb31631b0250001990697","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"66f6ae170aba3300014c3c49","post_id":"66f6ae120aba3300014c3c45","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"66f6b3a20aba3300014c3c6b","post_id":"66f6b3910aba3300014c3c67","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"66f82dfb47530500019e9070","post_id":"66f82dfa47530500019e906c","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"671b6b9ce39b3b0001b04b01","post_id":"62a48f659cd77522dc21c27f","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"671b6c22e39b3b0001b04b09","post_id":"671b6c1fe39b3b0001b04b05","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"6727a24e62baf50001937d61","post_id":"6727a23962baf50001937d5d","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"672e1ac662baf50001937ded","post_id":"672e1aa962baf50001937de9","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"674088e64ffcb50001b16e22","post_id":"62a48f659cd77522dc21c275","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"674088fa4ffcb50001b16e2a","post_id":"62a48f659cd77522dc21c27d","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"674088ff4ffcb50001b16e2e","post_id":"62a48f659cd77522dc21c279","product_id":"62a48eba9cd77522dc21c093","sort_order":0},{"id":"67f67955dc6818000165729b","post_id":"67f6791adc68180001657297","product_id":"62a48eba9cd77522dc21c093","sort_order":0}],"offer_redemptions":[],"stripe_prices":[],"snippets":[],"custom_theme_settings":[{"id":"62a48ebb9cd77522dc21c258","theme":"casper","key":"title_font","type":"select","value":"Modern sans-serif"},{"id":"62a48ebb9cd77522dc21c259","theme":"casper","key":"body_font","type":"select","value":"Elegant serif"},{"id":"62a48ebb9cd77522dc21c25a","theme":"casper","key":"show_publication_cover","type":"boolean","value":"true"},{"id":"62a48ebb9cd77522dc21c25b","theme":"casper","key":"header_style","type":"select","value":"Center aligned"},{"id":"62a48ebb9cd77522dc21c25c","theme":"casper","key":"show_logo_in_navigation","type":"boolean","value":"false"},{"id":"62a48ebb9cd77522dc21c25d","theme":"casper","key":"feed_layout","type":"select","value":"Classic"},{"id":"62a48ebb9cd77522dc21c25e","theme":"casper","key":"color_scheme","type":"select","value":"Light"},{"id":"62a48ebb9cd77522dc21c25f","theme":"casper","key":"post_image_style","type":"select","value":"Wide"},{"id":"62a48ebb9cd77522dc21c260","theme":"casper","key":"email_signup_text","type":"text","value":"Sign up for more like this."},{"id":"62a48ebb9cd77522dc21c261","theme":"casper","key":"show_recent_posts_footer","type":"boolean","value":"true"},{"id":"62a48f869cd77522dc21c2a8","theme":"dunnkers-theme-edition","key":"title_font","type":"select","value":"Modern sans-serif"},{"id":"62a48f869cd77522dc21c2a9","theme":"dunnkers-theme-edition","key":"body_font","type":"select","value":"Modern sans-serif"},{"id":"62a48f869cd77522dc21c2aa","theme":"dunnkers-theme-edition","key":"email_signup_text","type":"text","value":null},{"id":"62a48f869cd77522dc21c2ab","theme":"dunnkers-theme-edition","key":"publication_cover_style","type":"select","value":"Fullscreen"},{"id":"62a48f869cd77522dc21c2ac","theme":"dunnkers-theme-edition","key":"show_featured_posts","type":"boolean","value":"true"},{"id":"62a48f869cd77522dc21c2ad","theme":"dunnkers-theme-edition","key":"featured_title","type":"text","value":"Featured articles"},{"id":"62a48f869cd77522dc21c2ae","theme":"dunnkers-theme-edition","key":"feed_title","type":"text","value":"Posts"},{"id":"62a48f869cd77522dc21c2af","theme":"dunnkers-theme-edition","key":"feed_layout","type":"select","value":"Expanded"},{"id":"62a48f869cd77522dc21c2b0","theme":"dunnkers-theme-edition","key":"show_share_links","type":"boolean","value":"false"},{"id":"62a48f869cd77522dc21c2b1","theme":"dunnkers-theme-edition","key":"show_author","type":"boolean","value":"true"},{"id":"62a48f869cd77522dc21c2b2","theme":"dunnkers-theme-edition","key":"show_related_posts","type":"boolean","value":"true"},{"id":"62a48f869cd77522dc21c2b3","theme":"dunnkers-theme-edition","key":"cover_icon_caption","type":"text","value":"Hi there üëãüèª."},{"id":"672e184162baf50001937dd5","theme":"edition","key":"navigation_layout","type":"select","value":"Logo on the left"},{"id":"672e184162baf50001937dd6","theme":"edition","key":"title_font","type":"select","value":"Modern sans-serif"},{"id":"672e184162baf50001937dd7","theme":"edition","key":"body_font","type":"select","value":"Modern sans-serif"},{"id":"672e184162baf50001937dd8","theme":"edition","key":"email_signup_text","type":"text","value":null},{"id":"672e184162baf50001937dd9","theme":"edition","key":"publication_cover_style","type":"select","value":"Fullscreen"},{"id":"672e184162baf50001937dda","theme":"edition","key":"show_featured_posts","type":"boolean","value":"true"},{"id":"672e184162baf50001937ddb","theme":"edition","key":"featured_title","type":"text","value":"Featured articles"},{"id":"672e184162baf50001937ddc","theme":"edition","key":"feed_title","type":"text","value":"Latest"},{"id":"672e184162baf50001937ddd","theme":"edition","key":"feed_layout","type":"select","value":"Expanded"},{"id":"672e184162baf50001937dde","theme":"edition","key":"show_author","type":"boolean","value":"true"},{"id":"672e184162baf50001937ddf","theme":"edition","key":"show_related_posts","type":"boolean","value":"true"}],"stripe_products":[]}}